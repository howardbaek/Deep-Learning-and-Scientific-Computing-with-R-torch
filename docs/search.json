[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Deep Learning and Scientific Computing with R torch",
    "section": "",
    "text": "Welcome!\n\nThis is the on-line edition of Deep Learning and Scientific Computing with R torch! Visit the GitHub repository for this site, or buy a physical copy from the publisher, CRC Press. You’ll also find the book at the usual outlets, e.g., Amazon.\nThis online work by Sigrid Keydana is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License.\n\nPreface\nThis is a book about torch, the R interface to PyTorch. PyTorch, as of this writing, is one of the major deep-learning and scientific-computing frameworks, widely used across industries and areas of research. With torch, you get to access its rich functionality directly from R, with no need to install, let alone learn, Python. Though still “young” as a project, torch already has a vibrant community of users and developers; the latter not just extending the core framework, but also, building on it in their own packages.\nIn this text, I’m attempting to attain three goals, corresponding to the book’s three major sections.\nThe first is a thorough introduction to core torch: the basic structures without whom nothing would work. Even though, in future work, you’ll likely go with higher-level syntactic constructs when possible, it is important to know what it is they take care of, and to have understood the core concepts. What’s more, from a practical point of view, you just need to be “fluent” in torch to some degree, so you don’t have to resort to “trial-and-error-programming” too often.\nIn the second section, basics explained, we proceed to explore various applications of deep learning, ranging from image recognition over time series and tabular data to audio classification. Here, too, the focus is on conceptual explanation. In addition, each chapter presents an approach you can use as a “template” for your own applications. Whenever adequate, I also try to point out the importance of incorporating domain knowledge, as opposed to the not-uncommon “big data, big models, big compute” approach.\nThe third section is special in that it highlights some of the non-deep-learning things you can do with torch: matrix computations (e.g., various ways of solving linear-regression problems), calculating the Discrete Fourier Transform, and wavelet analysis. Here, more than anywhere else, the conceptual approach is very important to me. Let me explain.\nFor one, I expect that in terms of educational background, my readers will vary quite a bit. With R being increasingly taught, and used, in the natural sciences, as well as other areas close to applied mathematics, there will be those who feel they can’t benefit much from a conceptual (though formula-guided!) explanation of how, say, the Discrete Fourier Transform works. To others, however, much of this may be uncharted territory, never to be entered if all goes its normal way. This may hold, for example, for people with a humanist, not-traditionally-empirically-oriented background, such as literature, cultural studies, or the philologies. Of course, chances are that if you’re among the latter, you may find my explanations, though concept-focused, still highly (or: too) mathematical. In that case, please rest assured that, to the understanding of these things (like many others worthwhile of understanding), it is a long way; but we have a life’s time.\nSecondly, even though deep learning has been “the” paradigm of the last decade, recent developments seem to indicate that interest in mathematical/domain-based foundations is (again – this being a recurring phenomenon) on the rise (Consider, for example, the Geometric Deep Learning approach, systematically explained in Bronstein et al. (2021), and conceptually introduced in Beyond alchemy: A first look at geometric deep learning.) In the future, I assume that we’ll likely see more and more “hybrid” approaches that integrate deep-learning techniques and domain knowledge. The Fourier Transform is not going away.\nLast but not least, on this topic, let me make clear that, of course, all chapters have torch code. In case of the Fourier Transform, for example, you’ll see not just the official way of doing this, using dedicated functionality, but also, various ways of coding the algorithm yourself – in a surprisingly small number of lines, and with highly impressive performance.\nThis, in a nutshell, is what to expect from the book. Before I close, there is one thing I absolutely need to say, all the more since even though I’d have liked to, I did not find occasion to address it much in the book, given the technicality of the content. In our societies, as adoption of machine/deep learning (“AI”) is growing, so are opportunities for misuse, by governments as well as private organizations. Often, harm may not even be intended; but still, outcomes can be catastrophic, especially for people belonging to minorities, or groups already at a disadvantage. Like that, even the inevitable, in most of today’s political systems, drive to make profits results in, at the very least, societies imbued with highly questionable features (think: surveillance, and the “quantification of everything”); and most likely, in discrimination, unfairness, and severe harm. Here, I cannot do more than draw attention to this problem, point you to an introductory blog post that perhaps you’ll find useful: Starting to think about AI Fairness, and just ask you to, please, be actively aware of this problem in public life as well as your own work and applications.\nFinally, let me end with saying thank you. There are far too many people to thank that I could ever be sure I haven’t left anyone out; so instead I’ll keep this short. I’m extremely grateful to my publisher, CRC Press (first and foremost, David Grubbs and Curtis Hill) for the extraordinarily pleasant interactions during all of the writing and editing phases. And very special thanks, for their support related to this book as well as their respective roles in the process, go to Daniel Falbel, the creator and maintainer of torch, who in-depth reviewed this book and helped me with many technical issues; Tracy Teal, my manager, who supported and encouraged me in every possible way; and Posit (formerly, RStudio), my employer, who lets me do things like this for a living.\nSigrid Keydana\n\n\n\n\nBronstein, Michael M., Joan Bruna, Taco Cohen, and Petar Velickovic. 2021. “Geometric Deep Learning: Grids, Groups, Graphs, Geodesics, and Gauges.” CoRR abs/2104.13478. https://arxiv.org/abs/2104.13478."
  },
  {
    "objectID": "basics_overview.html",
    "href": "basics_overview.html",
    "title": "1  Overview",
    "section": "",
    "text": "This book has three sections. The second and third will explore various deep learning applications and essential scientific computation techniques, respectively. Before though, in this first part, we are going to learn about torch’s basic building blocks: tensors, automatic differentiation, optimizers, and modules. I’d call this part “torch basics”, or, following a common template, “Getting started with torch”, were it not for a certain false impression this could create. These are basics, true, but basics in the sense of foundations: Having worked through the next chapters, you’ll have solid conceptions about how torch works, and you’ll have seen enough code to feel comfortable experimenting with the more involved examples encountered in later sections. In other words, you’ll be, to some degree, fluent in torch.\nIn addition, you’ll have coded a neural network from scratch – twice, even: One version will involve just raw tensors and their in-built capabilities, while the other will make use of dedicated torch structures that encapsulate, in an object-oriented way, functionality essential to neural network training. As a consequence, you’ll be excellently equipped for part two, where we’ll see how to apply deep learning to different tasks and domains."
  },
  {
    "objectID": "what_is_torch.html#in-torch-world",
    "href": "what_is_torch.html#in-torch-world",
    "title": "2  On torch, and how to get it",
    "section": "2.1 In torch world",
    "text": "2.1 In torch world\ntorch is an R port of PyTorch, one of the two (as of this writing) most-employed deep learning frameworks in industry and research. By its design, it is also an excellent tool to use in various types of scientific computation tasks (a subset of which you’ll encounter in the book’s final part). It is written entirely in R and C++ (including a bit of C). No Python installation is required to use it.\nOn the Python (PyTorch) side, the ecosystem appears as a set of concentric cycles. In the middle, there’s PyTorch itself, the core library without which nothing could work. Surrounding it, we have the inner circle of what could be called framework libraries, dedicated to special types of data (images, sound, text …), or centered on workflow tasks, like deployment. Then, there is the broader ecosystem of add-ons, specializations, and libraries for whom PyTorch is a building block, or a tool.\nOn the R side, we have the same “heart” – all depends on core torch – and we do have the same types of libraries; but the categories, the “circles”, appear less clearly set off from each other. There are no strict boundaries. There’s just a vibrant community of developers, of diverse origin and with diverse goals, working to further develop and extend torch, so it can help more and more people accomplish their various tasks. The ecosystem growing so quickly, I’ll refrain from naming individual packages – at any time, visit the torch website to see a featured subset.\nThere are three packages, though, that I will name here, since they are used in the book: torchvision , torchaudio, and luz. The former two bundle domain-specific transformations, deep learning models, datasets, and utilities for images (incl. video) and audio data, respectively. The third is a high-level, intuitive, nice-to-use interface to torch, allowing to define, train, and evaluate a neural network in just a few lines of code. Like torch itself, all three packages can be installed from CRAN."
  },
  {
    "objectID": "what_is_torch.html#installing-and-running-torch",
    "href": "what_is_torch.html#installing-and-running-torch",
    "title": "2  On torch, and how to get it",
    "section": "2.2 Installing and running torch",
    "text": "2.2 Installing and running torch\ntorch is available for Windows, MacOS, and Linux. If you have a compatible GPU, and the necessary NVidia software installed, you can benefit from significant speedup, a speedup that will depend on the type of model trained. All examples in this book, though, have been chosen so they can be run on the CPU, without posing taxing demands on your patience.\nDue to their often-transient character, I won’t elaborate on compatibility issues here, in the book; analogously, I’ll refrain from listing concrete installation instructions. At any time, you’ll find up-to-date information in the vignette; and you’re more than welcome, should you encounter problems or have questions, to open an issue in the torch GitHub repository."
  },
  {
    "objectID": "tensors.html#whats-in-a-tensor",
    "href": "tensors.html#whats-in-a-tensor",
    "title": "3  Tensors",
    "section": "3.1 What’s in a tensor?",
    "text": "3.1 What’s in a tensor?\nTo do anything useful with torch, you need to know about tensors. Not tensors in the math/physics sense. In deep learning frameworks such as TensorFlow and (Py-)Torch, tensors are “just” multi-dimensional arrays optimized for fast computation – not on the CPU only but also, on specialized devices such as GPUs and TPUs.\nIn fact, a torch tensor is like an R array, in that it can be of arbitrary dimensionality. But unlike array, it is designed for fast and scalable execution of mathematical calculations, and you can move it to the GPU. (It also has an extra capability of enormous practical impact – automatic differentiation – but we reserve that for the next chapter.)\nTechnically, a tensor feels a lot like an R6 object, in that you can access its fields and methods using $-syntax. Let’s create one and print it:\n\nlibrary(torch)\n\nt1 &lt;- torch_tensor(1)\nt1\n\ntorch_tensor\n 1\n[ CPUFloatType{1} ]\nThis is a tensor that holds just a single value, 1. It “lives” on the CPU, and its type is Float . Now take a look at the 1 in braces, {1}. This is not yet another indication of the tensor’s value. It indicates the tensor shape, or put differently: the space it lives in and the extent of its dimensions. Here, we have a one-dimensional tensor, that is, a vector. Just as in base R, vectors can consist of a single element only. (Remember that base R does not differentiate between 1 and c(1)).\nWe can use the aforementioned $-syntax to individually ascertain these properties, accessing the respective fields in the object one-by-one:\n\nt1$dtype\n\ntorch_Float\n\nt1$device\n\ntorch_device(type='cpu')\n\nt1$shape\n\n[1] 1\nWe can also directly change some of these properties, making use of the tensor object’s $to() method:\n\nt2 &lt;- t1$to(dtype = torch_int())\nt2$dtype\n\ntorch_Int\n\n# only applicable if you have a GPU\nt2 &lt;- t1$to(device = \"cuda\")\nt2$device\n\ntorch_device(type='cuda', index=0)\nHow about changing the shape? This is a topic deserving of treatment of its own, but as a first warm-up, let’s play around a bit. Without changing its value, we can turn this one-dimensional “vector tensor” into a two-dimensional “matrix tensor”:\n\nt3 &lt;- t1$view(c(1, 1))\nt3$shape\n\n[1] 1 1\nConceptually, this is analogous to how in R, we can have a one-element vector as well as a one-element matrix:\n\nc(1)\nmatrix(1)\n\n[1] 1\n\n     [,1]\n[1,]    1\nNow that we have an idea what a tensor is, let’s think about ways to create some."
  },
  {
    "objectID": "tensors.html#creating-tensors",
    "href": "tensors.html#creating-tensors",
    "title": "3  Tensors",
    "section": "3.2 Creating tensors",
    "text": "3.2 Creating tensors\nWe’ve already seen one way to create a tensor: calling torch_tensor() and passing in an R value. This way generalizes to multi-dimensional objects; we’ll see a few examples soon.\nHowever, that procedure can get unwieldy when we have to pass in lots of different values. Luckily, there is an alternative approach that applies whenever values should be identical throughout, or follow an apparent pattern. We’ll illustrate this technique as well in this section.\n\n3.2.1 Tensors from values\nAbove, we passed in a one-element vector to torch_tensor(); we can pass in longer vectors just the same way:\n\ntorch_tensor(1:5)\n\ntorch_tensor\n 1\n 2\n 3\n 4\n 5\n[ CPULongType{5} ]\nWhen given an R value (or a sequence of values), torch determines a suitable data type itself. Here, the assumption is that an integer type is desired, and torch chooses the highest-precision type available (torch_long() is synonymous to torch_int64()).\nIf we want a floating-point tensor instead, we can use $to() on the newly created instance (as we saw above). Alternatively, we can just let torch_tensor() know right away:\n\ntorch_tensor(1:5, dtype = torch_float())\n\ntorch_tensor\n 1\n 2\n 3\n 4\n 5\n[ CPUFloatType{5} ]\nAnalogously, the default device is the CPU; but we can also create a tensor that, right from the outset, is located on the GPU:\n\ntorch_tensor(1:5, device = \"cuda\")\n\ntorch_tensor\n 1\n 2\n 3\n 4\n 5\n[ CPUFloatType{5} ]\nNow, so far all we’ve been creating is vectors; what about matrices, that is, two-dimensional tensors?\nWe can pass in an R matrix just the same way:\n\ntorch_tensor(matrix(1:9, ncol = 3))\n\ntorch_tensor\n 1  4  7\n 2  5  8\n 3  6  9\n[ CPULongType{3,3} ]\nLook at the result. The numbers 1 to 9 appear column after column, just as in the R matrix we created it from. This may, or may not, be the intended outcome. If it’s not, just pass byrow = TRUE to the call to matrix():\n\ntorch_tensor(matrix(1:9, ncol = 3, byrow = TRUE))\n\ntorch_tensor\n 1  2  3\n 4  5  6\n 7  8  9\n[ CPULongType{3,3} ]\nWhat about higher-dimensional data? Following the same principle, we can pass in an array:\n\ntorch_tensor(array(1:24, dim = c(4, 3, 2)))\n\ntorch_tensor\n(1,.,.) = \n   1  13\n   5  17\n   9  21\n\n(2,.,.) = \n   2  14\n   6  18\n  10  22\n\n(3,.,.) = \n   3  15\n   7  19\n  11  23\n\n(4,.,.) = \n   4  16\n   8  20\n  12  24\n[ CPULongType{4,3,2} ]\nAgain, the result follows R’s array population logic. If that’s not what you want, it is probably easier to build up the tensor programmatically.\nBefore you start to panic, though, think about how rarely you’ll need to do this. In practice, you’ll mostly be creating tensors from an R dataset. We’ll take a close look at that in the last subsection, “Tensors from datasets”. Before though, it is instructive to spend a little time inspecting that last output.\nHere, pictorially, is the object we created (fig. 3.1). Let’s call the axis that extends to the right x, the one that goes into the page, y, and the one that points up, z. Then the tensor extends 4, 3, and 2 units, respectively, in the x, y, and z directions.\n\n\n\nFigure 3.1: A 4x3x2 tensor.\n\n\nThe array we passed to torch_tensor() prints like this:\n\narray(1:24, dim = c(4, 3, 2))\n\n, , 1\n\n     [,1] [,2] [,3]\n[1,]    1    5    9\n[2,]    2    6   10\n[3,]    3    7   11\n[4,]    4    8   12\n\n, , 2\n\n     [,1] [,2] [,3]\n[1,]   13   17   21\n[2,]   14   18   22\n[3,]   15   19   23\n[4,]   16   20   24\nCompare that with how the tensor prints, above. Array and tensor slice the object in different ways. The tensor slices its values into 3x2 rectangles, extending up and to the back, one for each of the four x-values. The array, on the other hand, splits them up by z-value, resulting in two big 4x3 slices that go up and to the right.\nAlternatively, we could say that the tensor starts thinking from the left/the “outside”; the array, from the right/the “inside”.\n\n\n3.2.2 Tensors from specifications\nThere are two broad conditions when torch’s bulk creation functions will come in handy: For one, when you don’t care about individual tensor values, but only about their distribution. Secondly, if they follow some conventional pattern.\nWhen we use bulk creation functions, instead of individual values we specify the shape they should have. Here, for example, we instantiate a 3x3 tensor, populated with standard-normally distributed values:\n\ntorch_randn(3, 3)\n\ntorch_tensor\n-0.6532  0.6557  2.0251\n-0.7914 -1.7220  1.0387\n 0.1931  1.0536 -0.2077\n[ CPUFloatType{3,3} ]\nAnd here is the equivalent for values that are uniformly distributed between zero and one:\n\ntorch_rand(3, 3)\n\ntorch_tensor\n 0.2498  0.5356  0.6515\n 0.3556  0.5799  0.1284\n 0.9884  0.4361  0.8040\n[ CPUFloatType{3,3} ]\nOften, we require tensors of all ones, or all zeroes:\n\ntorch_zeros(2, 5)\n\ntorch_tensor\n 0  0  0  0  0\n 0  0  0  0  0\n[ CPUFloatType{2,5} ]\n\ntorch_ones(2, 2)\n\ntorch_tensor\n 1  1\n 1  1\n[ CPUFloatType{2,2} ]\nMany more of these bulk creation functions exist. To wrap up, let’s see how to create some matrix types that are common in linear algebra. Here’s an identity matrix:\n\ntorch_eye(n = 5)\n\ntorch_tensor\n 1  0  0  0  0\n 0  1  0  0  0\n 0  0  1  0  0\n 0  0  0  1  0\n 0  0  0  0  1\n[ CPUFloatType{5,5} ]\nAnd here, a diagonal matrix:\n\ntorch_diag(c(1, 2, 3))\n\ntorch_tensor\n 1  0  0\n 0  2  0\n 0  0  3\n[ CPUFloatType{3,3} ]\n\n\n3.2.3 Tensors from datasets\nNow we look at how to create tensors from R datasets. Depending on the dataset itself, this process can feel “automatic” or require some thought and action.\nFirst, let’s try JohnsonJohnson that comes with base R. It is a time series of quarterly earnings per Johnson & Johnson share.\n\nJohnsonJohnson\n\n      Qtr1  Qtr2  Qtr3  Qtr4\n1960  0.71  0.63  0.85  0.44\n1961  0.61  0.69  0.92  0.55\n1962  0.72  0.77  0.92  0.60\n1963  0.83  0.80  1.00  0.77\n1964  0.92  1.00  1.24  1.00\n1965  1.16  1.30  1.45  1.25\n1966  1.26  1.38  1.86  1.56\n1967  1.53  1.59  1.83  1.86\n1968  1.53  2.07  2.34  2.25\n1969  2.16  2.43  2.70  2.25\n1970  2.79  3.42  3.69  3.60\n1971  3.60  4.32  4.32  4.05\n1972  4.86  5.04  5.04  4.41\n1973  5.58  5.85  6.57  5.31\n1974  6.03  6.39  6.93  5.85\n1975  6.93  7.74  7.83  6.12\n1976  7.74  8.91  8.28  6.84\n1977  9.54 10.26  9.54  8.73\n1978 11.88 12.06 12.15  8.91\n1979 14.04 12.96 14.85  9.99\n1980 16.20 14.67 16.02 11.61\nCan we just pass this to torch_tensor() and magically get what we want?\n\ntorch_tensor(JohnsonJohnson)\n\ntorch_tensor\n  0.7100\n  0.6300\n  0.8500\n  0.4400\n  0.6100\n  0.6900\n  0.9200\n  0.5500\n  0.7200\n  0.7700\n  0.9200\n  0.6000\n  0.8300\n  0.8000\n  1.0000\n  0.7700\n  0.9200\n  1.0000\n  1.2400\n  1.0000\n  1.1600\n  1.3000\n  1.4500\n  1.2500\n  1.2600\n  1.3800\n  1.8600\n  1.5600\n  1.5300\n  1.5900\n... [the output was truncated (use n=-1 to disable)]\n[ CPUFloatType{84} ]\nLooks like we can! The values are arranged exactly the way we want them; quarter after quarter.\nMagic? Not really. torch can only work with what it is given; and here, what it is given is actually a vector of doubles arranged in quarterly order. The data just print the way they do because they are of class ts:\n\nunclass(JohnsonJohnson)\n\n[1]  0.71  0.63  0.85  0.44  0.61  0.69  0.92  0.55  0.72\n[10] 0.77  0.92  0.60  0.83  0.80  1.00  0.77 0.92  1.00\n[19] 1.24  1.00  1.16  1.30  1.45  1.25  1.26  1.38  1.86\n[28] 1.56  1.53  1.59  1.83  1.86 1.53  2.07  2.34  2.25\n[37] 2.16  2.43  2.70  2.25  2.79  3.42  3.69  3.60  3.60\n[46] 4.32  4.32  4.05 4.86  5.04  5.04  4.41  5.58  5.85\n[55] 6.57  5.31  6.03  6.39  6.93  5.85  6.93  7.74  7.83\n[64] 6.12 7.74  8.91  8.28  6.84  9.54 10.26  9.54  8.73\n[73] 11.88 12.06 12.15  8.91 14.04 12.96 14.85  9.99 16.20\n[82] 14.67 16.02 11.61 \nattr(,\"tsp\")\n[1] 1960.00 1980.75    4.00\nSo this went well. Let’s try another one. Who is not kept up at night, pondering trunk thickness of orange trees?\n\nlibrary(dplyr)\n\nglimpse(Orange)\n\nRows: 35\nColumns: 3\n$ Tree          &lt;ord&gt; 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2,...\n$ age           &lt;dbl&gt; 118, 484, 664, 1004, 1231, 1372, 1582,...\n$ circumference &lt;dbl&gt; 30, 58, 87, 115, 120, 142, 145, 33, 69,...\n\ntorch_tensor(Orange)\n\nError in torch_tensor_cpp(data, dtype, device, requires_grad,\npin_memory) : R type not handled\nWhich type is not handled here? It seems obvious that the “culprit” must be Tree, an ordered-factor column. Let’s first check if torch can handle factors:\n\nf &lt;- factor(c(\"a\", \"b\", \"c\"), ordered = TRUE)\ntorch_tensor(f)\n\ntorch_tensor\n 1\n 2\n 3\n[ CPULongType{3} ]\nSo this worked fine. Then what else could it be? The problem here is the containing structure, the data.frame. We need to call as.matrix() on it first. Due to the presence of the factor, though, this will result in a matrix of all strings, which is not what we want. Therefore, we first extract the underlying levels (integers) from the factor, and then convert the data.frame to a matrix:\n\norange_ &lt;- Orange %&gt;% \n  mutate(Tree = as.numeric(Tree)) %&gt;%\n  as.matrix()\n\ntorch_tensor(orange_) %&gt;% print(n = 7)\n\ntorch_tensor\n    2   118    30\n    2   484    58\n    2   664    87\n    2  1004   115\n    2  1231   120\n    2  1372   142\n    2  1582   145\n... [the output was truncated (use n=-1 to disable)]\n[ CPUFloatType{35,3} ]\nLet’s try the same thing with another data.frame, okc from modeldata:\n\nlibrary(modeldata)\n\ndata(okc)\nokc %&gt;% glimpse()\n\nRows: 59,855\nColumns: 6\n$ age      &lt;int&gt; 22, 35, 38, 23, 29, 29, 32, 31, 24,...\n$ diet     &lt;chr&gt; \"strictly anything\", \"mostly other\",...\n$ height   &lt;int&gt; 75, 70, 68, 71, 66, 67, 65, 65, 67, 65,...\n$ location &lt;chr&gt; \"south san francisco\", \"oakland\",... \n$ date     &lt;date&gt; 2012-06-28, 2012-06-29, 2012-06-27,...\n$ Class    &lt;fct&gt; other, other, other, other, other, stem,...\nWe have two integer columns, which is fine, and one factor column, which we know how to handle. But what about the character and date columns? Trying to create a tensor from the date column individually, we see:\n\nprint(torch_tensor(okc$date), n = 7)\n\ntorch_tensor\n 15519\n 15520\n 15518\n 15519\n 15518\n 15520\n 15516\n... [the output was truncated (use n=-1 to disable)]\n[ CPUFloatType{59855} ]\nThis didn’t throw an error, but what does it mean? These are the actual values stored in an R Date, namely, the number of days since January 1, 1970. Technically, thus, we have a working conversion – whether the result makes sense pragmatically is a question of how you’re going to use it. Put differently, you’ll probably want to further process these data before using them in a computation, and how you do this will depend on the context.\nNext, let’s see about location, one of the columns of type character. What happens if we just pass it to torch as-is?\n\ntorch_tensor(okc$location)\n\nError in torch_tensor_cpp(data, dtype, device, requires_grad,\npin_memory) : R type not handled\nIn fact, there are no tensors in torch that store strings. We have to apply some scheme that converts them to a numeric type first. In cases like the present one, where every observation contains a single entity (as opposed to, say, a sentence or a paragraph), the easiest way of doing this from R is to first convert to factor, then to numeric, and then, to tensor:\n\nokc$location %&gt;%\n  factor() %&gt;%\n  as.numeric() %&gt;%\n  torch_tensor() %&gt;%\n  print(n = 7)\n\ntorch_tensor\n 120\n  74\n 102\n  10\n 102\n 102\n 102\n... [the output was truncated (use n=-1 to disable)]\n[ CPUFloatType{59855} ]\nTrue, this works well technically. It does, however, reduce information. For example, the first and third locations are “south san francisco” and “san francisco”, respectively. Once converted to factors, these are just as distant, semantically, as are “san francisco” and any other location. Again, whether this is of relevance depends on the specifics of the data, as well as your goal. If you think it does matter, you have a range of options, including, for example, grouping observations by some criterion, or converting to latitude/longitude. These considerations are by no means torch-specific; we just mention them here because they affect the “data ingestion workflow” to torch.\nFinally, no excursion into the world of real-life data science is complete without a consideration of NAs. Let’s see:\n\ntorch_tensor(c(1, NA, 3))\n\ntorch_tensor\n 1\nnan\n 3\n[ CPUFloatType{3} ]\nR’s NA gets converted to NaN. Can you work with that? Some torch function can. For example, torch_nanquantile() just ignores the NaNs:\n\ntorch_nanquantile(torch_tensor(c(1, NA, 3)), q = 0.5)\n\ntorch_tensor\n 2\n[ CPUFloatType{1} ]\nHowever, if you’re going to train a neural network, for example, you’ll need to think about how to meaningfully replace these missing values first. But that’s a topic for a later time."
  },
  {
    "objectID": "tensors.html#operations-on-tensors",
    "href": "tensors.html#operations-on-tensors",
    "title": "3  Tensors",
    "section": "3.3 Operations on tensors",
    "text": "3.3 Operations on tensors\nWe can perform all the usual mathematical operations on tensors.: add, subtract, divide … These operations are available as functions (starting with torch_) as well as as methods on objects (invoked with $-syntax). For example, the following are equivalent:\n\nt1 &lt;- torch_tensor(c(1, 2))\nt2 &lt;- torch_tensor(c(3, 4))\n\ntorch_add(t1, t2)\n# equivalently\nt1$add(t2)\n\ntorch_tensor\n 4\n 6\n[ CPUFloatType{2} ]\nIn both cases, a new object is created; neither t1 nor t2 are modified. There exists an alternate method that modifies its object in-place:\n\nt1$add_(t2)\n\ntorch_tensor\n 4\n 6\n[ CPUFloatType{2} ]\n\nt1\n\ntorch_tensor\n 4\n 6\n[ CPUFloatType{2} ]\nIn fact, the same pattern applies for other operations: Whenever you see an underscore appended, the object is modified in-place.\nNaturally, in a scientific-computing setting, matrix operations are of special interest. Let’s start with the dot product of two one-dimensional structures, i.e., vectors.\n\nt1 &lt;- torch_tensor(1:3)\nt2 &lt;- torch_tensor(4:6)\nt1$dot(t2)\n\ntorch_tensor\n32\n[ CPULongType{} ]\nWere you thinking this shouldn’t work? Should we have needed to transpose (torch_t()) one of the tensors? In fact, this also works:\n\nt1$t()$dot(t2)\n\ntorch_tensor\n32\n[ CPULongType{} ]\nThe reason the first call worked, too, is that torch does not distinguish between row vectors and column vectors. In consequence, if we multiply a vector with a matrix, using torch_matmul(), we don’t need to worry about the vector’s orientation either:\n\nt3 &lt;- torch_tensor(matrix(1:12, ncol = 3, byrow = TRUE))\nt3$matmul(t1)\n\ntorch_tensor\n 14\n 32\n 50\n 68\n[ CPULongType{4} ]\nThe same function, torch_matmul(), would be used to multiply two matrices. Note how this is different from what torch_multiply() does, namely, scalar-multiply its arguments:\n\ntorch_multiply(t1, t2)\n\ntorch_tensor\n  4\n 10\n 18\n[ CPULongType{3} ]\nMany more tensor operations exist, some of which you’ll meet over the course of this journey. But there is one group that deserves special mention.\n\n3.3.1 Summary operations\nIf you have an R matrix and are about to compute a sum, this could, normally, mean one of three things: the global sum, row sums, or column sums. Let’s see all three of them at work (using apply() for a reason):\n\nm &lt;- outer(1:3, 1:6)\n\nsum(m)\napply(m, 1, sum)\napply(m, 2, sum)\n\n[1] 126\n[1]  21 42 63\n[1]   6 12 18 24 30 36\nAnd now, the torch equivalents. We start with the overall sum.\n\nt &lt;- torch_outer(torch_tensor(1:3), torch_tensor(1:6))\nt$sum()\n\ntorch_tensor\n126\n[ CPULongType{} ]\nIt gets more interesting for the row and column sums. The dim argument tells torch which dimension(s) to sum over. Passing in dim = 1, we see:\n\nt$sum(dim = 1)\n\ntorch_tensor\n  6\n 12\n 18\n 24\n 30\n 36\n[ CPULongType{6} ]\nUnexpectedly, these are the column sums! Before drawing conclusions, let’s check what happens with dim = 2:\n\nt$sum(dim = 2)\n\ntorch_tensor\n 21\n 42\n 63\n[ CPULongType{3} ]\nNow, we have sums over rows. Did we misunderstand something about how torch orders dimensions? No, it’s not that. In torch, when we’re in two dimensions, we think rows first, columns second. (And as you’ll see in a minute, we start indexing with 1, just as in R in general.)\nInstead, the conceptual difference is specific to aggregating, or “grouping”, operations. In R, grouping, in fact, nicely characterizes what we have in mind: We group by row (dimension 1) for row summaries, by column (dimension 2) for column summaries. In torch, the thinking is different: We collapse the columns (dimension 2) to compute row summaries, the rows (dimension 1) for column summaries.\nThe same thinking applies in higher dimensions. Assume, for example, that we been recording time series data for four individuals. There are two features, and both of them have been measured at three times. If we were planning to train a recurrent neural network (much more on that later), we would arrange the measurements like so:\n\nDimension 1: Runs over individuals.\nDimension 2: Runs over points in time.\nDimension 3: Runs over features.\n\nThe tensor then would look like this:\n\nt &lt;- torch_randn(4, 3, 2)\nt\n\ntorch_tensor\n(1,.,.) = \n -1.3427  1.1303\n  1.0430  0.8232\n  0.7952 -0.2447\n\n(2,.,.) = \n -1.9929  0.1251\n  0.4143  0.3523\n  0.9819  0.3219\n\n(3,.,.) = \n  0.6389 -0.2606\n  2.4011  0.2656\n -0.1750 -0.2597\n\n(4,.,.) = \n  1.4534  0.7229\n  1.2503 -0.2975\n  1.6749 -1.2154\n[ CPUFloatType{4,3,2} ]\nTo obtain feature averages, independently of subject and time, we would collapse dimensions 1 and 2:\n\nt$mean(dim = c(1, 2))\n\ntorch_tensor\n-0.1600\n 0.1363\n[ CPUFloatType{2} ]\nIf, on the other hand, we wanted feature averages, but individually per person, we’d do:\n\nt$mean(dim = 2)\n\ntorch_tensor\n-0.6153  0.8290\n 0.3961  0.2739\n-0.0579  0.1966\n-0.3628 -0.7544\n[ CPUFloatType{4,2} ]\nHere, the single feature “collapsed” is the time step."
  },
  {
    "objectID": "tensors.html#accessing-parts-of-a-tensor",
    "href": "tensors.html#accessing-parts-of-a-tensor",
    "title": "3  Tensors",
    "section": "3.4 Accessing parts of a tensor ",
    "text": "3.4 Accessing parts of a tensor \nOften, when working with tensors, some computational step is meant to operate on just part of its input tensor. When that part is a single entity (value, row, column …), we commonly refer to this as indexing; when it’s a range of such entities, it is called slicing.\n\n3.4.1 “Think R”\nBoth indexing and slicing work essentially as in R. There are a few syntactic extensions, and I’ll present these in the subsequent section. But overall you should find the behavior intuitive.\nThis is because just as in R, indexing in torch is one-based. And just as in R, singleton dimensions are dropped.\nIn the below example, we ask for the first column of a two-dimensional tensor; the result is one-dimensional, i.e., a vector:\n\nt &lt;- torch_tensor(matrix(1:9, ncol = 3, byrow = TRUE))\nt[1, ]\n\ntorch_tensor\n 1\n 2\n 3\n[ CPULongType{3} ]\nIf we specify drop = FALSE, though, dimensionality is preserved:\n\nt[1, , drop = FALSE]\n\ntorch_tensor\n 1  2  3\n[ CPULongType{1,3} ]\nWhen slicing, there are no singleton dimensions – and thus, no additional considerations to be taken into account:\n\nt &lt;- torch_rand(3, 3, 3)\nt[1:2, 2:3, c(1, 3)]\n\ntorch_tensor\n(1,.,.) = \n  0.5273  0.3781\n  0.5303  0.9537\n\n(2,.,.) = \n  0.2966  0.7160\n  0.5421  0.4284\n[ CPUFloatType{2,2,2} ]\nIn sum, thus, indexing and slicing work very much like in R. Now, let’s look at the aforementioned extensions that further enhance usability.\n\n3.4.1.1 Beyond R\nOne of these extensions concerns accessing the last element in a tensor. Conveniently, in torch, we can use -1 to accomplish that:\n\nt &lt;- torch_tensor(matrix(1:4, ncol = 2, byrow = TRUE))\nt[-1, -1]\n\ntorch_tensor\n4\n[ CPULongType{} ]\nNote how in R, negative indices have a quite different effect, causing elements at respective positions to be removed.\nAnother useful feature extends slicing syntax to allow for a step pattern, to be specified after a second colon. Here, we request values from every second column between columns one and eight:\n\nt &lt;- torch_tensor(matrix(1:20, ncol = 10, byrow = TRUE))\nt[ , 1:8:2]\n\ntorch_tensor\n  1   3   5   7\n 11  13  15  17\n[ CPULongType{2,4} ]\nFinally, sometimes the same code should be able to work with tensors of different dimensionalities. In this case, we can use .. to collectively designate any existing dimensions not explicitly referenced.\nFor example, say we want to index into the first dimension of whatever tensor is passed, be it a matrix, an array, or some higher-dimensional structure. The following\n\nt[1, ..]\n\nwill work for all:\n\nt1 &lt;- torch_randn(2, 2)\nt2 &lt;- torch_randn(2, 2, 2)\nt3 &lt;- torch_randn(2, 2, 2, 2)\nt1[1, ..]\nt2[1, ..]\nt3[1, ..]\n\ntorch_tensor\n-0.6179\n-1.4769\n[ CPUFloatType{2} ]\n\n\ntorch_tensor\n 1.0602 -0.9028\n 0.2942  0.4611\n[ CPUFloatType{2,2} ]\n\n\ntorch_tensor\n(1,.,.) = \n  1.3304 -0.6018\n  0.0825  0.1221\n\n(2,.,.) = \n  1.7129  1.2932\n  0.2371  0.9041\n[ CPUFloatType{2,2,2} ]\nIf we wanted to index into the last dimension instead, we’d write t[.., 1]. We can even combine both:\n\nt3[1, .., 2]\n\ntorch_tensor\n-0.6018  0.1221\n 1.2932  0.9041\n[ CPUFloatType{2,2} ]\nNow, a topic just as important as indexing and slicing is reshaping of tensors."
  },
  {
    "objectID": "tensors.html#reshaping-tensors",
    "href": "tensors.html#reshaping-tensors",
    "title": "3  Tensors",
    "section": "3.5 Reshaping tensors",
    "text": "3.5 Reshaping tensors\nSay you have a tensor with twenty-four elements. What is its shape? It could be any of the following:\n\na vector of length 24\na matrix of shape 24 x 1, or 12 x 2, or 6 x 4, or …\na three-dimensional array of size 24 x 1 x 1, or 12 x 2 x 1, or …\nand so on (in fact, it could even have shape 24 x 1 x 1 x 1 x 1)\n\nWe can modify a tensor’s shape, without juggling around its values, using the view() method. Here is the initial tensor, a vector of length 24:\n\nt &lt;- torch_zeros(24)\nprint(t, n = 3)\n\ntorch_tensor\n 0\n 0\n 0\n... [the output was truncated (use n=-1 to disable)]\n[ CPUFloatType{24} ]\nHere is that same vector, reshaped to a wide matrix:\n\nt2 &lt;- t$view(c(2, 12))\nt2\n\ntorch_tensor\n 0  0  0  0  0  0  0  0  0  0  0  0\n 0  0  0  0  0  0  0  0  0  0  0  0\n[ CPUFloatType{2,12} ]\nSo we have a new tensor, t2, but interestingly (and importantly, performance-wise), torch did not have to allocate any new storage for its values. This we can verify for ourselves. Both tensors store their data in the same location:\n\nt$storage()$data_ptr()\nt2$storage()$data_ptr()\n\n[1] \"0x55cd15789180\"\n[1] \"0x55cd15789180\"\nLet’s talk a bit about how this is possible.\n\n3.5.1 Zero-copy reshaping vs. reshaping with copy\nWhenever we ask torch to perform an operation that changes the shape of a tensor, it tries to fulfill the request without allocating new storage for the tensor’s contents. This is possible because the same data – the same bytes, ultimately – can be read in different ways. All that is needed is storage for the metadata.\nHow does torch do it? Let’s see a concrete example. We start with a 3 x 5 matrix.\n\nt &lt;- torch_tensor(matrix(1:15, nrow = 3, byrow = TRUE))\nt\n\n torch_tensor\n  1   2   3   4   5\n  6   7   8   9  10\n 11  12  13  14  15\n[ CPULongType{3,5} ]\nTensors have a stride() method that tracks, for every dimension, how many elements have to be traversed to arrive at its next element. For the above tensor t, to go to the next row, we have to skip over five elements, while to go to the next column, we need to skip just one:\n\nt$stride()\n\n[1] 5 1\nNow we reshape the tensor so it has five rows and three columns instead. Remember, the data themselves do not change.\n\nt2 &lt;- t$view(c(5, 3))\nt2\n\ntorch_tensor\n  1   2   3\n  4   5   6\n  7   8   9\n 10  11  12\n 13  14  15\n[ CPULongType{5,3} ]\nThis time, to arrive at the next row, we just skip three elements instead of five. To get to the next column, we still just “jump over” a single element only:\n\nt2$stride()\n\n[1] 3 1\nNow you may be thinking, what if the order of the elements also has to change? For example, in matrix transposition. Is that still doable with the metadata-only approach?\n\nt3 &lt;- t$t()\nt3\n\ntorch_tensor\n  1   6  11\n  2   7  12\n  3   8  13\n  4   9  14\n  5  10  15\n[ CPULongType{5,3} ]\nIn fact, it must be, as both the original tensor and its transpose point to the same place in memory:\n\nt$storage()$data_ptr()\nt3$storage()$data_ptr()\n\n[1] \"0x55cd1cd4a840\"\n[1] \"0x55cd1cd4a840\"\nAnd it makes sense: This will work if we know that to arrive at the next row, we just skip a single element, while to arrive at the next column, that’s five to skip over now. Let’s verify:\n\nt3$stride()\n\n[1] 1 5\nExactly.\nWhenever possible, torch will try to handle shape-changing operations in this way.\nAnother such zero-copy operation (and one we’ll see a lot) is squeeze(), together with its antagonist, unsqueeze(). The latter adds a singleton dimension at the requested position, the former removes it. For example:\n\nt &lt;- torch_randn(3)\nt\n\nt$unsqueeze(1)\n\ntorch_tensor\n 0.2291\n-0.9454\n 1.6630\n[ CPUFloatType{3} ]\n\ntorch_tensor\n 0.2291 -0.9454  1.6630\n[ CPUFloatType{1,3} ]\nHere we added a singleton dimension in front. Alternatively, we could have used t$unsqueeze(2) to add it at the end.\nNow, will that zero-copy technique ever fail? Here is an example where it does:\n\nt &lt;- torch_randn(3, 3)\nt$t()$view(9)\n\n Error in (function (self, size)  : \n  view size is not compatible with input tensor's size and\n  stride (at least one dimension spans across two contiguous\n  subspaces). Use .reshape(...) instead. [...]\nWhen two operations that change the stride are executed in sequence, the second is pretty likely to fail. There is a way to exactly determine whether it will fail or not; but the easiest way is to just use a different method instead of view(): reshape(). The latter will “automagically” work metadata-only if that is possible, but make a copy if not:\n\nt &lt;- torch_randn(3, 3)\nt2 &lt;- t$t()$reshape(9)\n\nt$storage()$data_ptr()\nt2$storage()$data_ptr()\n\n[1] \"0x55cd1622a000\"\n[1] \"0x55cd19d31e40\"\nAs expected, both tensors are now stored in different locations.\nFinally, we are going to end this long chapter with a feature that may seem overwhelming at first, but is of tremendous importance performance-wise. Like with so many things, it takes time to get accustomed to, but rest assured: You’ll encounter it again and again, in this book and in many projects using torch. It is called broadcasting."
  },
  {
    "objectID": "tensors.html#broadcasting",
    "href": "tensors.html#broadcasting",
    "title": "3  Tensors",
    "section": "3.6 Broadcasting",
    "text": "3.6 Broadcasting\nWe often have to perform operations on tensors with shapes that don’t match exactly.\nOf course, we wouldn’t probably try to add, say, a length-two vector to a length-five vector. But there are things we may want to do: for example, multiply every element by a scalar. This works:\n\nt1 &lt;- torch_randn(3, 5)\nt1 * 0.5\n\ntorch_tensor\n-0.4845  0.3092 -0.3710  0.3558 -0.2126\n-0.3419  0.1160  0.1800 -0.0094 -0.0189\n-0.0468 -0.4030 -0.3172 -0.1558 -0.6247\n[ CPUFloatType{3,5} ]\nThat was probably a bit underwhelming. We’re used to that; from R. But the following does not work in R. The intention here would be to add the same vector to every row in a matrix:\n\nm &lt;- matrix(1:15, ncol = 5, byrow = TRUE)\nm2 &lt;- matrix(1:5, ncol = 5, byrow = TRUE)\n\nm + m2\n\nError in m + m2 : non-conformable arrays\nNeither does it help if we make m2 a vector.\n\nm3 &lt;- 1:5\n\nm + m3\n\n     [,1] [,2] [,3] [,4] [,5]\n[1,]    2    6    5    9    8\n[2,]    8   12   11   10   14\n[3,]   14   13   17   16   20\nSyntactically this worked, but semantics-wise this is not what we intended.\nNow, we try both of the above with torch. First, again, the scenario where both tensors are two-dimensional (even though, conceptually, one of them is a row vector):\n\nt &lt;- torch_tensor(m)\nt2 &lt;- torch_tensor(m2)\n\nt$shape\nt2$shape\n\nt$add(t2)\n\n[1] 3 5\n[1] 1 5\ntorch_tensor\n  2   4   6   8  10\n  7   9  11  13  15\n 12  14  16  18  20\n[ CPULongType{3,5} ]\nAnd now, with the thing to be added a one-dimensional tensor:\n\nt3 &lt;- torch_tensor(m3)\n\nt3$shape\n\nt$add(t3)\n\n[1] 5\ntorch_tensor\n  2   4   6   8  10\n  7   9  11  13  15\n 12  14  16  18  20\n[ CPULongType{3,5} ]\nIn torch, both ways worked as intended. Let’s see why.\nAbove, I’ve printed the tensor shapes for a reason. To a tensor of shape 3 x 5, we were able to add both a tensor of shape 3 and a tensor of shape 1 x 5. Together, these illustrate how broadcasting works. In a nutshell, this is what happens:\n\nThe 1 x 5 tensor, when used as an addend, is virtually expanded, that is, treated as if it contained the same row three times. This kind of expansion can only be performed if the non-matching dimension is a singleton, and if it is located on the left.\nThe same thing happens to the shape-3 tensor, but there is one additional step that takes place first: A leading dimension of size 1 is – virtually – appended on the left. This puts us in exactly the same state we were in in (1), and we continue from there.\n\nImportantly, no physical expansions take place.\nLet’s systematize these rules.\n\n3.6.1 Broadcasting rules\nThe rules are the following. The first, unspectactular though it may look, is the basis for everything else.\n\nWe align tensor shapes, starting from the right.\n\nSay we have two tensors, one of size 3 x 7 x 1, the other of size 1 x 5. Here they are, right-aligned:\n# t1, shape:        3  7  1\n# t2, shape:           1  5\n\nStarting from the right, the sizes along aligned axes either have to match exactly, or one of them has to be equal to 1. In the latter case, the singleton-dimension tensor is broadcast to the non-singleton one.\n\nIn the above example, broadcasting happens twice – once for each tensor. This (virtually) yields\n# t1, shape:        3  7  5\n# t2, shape:           7  5\n\nIf, on the left, one of the tensors has an additional axis (or more than one), the other is virtually expanded to have a dimension of size 1 in that place, in which case broadcasting will occur as stated in (2).\n\nIn our example, this happens to the second tensor. First, there is a virtual expansion\n# t1, shape:        3  7  5\n# t2, shape:        1  7  5\nand then, broadcasting takes place:\n# t1, shape:        3  7  5\n# t2, shape:        3  7  5\nIn this example, we see that broadcasting can act on both tensors at the same time. The thing to keep in mind, though, is that we always start looking from the right. For example, no broadcasting in the world could make this work:\n\ntorch_zeros(4, 3, 2, 1)$add(torch_ones(4, 3, 2)) # error!\n\n\nNow, that was one of the longest, and least applied-seeming, perhaps, chapters in the book. But feeling comfortable with tensors is, I dare say, a precondition for being fluent in torch. The same goes for the topic covered in the next chapter, automatic differentiation. But the difference is, there torch does all the heavy lifting for us. We just need to understand what it’s doing."
  },
  {
    "objectID": "autograd.html#why-compute-derivatives",
    "href": "autograd.html#why-compute-derivatives",
    "title": "4  Autograd",
    "section": "4.1 Why compute derivatives?",
    "text": "4.1 Why compute derivatives?\nIn supervised machine learning, we have at our disposal a training set, where the variable we’re hoping to predict is known. This is the target, or ground truth. We now develop and train a prediction algorithm, based on a set of input variables, the predictors. This training, or learning, process, is based on comparing the algorithm’s predictions with the ground truth, a comparison that leads to a number capturing how good or bad the current predictions are. To provide this number is the job of the loss function.\nOnce it is aware of the current loss, an algorithm can adjust its parameters – the weights, in a neural network – in order to deliver better predictions. It just has to know in which direction to adjust them. This information is made available by the gradient, the vector of derivatives.\nAs an example, we imagine a loss function that looks like this (fig. 4.1):\n\n\n\nFigure 4.1: Hypothetical loss function (a paraboloid).\n\n\nThis is a quadratic function of two variables: \\(f(x_1, x_2) = 0.2 {x_1}^2 + 0.2 {x_2}^2 - 5\\). It has its minimum at (0,0), and this is the point we’d like to be at. As humans, standing at the location designated by the white dot, and looking at the landscape, we have a pretty clear idea how to go downhill fast (assuming we’re not scared by the slope). To find the best direction computationally, however, we compute the gradient.\nTake the \\(x_1\\) direction. The derivative of the function with respect to \\(x_1\\) indicates how its value varies as \\(x_1\\) varies. As we know the function in closed form, we can compute that: \\(\\frac{\\partial f}{\\partial x_1} = 0.4 x_1\\). This tells us that as \\(x_1\\) increases, loss increases, and how fast. But we want loss to decrease, so we have to go in the opposite direction.\nThe same holds for the \\(x_2\\)-axis. We compute the derivative (\\(\\frac{\\partial f}{\\partial x_2} = 0.4 x_2\\)). Again, we want to take the direction opposite to where the derivative points. Overall, this yields a descent direction of \\(\\begin{bmatrix}-0.4x_1\\\\-0.4x_2 \\end{bmatrix}\\).\nDescriptively, this strategy is called steepest descent. Commonly referred to as gradient descent, it is the most basic optimization algorithm in deep learning. Perhaps unintuitively, it is not always the most efficient way. And there’s another question: Can we assume that this direction, computed at the starting point, will remain optimal as we continue descending? Maybe we’d better regularly recompute directions instead? Questions like this will be addressed in later chapters."
  },
  {
    "objectID": "autograd.html#automatic-differentiation-example",
    "href": "autograd.html#automatic-differentiation-example",
    "title": "4  Autograd",
    "section": "4.2 Automatic differentiation example",
    "text": "4.2 Automatic differentiation example\nNow that we know why we need derivatives, let’s see how automatic differentiation (AD) would compute them.\nThis (fig. 4.2) is how our above function could be represented in a computational graph. x1 and x2 are input nodes, corresponding to function parameters \\(x_1\\) and \\(x_2\\). x7 is the function’s output; all other nodes are intermediate ones, necessary to ensure correct order of execution. (We could have given the constants, -5 , 0.2, and 2, their own nodes as well; but as they’re remaining, well, constant anyway, we’re not too interested in them and prefer having a simpler graph.)\n\n\n\nFigure 4.2: Example of a computational graph.\n\n\nIn reverse-mode AD, the flavor of automatic differentiation implemented by torch, the first thing that happens is to calculate the function’s output value. This corresponds to a forward pass through the graph. Then, a backward pass is performed to calculate the gradient of the output with respect to both inputs, x1 and x2. In this process, information becomes available, and is built up, from the right:\n\nAt x7, we calculate partial derivatives with respect to x5 and x6. Basically, the equation to differentiate looks like this: \\(f(x_5, x_6) = x_5 + x_6 - 5\\). Thus, both partial derivatives are 1.\nFrom x5, we move to the left to see how it depends on x3. We find that \\(\\frac{\\partial x_5}{\\partial x_3} = 0.2\\). At this point, applying the chain rule of calculus, we already know how the output depends on x3: \\(\\frac{\\partial f}{\\partial x_3} = 0.2 * 1 = 0.2\\).\nFrom x3, we take the final step to x. We learn that \\(\\frac{\\partial x_3}{\\partial x_1} = 2 x_1\\). Now, we again apply the chain rule, and are able to formulate how the function depends on its first input: \\(\\frac{\\partial f}{\\partial x_1} = 2 x_1 * 0.2 * 1 = 0.4 x_1\\).\nAnalogously, we determine the second partial derivative, and thus, already have the gradient available: \\(\\nabla f = \\frac{\\partial f}{\\partial x_1} + \\frac{\\partial f}{\\partial x_2} = 0.4 x_1 + 0.4 x_2\\).\n\nThat is the principle. In practice, different frameworks implement reverse-mode automatic differentiation differently. We’ll catch a glimpse of how torch does it in the next section."
  },
  {
    "objectID": "autograd.html#automatic-differentiation-with-torch-autograd",
    "href": "autograd.html#automatic-differentiation-with-torch-autograd",
    "title": "4  Autograd",
    "section": "4.3 Automatic differentiation with torch autograd",
    "text": "4.3 Automatic differentiation with torch autograd\nFirst, a quick note on terminology. In torch, the AD engine is usually referred to as autograd, and that is the way you’ll see it denoted in most of the rest of this book. Now, back to the task.\nTo construct the above computational graph with torch, we create “source” tensors x1 and x2. These will mimic the parameters whose impact we’re interested in. However, if we just proceed “as usual”, creating the tensors the way we’ve been doing so far, torch will not prepare for AD. Instead, we need to pass in requires_grad = TRUE when instantiating those tensors:\n\nlibrary(torch)\n\nx1 &lt;- torch_tensor(2, requires_grad = TRUE)\nx2 &lt;- torch_tensor(2, requires_grad = TRUE)\n\n(By the way, the value 2 for both tensors was chosen completely arbitrarily.)\nNow, to create “invisible” nodes x3 to x6 , we square and multiply accordingly. Then x7 stores the final result.\n\nx3 &lt;- x1$square()\nx5 &lt;- x3 * 0.2\n\nx4 &lt;- x2$square()\nx6 &lt;- x4 * 0.2\n\nx7 &lt;- x5 + x6 - 5\nx7\n\ntorch_tensor\n-3.4000\n[ CPUFloatType{1} ][ grad_fn = &lt;SubBackward1&gt; ]\nNote that we have to add requires_grad = TRUE when creating the “source” tensors only. All dependent nodes in the graph inherit this property. For example:\n\nx7$requires_grad\n\n[1] TRUE\nNow, all prerequisites are fulfilled to see automatic differentiation at work. All we need to do to determine how x7 depends on x1 and x2 is call backward():\n\nx7$backward()\n\nDue to this call, the $grad fields have been populated in x1 and x2:\n\nx1$grad\nx2$grad\n\n 0.8000\n[ CPUFloatType{1} ]\ntorch_tensor\n 0.8000\n[ CPUFloatType{1} ]\nThese are the partial derivatives of x7 with respect to x1 and x2, respectively. Conforming to our manual calculations above, both amount to 0.8, that is, 0.4 times the tensor values 2 and 2.\nHow about the accumulation process we said was needed to build up those end-to-end derivatives? Can we “follow” the end-to-end derivative as it’s being built up? For example, can we see how the final output depends on x3?\n\nx3$grad\n\n[W TensorBody.h:470] Warning: The .grad attribute of a Tensor\nthat is not a leaf Tensor is being accessed. Its .grad attribute\nwon't be populated during autograd.backward().\nIf you indeed want the .grad field to be populated for a \nnon-leaf Tensor, use .retain_grad() on the non-leaf Tensor.[...]\n\ntorch_tensor\n[ Tensor (undefined) ]\nThe field does not seem to be populated. In fact, while it has to compute them, torch throws away the intermediate aggregates once they are no longer needed, to save memory. We can, however, ask it to keep them, using retain_grad = TRUE:\n\nx3 &lt;- x1$square()\nx3$retain_grad()\n\nx5 &lt;- x3 * 0.2\nx5$retain_grad()\n\nx4 &lt;- x2$square()\nx4$retain_grad()\n\nx6 &lt;- x4 * 0.2\nx6$retain_grad()\n\nx7 &lt;- x5 + x6 - 5\nx7$backward()\n\nNow, we find that x3’s grad field is populated:\n\nx3$grad\n\ntorch_tensor\n 0.2000\n[ CPUFloatType{1} ]\nThe same goes for x4, x5, and x6:\n\nx4$grad\nx5$grad\nx6$grad\n\ntorch_tensor\n 0.2000\n[ CPUFloatType{1} ]\ntorch_tensor\n 1\n[ CPUFloatType{1} ]\ntorch_tensor\n 1\n[ CPUFloatType{1} ]\nThere is one remaining thing we might be curious about. We’ve managed to catch a glimpse of the gradient-accumulation process from the “running gradient” point of view, in a sense; but how about the individual derivatives that need to be taken in order to proceed with accumulation? For example, what x3$grad tells us is how the output depends on the intermediate state at x3; how do we get from there to x1, the actual input node?\nIt turns out that of that aspect, too, we can get an idea. During the forward pass, torch already takes a note on what it will have to do, later, to calculate the individual derivatives. This “recipe” is stored in a tensor’s grad_fn field. For x3, this adds the “missing link” to x1:\n\nx3$grad_fn\n\nPowBackward0\nThe same works for x4, x5, and x6:\n\nx4$grad_fn\nx5$grad_fn\nx6$grad_fn\n\nPowBackward0\nMulBackward1\nMulBackward1\nAnd there we are! We’ve seen how torch computes derivatives for us, and we’ve even caught a glimpse of how it does it. Now, we are ready to play around with our first two applied tasks."
  },
  {
    "objectID": "optim_1.html#an-optimization-classic",
    "href": "optim_1.html#an-optimization-classic",
    "title": "5  Function minimization with autograd",
    "section": "5.1 An optimization classic",
    "text": "5.1 An optimization classic\nIn optimization research, the Rosenbrock function is a classic. It is a function of two variables; its minimum is at (1,1). If you take a look at its contours, you see that the minimum lies inside a stretched-out, narrow valley (fig. 5.1):\n\n\n\nFigure 5.1: Rosenbrock function.\n\n\nHere is the function definition. a and b are parameters that can be freely chosen; the values we use here are a frequent choice.\n\na &lt;- 1\nb &lt;- 5\n\nrosenbrock &lt;- function(x) {\n  x1 &lt;- x[1]\n  x2 &lt;- x[2]\n  (a - x1)^2 + b * (x2 - x1^2)^2\n}"
  },
  {
    "objectID": "optim_1.html#minimization-from-scratch",
    "href": "optim_1.html#minimization-from-scratch",
    "title": "5  Function minimization with autograd",
    "section": "5.2 Minimization from scratch",
    "text": "5.2 Minimization from scratch\nThe scenario is the following. We start at some given point (x1,x2), and set out to find the location where the Rosenbrock function has its minimum.\nWe follow the strategy outlined in the previous chapter: compute the function’s gradient at our current position, and use it to go the opposite way. We don’t know how far to go; if we take too big a big step we may easily overshoot. (If you look back at the contour plot, you see that if you were standing at one of the steep cliffs east or west of the minimum, this could happen very fast.)\nThus, it is best to proceed iteratively, taking moderate steps and re-evaluating the gradient every time.\nIn a nutshell, the optimization procedure then looks somewhat like this:\n\nlibrary(torch)\n\n# attention: this is not the correct procedure yet!\n\nfor (i in 1:num_iterations) {\n\n  # call function, passing in current parameter value\n  value &lt;- rosenbrock(x)\n\n  # compute gradient of value w.r.t. parameter\n  value$backward()\n\n  # manually update parameter, subtracting a fraction\n  # of the gradient\n  # this is not quite correct yet!\n  x$sub_(lr * x$grad)\n}\n\nAs written, this code snippet demonstrates our intentions, but it’s not quite correct (yet). It is also missing a few prerequisites: Neither the tensor x nor the variables lr and num_iterations have been defined. Let’s make sure we have those ready first. lr, for learning rate, is the fraction of the gradient to subtract on every step, and num_iterations is the number of steps to take. Both are a matter of experimentation.\n\nlr &lt;- 0.01\n\nnum_iterations &lt;- 1000\n\nx is the parameter to optimize, that is, it is the function input that hopefully, at the end of the process, will yield the minimum possible function value. This makes it the tensor with respect to which we want to compute the function value’s derivative. And that, in turn, means we need to create it with requires_grad = TRUE:\n\nx &lt;- torch_tensor(c(-1, 1), requires_grad = TRUE)\n\nThe starting point, (-1,1), here has been chosen arbitrarily.\nNow, all that remains to be done is apply a small fix to the optimization loop. With autograd enabled on x, torch will record all operations performed on that tensor, meaning that whenever we call backward(), it will compute all required derivatives. However, when we subtract a fraction of the gradient, this is not something we want a derivative to be calculated for! We need to tell torch not to record this action, and that we can do by wrapping it in with_no_grad().\nThere’s one other thing we have to tell it. By default, torch accumulates the gradients stored in grad fields. We need to zero them out for every new calculation, using grad$zero_().\nTaking into account these considerations, the parameter update should look like this:\n\nwith_no_grad({\n  x$sub_(lr * x$grad)\n  x$grad$zero_()\n})\n\nHere is the complete code, enhanced with logging statements that make it easier to see what is going on.\n\nnum_iterations &lt;- 1000\n\nlr &lt;- 0.01\n\nx &lt;- torch_tensor(c(-1, 1), requires_grad = TRUE)\n\nfor (i in 1:num_iterations) {\n  if (i %% 100 == 0) cat(\"Iteration: \", i, \"\\n\")\n\n  value &lt;- rosenbrock(x)\n  if (i %% 100 == 0) {\n    cat(\"Value is: \", as.numeric(value), \"\\n\")\n  }\n\n  value$backward()\n  if (i %% 100 == 0) {\n    cat(\"Gradient is: \", as.matrix(x$grad), \"\\n\")\n  }\n\n  with_no_grad({\n    x$sub_(lr * x$grad)\n    x$grad$zero_()\n  })\n}\n\nIteration:  100 \nValue is:  0.3502924 \nGradient is:  -0.667685 -0.5771312 \n\nIteration:  200 \nValue is:  0.07398106 \nGradient is:  -0.1603189 -0.2532476 \n\nIteration:  300 \nValue is:  0.02483024 \nGradient is:  -0.07679074 -0.1373911 \n\nIteration:  400 \nValue is:  0.009619333 \nGradient is:  -0.04347242 -0.08254051 \n\nIteration:  500 \nValue is:  0.003990697 \nGradient is:  -0.02652063 -0.05206227 \n\nIteration:  600 \nValue is:  0.001719962 \nGradient is:  -0.01683905 -0.03373682 \n\nIteration:  700 \nValue is:  0.0007584976 \nGradient is:  -0.01095017 -0.02221584 \n\nIteration:  800 \nValue is:  0.0003393509 \nGradient is:  -0.007221781 -0.01477957\n\nIteration:  900 \nValue is:  0.0001532408 \nGradient is:  -0.004811743 -0.009894371 \n\nIteration:  1000 \nValue is:  6.962555e-05 \nGradient is:  -0.003222887 -0.006653666 \nAfter thousand iterations, we have reached a function value lower than 0.0001. What is the corresponding (x1,x2)-position?\n\nx\n\ntorch_tensor\n 0.9918\n 0.9830\n[ CPUFloatType{2} ]\nThis is rather close to the true minimum of (1,1). If you feel like, play around a little, and try to find out what kind of difference the learning rate makes. For example, try 0.001 and 0.1, respectively.\nIn the next chapter, we will build a neural network from scratch. There, the function we minimize will be a loss function, namely, the mean squared error arising from a regression problem."
  },
  {
    "objectID": "network_1.html#idea",
    "href": "network_1.html#idea",
    "title": "6  A neural network from scratch",
    "section": "6.1 Idea",
    "text": "6.1 Idea\nIn a nutshell, a network is a function from inputs to outputs. A suitable function, thus, is what we’re looking for.\nTo find it, let’s first think of regression as linear regression. What linear regression does is multiply and add. For each independent variable, there is a coefficient that multiplies it. On top of that, there is a so-called bias term that gets added at the end. (In two dimensions, regression coefficient and bias correspond to slope and x-intercept of the regression line.)\nThinking about it, multiplication and addition are things we can do with tensors – one could even say they are made for exactly that. Let’s take an example where the input data consist of a hundred observations, with three features each. For example:\n\nlibrary(torch)\n\nx &lt;- torch_randn(100, 3)\nx$size()\n\n[1] 100   3\nTo store the per-feature coefficients that should multiply x, we need a column vector of length 3, the number of features. Alternatively, preparing for a modification we’re going to make very soon, this can be a matrix whose columns are of length three, that is, a matrix with three rows. How many columns should it have? Let’s say we want to predict a single output feature. In that case, the matrix should be of size 3 x 1.\nHere comes a suitable candidate, initialized randomly. Note how the tensor is created with requires_grad = TRUE, as it represents a parameter we’ll want the network to learn.\n\nw &lt;- torch_randn(3, 1, requires_grad = TRUE)\n\nThe bias tensor then has to be of size 1 x 1:\n\nb &lt;- torch_zeros(1, 1, requires_grad = TRUE)\n\nNow, we can get a “prediction” by multiplying the data with the weight matrix w and adding the bias b:\n\ny &lt;- x$matmul(w) + b\nprint(y, n = 10)\n\ntorch_tensor\n-2.1600\n-3.3244\n 0.6046\n 0.4472\n-0.4971\n-0.0530\n 5.1259\n-1.1595\n-0.5960\n-1.4584\n... [the output was truncated (use n=-1 to disable)]\n[ CPUFloatType{100,1} ][ grad_fn = &lt;AddBackward0&gt; ]\nIn math notation, what we’ve done here is implement the function:\n\\[\nf(\\mathbf{X}) = \\mathbf{X}\\mathbf{W} + \\mathbf{b}\n\\]\nHow does this relate to neural networks?"
  },
  {
    "objectID": "network_1.html#layers",
    "href": "network_1.html#layers",
    "title": "6  A neural network from scratch",
    "section": "6.2 Layers",
    "text": "6.2 Layers\nCircling back to neural-network terminology, what we’ve done here is prototype the action of a network that has a single layer: the output layer. However, a single-layer network is hardly the type you’d be interested in building – why would you, when you could simply do linear regression instead? In fact, one of the defining features of neural networks is their ability to chain an unlimited (in theory) number of layers. Of these, all but the output layer may be referred to as “hidden” layers, although from the point of view of someone who uses a deep learning framework such as torch, they are not that hidden after all.\nLet’s say we want our network to have one hidden layer. Its size, meaning, the number of units it has, will be an important factor in determining the network’s power. This number is reflected in the weight matrix we create: A layer with eight units will need a weight matrix with eight columns.\n\nw1 &lt;- torch_randn(3, 8, requires_grad = TRUE)\n\nEach unit has its own value for bias, too.\n\nb1 &lt;- torch_zeros(1, 8, requires_grad = TRUE)\n\nJust like we saw before, the hidden layer will multiply the input it receives by the weights and add the bias. That is, it applies the function \\(f\\) displayed above. Then, another function is applied. This function receives its input from the hidden layer and produces the final output. In a nutshell, what is happening here is function composition: Calling the second function \\(g\\), the overall transformation is \\(g(f(\\mathbf{X})\\), or \\(g \\circ f\\).\nFor \\(g\\) to yield an output analogous to the single-layer architecture above, its weight matrix has to take the eight-column hidden layer to a single column. That is, w2 looks like this:\n\nw2 &lt;- torch_randn(8, 1, requires_grad = TRUE)\n\nThe bias, b2, is a single value, like b1:\n\nb2 &lt;- torch_randn(1, 1, requires_grad = TRUE)\n\nOf course, there is no reason to stop at one hidden layer, and once we’ve built up the complete apparatus, please feel invited to experiment with the code. But first, we need to add in a few other types of components. For one, with our most recent architecture, what we’re doing is chain, or compose, functions – which is good. But all these functions are doing is add and multiply, implying that they are linear. The power of neural networks, however, is usually associated with nonlinearity. Why?"
  },
  {
    "objectID": "network_1.html#activation-functions",
    "href": "network_1.html#activation-functions",
    "title": "6  A neural network from scratch",
    "section": "6.3 Activation functions",
    "text": "6.3 Activation functions\nImagine, for a moment, that we had a network with three layers, and all each layer did was multiply its input by its weight matrix. (Having a bias term doesn’t really change anything. But it makes the example more complex, so we’re “abstracting it out”.)\nThis gives us a chain of matrix multiplications: \\(f(\\mathbf{X}) = ((\\mathbf{X} \\mathbf{W}_1)\\mathbf{W}_2)\\mathbf{W}_3\\). Now, this can be rearranged so that all the weight matrices are multiplied together before application to \\(\\mathbf{X}\\): \\(f(\\mathbf{X}) = \\mathbf{X} (\\mathbf{W}_1\\mathbf{W}_2\\mathbf{W}_3)\\). Thus, this three-layer network can be simplified to a single-layer one, where \\(f(\\mathbf{X}) = \\mathbf{X} \\mathbf{W}_4\\). And now, we have lost all advantages associated with deep neural networks.\nThis is where activation functions, sometimes called “nonlinearities”, come in. They introduce non-linear operations that cannot be modeled by matrix multiplication. Historically, the prototypical activation function has been the sigmoid, and it’s still extremely important today. Its constitutive action is to squish its input between zero and one, yielding a value that can be interpreted as a probability. But in regression, this is not usually what we want, and neither would it be for most hidden layers.\nInstead, the most-used activation function inside a network is the so-called ReLU, or Rectified Linear Unit. This is a long name for something rather straightforward: All negative values are set to zero. In torch, this can be accomplished using the relu() function:\n\nt &lt;- torch_tensor(c(-2, 1, 5, -7))\nt$relu()\n\ntorch_tensor\n 0\n 1\n 5\n 0\n[ CPUFloatType{4} ]\nWhy would this be nonlinear? One criterion for a linear function is that when you have two inputs, it doesn’t matter if you first add them and then, apply the transformation, or if you start by applying the transformation independently to both inputs and then, go ahead and add them. But with ReLU, this does not work:\n\nt1 &lt;- torch_tensor(c(1, 2, 3))\nt2 &lt;- torch_tensor(c(1, -2, 3))\n\nt1$add(t2)$relu()\n\ntorch_tensor\n 2\n 0\n 6\n[ CPUFloatType{3} ]\n\nt1_clamped &lt;- t1$relu()\nt2_clamped &lt;- t2$relu()\n\nt1_clamped$add(t2_clamped)\n\ntorch_tensor\n 2\n 2\n 6\n[ CPUFloatType{3} ]\nThe results are not the same.\nWrapping up so far, we’ve talked about how to code layers and activation functions. There is just one further concept to discuss before we can build the complete network. This is the loss function."
  },
  {
    "objectID": "network_1.html#loss-functions",
    "href": "network_1.html#loss-functions",
    "title": "6  A neural network from scratch",
    "section": "6.4 Loss functions",
    "text": "6.4 Loss functions\nPut abstractly, the loss is a measure of how far away we are from our goal. When minimizing a function, like we did in the previous chapter, this is the difference between the current function value and the smallest value it can take. With neural networks, we are free to choose a suitable loss function as we like, provided it matches our task. For regression-type tasks, this often will be mean squared error (MSE), although it doesn’t have to be. For example, there could be reasons to use mean absolute error instead.\nIn torch, computation of mean squared error is a one-liner:\n\ny &lt;- torch_randn(5)\ny_pred &lt;- y + 0.01\n\nloss &lt;- (y_pred - y)$pow(2)$mean()\n\nloss\n\ntorch_tensor\n9.99999e-05\n[ CPUFloatType{} ]\nAs soon as we have the loss, we’ll be able to update the weights, subtracting a fraction of its gradient. We’ve already seen how to do this in the last chapter, and will see it again shortly.\nWe now take the pieces discussed and put them together."
  },
  {
    "objectID": "network_1.html#implementation",
    "href": "network_1.html#implementation",
    "title": "6  A neural network from scratch",
    "section": "6.5 Implementation",
    "text": "6.5 Implementation\nWe split this into three parts. This way, when later we refactor individual components to make use of higher-level torch functionality, it will be easier to see the areas where encapsulation and modularization are occurring.\n\n6.5.1 Generate random data\nOur example data consist of one hundred observations. The input, x, has three features; the target, y, just one. y is generated from x, but with some noise added.\n\nlibrary(torch)\n\n# input dimensionality (number of input features)\nd_in &lt;- 3\n# number of observations in training set\nn &lt;- 100\n\nx &lt;- torch_randn(n, d_in)\ncoefs &lt;- c(0.2, -1.3, -0.5)\ny &lt;- x$matmul(coefs)$unsqueeze(2) + torch_randn(n, 1)\n\nNext, the network.\n\n\n6.5.2 Build the network\nThe network has two layers: a hidden layer and the output layer. This means that we need two weight matrices and two bias tensors. For no special reason, the hidden layer here has thirty-two units:\n\n# dimensionality of hidden layer\nd_hidden &lt;- 32\n# output dimensionality (number of predicted features)\nd_out &lt;- 1\n\n# weights connecting input to hidden layer\nw1 &lt;- torch_randn(d_in, d_hidden, requires_grad = TRUE)\n# weights connecting hidden to output layer\nw2 &lt;- torch_randn(d_hidden, d_out, requires_grad = TRUE)\n\n# hidden layer bias\nb1 &lt;- torch_zeros(1, d_hidden, requires_grad = TRUE)\n# output layer bias\nb2 &lt;- torch_zeros(1, d_out, requires_grad = TRUE)\n\nWith their current values – results of random initialization – those weights and biases won’t be of much use. Time to train the network.\n\n\n6.5.3 Train the network\nTraining the network means passing the input through its layers, calculating the loss, and adjusting the parameters (weights and biases) in a way that predictions improve. These activities we keep repeating until performance seems sufficient (which, in real-life applications, would have to be defined very carefully). Technically, each repeated application of these steps is called an epoch.\nJust like with function minimization, deciding on a suitable learning rate (the fraction of the gradient to subtract) needs some experimentation.\nLooking at the below training loop, you see that, logically, it consists of four parts:\n\ndo a forward pass, yielding the network’s predictions (if you dislike the one-liner, feel free to split it up);\ncompute the loss (this, too, being a one-liner – we merely added some logging);\nhave autograd calculate the gradient of the loss with respect to the parameters; and\nupdate the parameters accordingly (again, taking care to wrap the whole action in with_no_grad(), and zeroing the grad fields on every iteration).\n\n\nlearning_rate &lt;- 1e-4\n\n### training loop ----------------------------------------\n\nfor (t in 1:200) {\n  \n  ### -------- Forward pass --------\n  \n  y_pred &lt;- x$mm(w1)$add(b1)$relu()$mm(w2)$add(b2)\n  \n  ### -------- Compute loss -------- \n  loss &lt;- (y_pred - y)$pow(2)$mean()\n  if (t %% 10 == 0)\n    cat(\"Epoch: \", t, \"   Loss: \", loss$item(), \"\\n\")\n  \n  ### -------- Backpropagation --------\n  \n  # compute gradient of loss w.r.t. all tensors with\n  # requires_grad = TRUE\n  loss$backward()\n  \n  ### -------- Update weights -------- \n  \n  # Wrap in with_no_grad() because this is a part we don't \n  # want to record for automatic gradient computation\n   with_no_grad({\n     w1 &lt;- w1$sub_(learning_rate * w1$grad)\n     w2 &lt;- w2$sub_(learning_rate * w2$grad)\n     b1 &lt;- b1$sub_(learning_rate * b1$grad)\n     b2 &lt;- b2$sub_(learning_rate * b2$grad)  \n     \n     # Zero gradients after every pass, as they'd\n     # accumulate otherwise\n     w1$grad$zero_()\n     w2$grad$zero_()\n     b1$grad$zero_()\n     b2$grad$zero_()  \n   })\n\n}\n\nEpoch: 10 Loss: 24.92771\nEpoch: 20 Loss: 23.56143\nEpoch: 30 Loss: 22.3069\nEpoch: 40 Loss: 21.14102\nEpoch: 50 Loss: 20.05027\nEpoch: 60 Loss: 19.02925\nEpoch: 70 Loss: 18.07328\nEpoch: 80 Loss: 17.16819\nEpoch: 90 Loss: 16.31367\nEpoch: 100 Loss: 15.51261\nEpoch: 110 Loss: 14.76012\nEpoch: 120 Loss: 14.05348\nEpoch: 130 Loss: 13.38944\nEpoch: 140 Loss: 12.77219\nEpoch: 150 Loss: 12.19302\nEpoch: 160 Loss: 11.64823\nEpoch: 170 Loss: 11.13535\nEpoch: 180 Loss: 10.65219\nEpoch: 190 Loss: 10.19666\nEpoch: 200 Loss: 9.766989\nThe loss decreases quickly at first, and then, not so rapidly anymore. But this example was not created to exhibit magnificent performance; the idea was to show how few lines of code are needed to build a “real” neural network.\nNow, the layers, the loss, the parameter updates – all that is still pretty “raw”: It’s (literally) just tensors. For such a small network this works fine, but it would get cumbersome pretty fast for more complex designs. The following two chapters, thus, will show how to abstract away weights and biases into neural network modules, swap self-made loss functions with built-in ones, and get rid of the verbose parameter update routine."
  },
  {
    "objectID": "modules.html#built-in-nn_modules",
    "href": "modules.html#built-in-nn_modules",
    "title": "7  Modules",
    "section": "7.1 Built-in nn_module()s",
    "text": "7.1 Built-in nn_module()s\nIn torch, a linear layer is created using nn_linear(). nn_linear() expects (at least) two arguments: in_features and out_features. Let’s say your input data has fifty observations with five features each; that is, it is of size 50 x 5. You want to build a hidden layer with sixteen units. Then in_features is 5, and out_features is 16. (The same 5 and 16 would constitute the number of rows/columns in the weight matrix if you built one yourself.)\n\nlibrary(torch)\nl &lt;- nn_linear(in_features = 5, out_features = 16)\n\nOnce created, the module readily informs you about its parameters:\n\nl\n\nAn `nn_module` containing 96 parameters.\nParameters\n weight: Float [1:16, 1:5]\n bias: Float [1:16]\nEncapsulation doesn’t keep us from inspecting the weight and bias tensors:\n\nl$weight\n\ntorch_tensor\n-0.2079 -0.1920  0.2926  0.0036 -0.0897\n 0.3658  0.0076 -0.0671  0.3981 -0.4215\n 0.2568  0.3648 -0.0374 -0.2778 -0.1662\n 0.4444  0.3851 -0.1225  0.1678 -0.3443\n-0.3998  0.0207 -0.0767  0.4323  0.1653\n 0.3997  0.0647 -0.2823 -0.1639 -0.0225\n 0.0479  0.0207 -0.3426 -0.1567  0.2830\n 0.0925 -0.4324  0.0448 -0.0039  0.1531\n-0.2924 -0.0009 -0.1841  0.2028  0.1586\n-0.3064 -0.4006 -0.0553 -0.0067  0.2575\n-0.0472  0.1238 -0.3583  0.4426 -0.0269\n-0.0275 -0.0295 -0.2687  0.2236  0.3787\n-0.2617 -0.2221  0.1503 -0.0627  0.1094\n 0.0122  0.2041  0.4466  0.4112  0.4168\n-0.4362 -0.3390  0.3679 -0.3045  0.1358\n 0.2979  0.0023  0.0695 -0.1906 -0.1526\n[ CPUFloatType{16,5} ]\n\nl$bias\n\ntorch_tensor\n-0.2314\n 0.2942\n 0.0567\n-0.1728\n-0.3220\n-0.1553\n-0.4149\n-0.2103\n-0.1769\n 0.4219\n-0.3368\n 0.0689\n 0.3625\n-0.1391\n-0.1411\n-0.2014\n[ CPUFloatType{16} ]\nAt this point, I need to ask for your indulgence. You’ve probably noticed that torch reports the weight matrix as being of size 16 x 5, not 5 x 16, like we said you’d create it when coding from scratch. This is due to an implementation detail inherited from the underlying C++ implementation, libtorch. For performance reasons, libtorch’s linear module stores the weight and bias tensors in transposed form. On the R side, all we can do is explicitly point you to it and thereby, hopefully, alleviate the confusion.\nLet’s go on. To apply this module to input data, just “call” it like a function:\n\nx &lt;- torch_randn(50, 5)\noutput &lt;- l(x)\noutput$size()\n\n[1] 50 16\nSo that’s the forward pass. How about gradient computation? Previously, when creating a tensor we wanted to figure as a “source” in gradient computation, we had to let torch know explicitly, passing requires_grad = TRUE. No such thing is required for built-in nn_module()s. We can immediately check that output knows what to do on backward():\n\noutput$grad_fn\n\nAddmmBackward0\nTo be sure though, let’s calculate some “dummy” loss based on output, and call backward(). We see that now, the linear module’s weight tensor has its grad field populated:\n\nloss &lt;- output$mean()\nloss$backward()\nl$weight$grad\n\ntorch_tensor\n0.01 *\n-0.3064  2.4118 -0.6095  0.3419 -1.6131\n -0.3064  2.4118 -0.6095  0.3419 -1.6131\n -0.3064  2.4118 -0.6095  0.3419 -1.6131\n -0.3064  2.4118 -0.6095  0.3419 -1.6131\n -0.3064  2.4118 -0.6095  0.3419 -1.6131\n -0.3064  2.4118 -0.6095  0.3419 -1.6131\n -0.3064  2.4118 -0.6095  0.3419 -1.6131\n -0.3064  2.4118 -0.6095  0.3419 -1.6131\n -0.3064  2.4118 -0.6095  0.3419 -1.6131\n -0.3064  2.4118 -0.6095  0.3419 -1.6131\n -0.3064  2.4118 -0.6095  0.3419 -1.6131\n -0.3064  2.4118 -0.6095  0.3419 -1.6131\n -0.3064  2.4118 -0.6095  0.3419 -1.6131\n -0.3064  2.4118 -0.6095  0.3419 -1.6131\n -0.3064  2.4118 -0.6095  0.3419 -1.6131\n -0.3064  2.4118 -0.6095  0.3419 -1.6131\n[ CPUFloatType{16,5} ]\nThus, once you work with nn_modules, torch automatically assumes that you’ll want gradients computed.\nnn_linear(), straightforward though it may be, is an essential building block encountered in most every model architecture. Others include:\n\nnn_conv1d(), nn_conv2d(), and nn_conv3d(), the so-called convolutional layers that apply filters to input data of varying dimensionality,\nnn_lstm() and nn_gru() , the recurrent layers that carry through a state,\nnn_embedding() that is used to embed categorical data in high-dimensional space,\nand more."
  },
  {
    "objectID": "modules.html#building-up-a-model",
    "href": "modules.html#building-up-a-model",
    "title": "7  Modules",
    "section": "7.2 Building up a model",
    "text": "7.2 Building up a model\nThe built-in nn_module()s give us layers, in usual speak. How do we combine those into models? Using the “factory function” nn_module(), we can define models of arbitrary complexity. But we may not always need to go that way.\n\n7.2.1 Models as sequences of layers: nn_sequential()index{nn_sequential()}\nIf all our model should do is propagate straight through the layers, we can use nn_sequential() to build it. Models consisting of all linear layers are known as Multi-Layer Perceptronsindex{Multi-Layer Perceptron (MLP)} (MLPs). Here is one:\n\nmlp &lt;- nn_sequential(\n  nn_linear(10, 32),\n  nn_relu(),\n  nn_linear(32, 64),\n  nn_relu(),\n  nn_linear(64, 1)\n)\n\nTake a close look at the layers involved. We’ve already seen nnf_relu(), the function that implements ReLU activation. (The f in nnf_ stands for functional.) Below, nn_relu, like nn_linear(), is a module, that is, an object. This is because nn_sequential() expects all its arguments to be modules.\nJust like the built-in modules, you can apply this model to data by just calling it:\n\nmlp(torch_randn(5, 10))\n\ntorch_tensor\n0.01 *\n-7.8097\n -9.0363\n -38.3282\n  5.3959\n -16.4837\n[ CPUFloatType{5,1} ][ grad_fn = &lt;AddmmBackward0&gt; ]\nThe single call triggered a complete forward pass through the network. Analogously, calling backward() will back-propagate through all the layers.\nWhat if you need the model to chain execution steps in a non-sequential way?\n\n\n7.2.2 Models with custom logic\nAs already hinted at, this is where you use nn_module().\nnn_module() creates constructors for custom-made R6 objects. Below, my_linear() is such a constructor. When called, it will return a linear module similar to the built-in nn_linear().\nTwo methods should be implemented in defining a constructor: initialize() and forward(). initialize() creates the module object’s fields, that is, the objects or values it “owns” and can access from inside any of its methods. forward() defines what should happen when the module is called on the input:\n\nmy_linear &lt;- nn_module(\n  initialize = function(in_features, out_features) {\n    self$w &lt;- nn_parameter(torch_randn(\n      in_features, out_features\n    ))\n    self$b &lt;- nn_parameter(torch_zeros(out_features))\n  },\n  forward = function(input) {\n    input$mm(self$w) + self$b\n  }\n)\n\nNote the use of nn_parameter(). nn_parameter() makes sure that the passed-in tensor is registered as a module parameter, and thus, is subject to backpropagation by default.\nTo instantiate the newly-defined module, call its constructor:\n\nl &lt;- my_linear(7, 1)\nl\n\nAn `nn_module` containing 8 parameters.\n\nParameters ────────────────────────────────────────────────────────────────────────────────────────────\n● w: Float [1:7, 1:1]\n● b: Float [1:1]\nGranted, in this example, there really is no custom logic we needed to define our own module for. But here, you have a template applicable to any use case. Later, we’ll see definitions of initialize() and forward() that are more complex, and we’ll encounter additional methods defined on modules. But the basic mechanism will remain the same.\nAt this point, you may feel like you’d like to rewrite last chapter’s neural network using modules. Feel free to do so! Or maybe wait until, in the next chapter, we’ll have learned about optimizers, and built-in loss functions. Once we’re done, we’ll return to our two examples, function minimization and the regression network. Then, we’ll be removing all do-it-yourself pieces rendered superfluous by torch."
  },
  {
    "objectID": "optimizers.html#why-optimizers",
    "href": "optimizers.html#why-optimizers",
    "title": "8  Optimizers",
    "section": "8.1 Why optimizers?",
    "text": "8.1 Why optimizers?\nTo this question, there are two main types of answer. First, the technical one.\nIf you look back at how we coded our first neural network, you’ll see that we proceeded like this:\n\ncompute predictions (forward pass),\ncalculate the loss,\nhave autograd compute partial derivatives (calling loss$backward()), and\nupdate the parameters, subtracting from each some fraction of the gradient.\n\nHere is how that last part looked:\n\nlibrary(torch)\n\n# compute gradient of loss w.r.t. all tensors with\n# requires_grad = TRUE\nloss$backward()\n  \n### -------- Update weights -------- \n  \n# Wrap in with_no_grad() because this is a part we don't \n# want to record for automatic gradient computation\nwith_no_grad({\n  w1 &lt;- w1$sub_(learning_rate * w1$grad)\n  w2 &lt;- w2$sub_(learning_rate * w2$grad)\n  b1 &lt;- b1$sub_(learning_rate * b1$grad)\n  b2 &lt;- b2$sub_(learning_rate * b2$grad)  \n     \n  # Zero gradients after every pass, as they'd accumulate\n  # otherwise\n  w1$grad$zero_()\n  w2$grad$zero_()\n  b1$grad$zero_()\n  b2$grad$zero_()  \n})\n\nNow this was a small network – imagine having to code such logic for architectures with tens or hundreds of layers! Surely this can’t be what developers of a deep learning framework want their users to do. Accordingly, weight updates are taken care of by specialized objects – the optimizers in question.\nThus, the technical type of answer concerns usability and convenience. But more is involved. With the above approach, there’s hardly a way to find a good learning rate other than by trial and error. And most probably, there is not even an optimal learning rate that would be constant over the whole training process. Fortunately, a rich tradition of research has turned up at set of proven update strategies. These strategies commonly involve a state kept between operations. This is another reason why, just like modules, optimizers are objects in torch.\nBefore we look deeper at these strategies, let’s see how we’d replace the above manual weight-updating process with a version that uses an optimizer."
  },
  {
    "objectID": "optimizers.html#using-built-in-torch-optimizers",
    "href": "optimizers.html#using-built-in-torch-optimizers",
    "title": "8  Optimizers",
    "section": "8.2 Using built-in torch optimizers",
    "text": "8.2 Using built-in torch optimizers\nAn optimizer needs to know what it’s supposed to optimize. In the context of a neural network model, this will be the network’s parameters. With no real difference between “model modules” and “layer modules”, however, we can demonstrate how it works using a single built-in module such as nn_linear().\nHere we instantiate a gradient descent optimizer designed to work on some linear module’s parameters:\n\nl &lt;- nn_linear(10, 2)\n\nopt &lt;- optim_sgd(l$parameters, lr = 0.1)\n\nIn addition to the always-required reference to what tensors should be optimized, optim_sgd() has just a single non-optional parameter: lr, the learning rate.\nOnce we have an optimizer object, parameter updates are triggered by calling its step() method. One thing remains unchanged, though. We still need to make sure gradients are not accumulated over training iterations. This means we still call zero_grad() – but this time, on the optimizer object.\nThis is the complete code replacing the above manual procedure:\n\n# compute gradient of loss w.r.t. all tensors with\n# requires_grad = TRUE\n# no change here\nloss$backward()\n\n# Still need to zero out gradients before the backward pass,\n# only this time, on the optimizer object\noptimizer$zero_grad()\n\n# use the optimizer to update model parameters\noptimizer$step()\n\nI’m sure you’ll agree that usability-wise, this is an enormous improvement. Now, let’s get back to our original question – why optimizers? – and talk more about the second, strategic part of the answer."
  },
  {
    "objectID": "optimizers.html#parameter-update-strategies",
    "href": "optimizers.html#parameter-update-strategies",
    "title": "8  Optimizers",
    "section": "8.3 Parameter update strategies",
    "text": "8.3 Parameter update strategies\nSearching for a good learning rate by trial and error is costly. And the learning rate isn’t even the only thing we’re uncertain about. All it does is specify how big of a step to take. However, that’s not the only unresolved question.\nSo far, we’ve always assumed that the direction of steepest descent, as given by the gradient, is the best way to go. This is not always the case, though. So we are left with uncertainties regarding both magnitude and direction of parameter updates.\nFortunately, over the last decade, there has been significant progress in research related to weight updating in neural networks. Here, we take a look at major considerations involved, and situate in context some of the most popular optimizers provided by torch.\nThe baseline to compare against is gradient descent, or steepest descent, the algorithm we’ve been using in our manual implementations of function minimization and neural-network training. Let’s quickly recall the guiding principle behind it.\n\n8.3.1 Gradient descent (a.k.a. steepest descent, a.k.a. stochastic gradient descent (SGD))\nThe gradient – the vector of partial derivatives, one for each input feature – indicates the direction in which a function increases most. Going in the opposite direction means we descend the fastest way possible. Or does it?\nUnfortunately, it is not that simple. It depends on the landscape that surrounds us, or put more technically, the contours of the function we want to minimize. To illustrate, compare two situations.\nThe first is the one we encountered when first learning about automatic differentiation. The example there was a quadratic function in two dimensions. We didn’t make a great deal out of it at the time, but an important point about this specific function was that the slope was the same in both dimensions. Under such conditions, steepest descent is optimal.\nLet’s verify that. The function was : \\(f(x_1, x_2) = 0.2 {x_1}^2 + 0.2 {x_2}^2 - 5\\), and its gradient, \\(\\begin{bmatrix}0.4\\\\0.4 \\end{bmatrix}\\). Now say we’re at point \\((x1, x2) = (6,6)\\). For each coordinate, we subtract 0.4 times its current value. Or rather, that would be if we had to use a learning rate of 1. But we don’t have to. If we pick a learning rate of 2.5, we can arrive at the minimum in a single step: \\((x_1, x_2) = (6 - 2.5*0.4*6, 6 - 2.5*0.4*6) = (0,0)\\). See below for an illustration of what happens in each case (fig. 8.1).\n\n\n\nFigure 8.1: Steepest descent on an isotropic paraboloid, using different learning rates.\n\n\nIn a nutshell, thus, with a isotropic function like this – the variance being the same in both directions – it is “just” a matter of getting the learning rate right.\nNow compare this to what happens if slopes in both directions are decidedly distinct.\nThis time, the coefficient for \\(x_2\\) is ten times as big as that for \\(x_1\\): We have \\(f(x_1, x_2) = 0.2 {x_1}^2 + 2 {x_2}^2 - 5\\). This means that as we progress in the \\(x_2\\) direction, the function value increases sharply, while in the \\(x_1\\) direction, it rises much more slowly. Thus, during gradient descent, we make far greater progress in one direction than the other.\nAgain, we investigate what happens for different learning rates. Below, we contrast three different settings. With the lowest learning rate, the process eventually reaches the minimum, but a lot more slowly than in the symmetric case. With a learning rate just slightly higher, descent gets lost in endless zig-zagging, oscillating between positive and negative values of the more influential variable, \\(x_2\\). Finally, a learning rate that, again, is just minimally higher, has a catastrophic effect: The function value explodes, zig-zagging up right to infinity (fig. 8.2).\n\n\n\nFigure 8.2: Steepest descent on a non-isotropic paraboloid, using (minimally!) different learning rates.\n\n\nThis should be pretty convincing – even with a pretty conventional function of just two variables, steepest descent is far from being a panacea! And in deep learning, loss functions will be a lot less well-behaved. This is where the need for more sophisticated algorithms arises: Enter – again – optimizers.\n\n\n8.3.2 Things that matter\nViewed conceptually, major modifications to steepest descent can be categorized by the considerations that drive them, or equivalently, by the problems they’re trying to solve. Here, we focus on three such considerations.\nFirst, instead of starting in a completely new direction every time we re-compute the gradient, we might want to keep a bit of the old direction – keep momentum, to use the technical term. This should help avoiding the inefficient zig-zagging seen in the example above.\nSecond, looking back at just that example of minimizing a non-symmetric function … Why, really, should we be constrained to using the same learning rate for all variables? When it’s evident that all variables don’t vary to the same degree, why don’t we update them in individually appropriate ways?\nThird – and this is a fix for problems that only arise once you’ve taken actions to reduce the learning rate for overly-impactful features – you also want to make sure that learning still progresses, that parameters still get updated.\nThese considerations are nicely illustrated by a few classics among the optimization algorithms.\n\n\n8.3.3 Staying on track: Gradient descent with momentum\nIn gradient descent with momentum, we don’t directly use the gradient to update the weights. Instead, you can picture weight updates as particles moving on a trajectory: They want to keep going in whatever direction they’re going – keep their momentum, in physics speak – but get continually deflected by collisions. These “collisions” are friendly nudges to, please, keep into account the gradient at the now current position. These dynamics result in a two-step update logic.\nIn the below formulas, the choice of symbols reflects the physical analogy. \\(\\mathbf{x}\\) is the position, “where we’re at” in parameter space – or more simply, the current values of the parameters. Time evolution is captured by superscripts, with \\(\\mathbf{y}^{(k)}\\) representing the state of variable \\(\\mathbf{y}\\) at the current time, \\(k\\). The instantaneous velocity at time \\(k\\) is just what is measured by the gradient, \\(\\mathbf{g}^{(k)}\\). But in updating position, we won’t directly make use of it. Instead, at each iteration, the update velocity is a combination of old velocity – weighted by momentum parameter \\(m\\) – and the freshly-computed gradient (weighted by the learning rate). Step one of the two-step logic captures this strategy:\n\\[\n\\mathbf{v}^{(k+1)} = m \\ \\mathbf{v}^{(k)} + lr \\ \\mathbf{g}^{(k)}\n\\tag{8.1}\\]\nThe second step then is the update of \\(\\mathbf{x}\\) due to this “compromise” velocity \\(\\mathbf{v}\\).\n\\[\n\\mathbf{x}^{(k+1)} = \\mathbf{x}^{(k)} - \\mathbf{v}^{(k+1)}\n\\tag{8.2}\\]\nBesides the physics analogy, there is another one you may find useful, one that makes use of a concept prominent in time series analysis. If we choose \\(m\\) and \\(lr\\) such that they add up to 1, the result is an exponentially weighted moving average. (While this conceptualization, I think, helps understanding, in practice there is no necessity to have \\(m\\) and \\(lr\\) summing to 1, though).\nNow, let’s return to the non-isotropic paraboloid, and compare SGD with and without momentum. For the latter (bright curve), I’m using a combination of \\(lr = 0.5\\) and \\(mu = 0.1\\). For SGD – dark curve – the learning rate is the “good one” from the figure above.Definitely, SGD with momentum requires far fewer steps to reach the minimum (fig. 8.3).\n\n\n\nFigure 8.3: SGD with momentum (white), compared with vanilla SGD (gray).\n\n\n\n\n8.3.4 Adagrad\nCan we do better yet? Now, we know that in our running example, it is really the fact that one feature changes much faster than the other that slows down optimization. Having separate learning rates per parameter thus clearly seems like a thing we want. In fact, most of the optimizers popular in deep learning have per-parameter learning rates. But how would you actually determine those?\nThis is where different algorithms differ. Adagrad, for example, divides each parameter update by the cumulative sum of its partial derivatives (squared, to be precise), where “cumulative” means we’re keeping track of them since the very first iteration. If we call that “accumulator variable” \\(s\\), refer to the parameter in question by \\(i\\), and count iterations using \\(k\\), this gives us the following formula for keeping \\(s\\) updated:\n\\[\ns_i^{(k)} = \\sum_{j=1}^k (g_i^{(j)})^2\n\\tag{8.3}\\]\n(By the way, feel free to skip over the formulas if you don’t like them. I’m doing my best to communicate what they do in words, so you shouldn’t miss out on essential information.)\nNow, the update rule for each parameter subtracts a portion of the gradient, as did vanilla steepest descent – but this time, that portion is determined not just by the (global) learning rate, but also, by the aforementioned cumulative sum of squared partials. The bigger that sum – that is, the bigger the gradients have been during training – the smaller the adjustment:1\n\\[\nx_i^{(k+1)} = x_i^{(k)} - \\frac{lr}{\\epsilon + \\sqrt{s_i^{(k)}}}\\ g_i^{(k)}\\\\\n\\tag{8.4}\\]\nThe net effect of this strategy is that, if a parameter has consistently high gradients, its influence is played down. Parameters with, habitually, tiny gradients, on the other hand, can be sure to receive a lot of attention once that changes.\nWith this algorithm, the global learning rate, \\(lr\\), is of lesser importance. In our running example, it turns out that for best results, we can (and should) use a very high learning rate: 3.7! Here (fig. 8.4) is the result, again comparing with vanilla gradient descent (gray curve):\n\n\n\nFigure 8.4: Adagrad (white), compared with vanilla SGD (gray).\n\n\nIn our example, thus, Adagrad performs excellently. But in training a neural network, we tend to run a lot of iterations. Then, with the way gradients are accumulated, the effective learning rate decreases more and more, and a dead end is reached.\nAre there other ways to have individual, per-parameter learning rates?\n\n\n8.3.5 RMSProp\nRMSProp replaces the cumulative-gradient strategy found in Adagrad with a weighted-average one. At each point, the “bookkeeping”, per-parameter variable \\(s_i\\) is a weighted average of its previous value and the previous (squared) gradient:\n\\[\ns_i^{(k+1)} = \\gamma \\ s_i^{(k)} + (1-\\gamma) \\ (g_i^{(k)})^2\n\\tag{8.5}\\]\nThe update then looks as with Adagrad:\n\\[\nx_i^{(k+1)} = x_i^{(k)} - \\frac{lr}{\\epsilon + \\sqrt{s_i^{(k)}}}\\ g_i^{(k)}\\\\\n\\tag{8.6}\\]\nIn this way, each parameter update gets weighted appropriately, without learning slowing down overall.\nHere is the result, again compared against the SGD baseline (fig. 8.5):\n\n\n\nFigure 8.5: RMSProp (white), compared with vanilla SGD (gray).\n\n\nAs of today, RMSProp is one of the most-often used optimizers in deep learning, with probably just Adam - to be introduced next – being more popular.\n\n\n8.3.6 Adam\nAdam combines two concepts we’ve already seen: momentum – to keep “on track” – and parameter-dependent updates, to avoid excessive dependence on fast-changing parameters. The logic is like this.2\nFor one, just like in SGD with momentum, we keep an exponentially weighted average of gradients. Here the weighting coefficient, \\(\\gamma_v\\), is usually set to 0.9.\n\\[\nv_i^{(k+1)} = \\gamma_v \\ v_i^{(k)} + (1-\\gamma_v) \\ g_i^{(k)}\n\\tag{8.7}\\]\nAlso, like in RMSProp, there is an exponentially weighted average of squared gradients, with weighting coefficient \\(\\gamma_s\\) usually set to 0.999.\n\\[\ns_i^{(k+1)} = \\gamma_s \\ s_i^{(k)} + (1-\\gamma_s) \\ (g_i^{(k)})^2\n\\tag{8.8}\\]\nThe parameter updates now make use of that information in the following way. The velocity determines the direction of the update, while both velocity and magnitude of gradients (together with the learning rate, \\(lr\\)) determine its size:\n\\[\nx_i^{(k+1)} = x_i^{(k)} - \\frac{lr \\\nv_i^{(k+1)}}{\\epsilon + \\sqrt{s_i^{(k+1)}}}\\ \\\\\n\\tag{8.9}\\]\nLet’s conclude this chapter by testing Adam on our running example (fig. 8.6).\n\n\n\nFigure 8.6: Adam (white), compared with vanilla SGD (gray).\n\n\nNext, we head on to loss functions, the last building block to look at before we re-factor the regression network and function minimization examples to benefit from torch modules and optimizers."
  },
  {
    "objectID": "optimizers.html#footnotes",
    "href": "optimizers.html#footnotes",
    "title": "8  Optimizers",
    "section": "",
    "text": "Here \\(\\epsilon\\) is just a tiny value added to avoid division by zero.↩︎\nActual implementations usually contain an additional step, but there is no need to go into details here.↩︎"
  },
  {
    "objectID": "loss_functions.html#torch-loss-functions",
    "href": "loss_functions.html#torch-loss-functions",
    "title": "9  Loss functions",
    "section": "9.1 torch loss functions",
    "text": "9.1 torch loss functions\nIn torch, loss functions start with nn_ or nnf_.\nUsing nnf_, you directly call a function. Correspondingly, its arguments (estimate and target) both are tensors. For example, here is nnf_mse_loss(), the built-in analog to what we coded manually:\n\nnnf_mse_loss(torch_ones(2, 2), torch_zeros(2, 2) + 0.1)\n\ntorch_tensor\n0.81\n[ CPUFloatType{} ]\nWith nn_, in contrast, you create an object:\n\nl &lt;- nn_mse_loss()\n\nThis object can then be called on tensors to yield the desired loss:\n\nl(torch_ones(2, 2),torch_zeros(2, 2) + 0.1)\n\ntorch_tensor\n0.81\n[ CPUFloatType{} ]\nWhether to choose object or function is mainly a matter of preference and context. In larger models, you may end up combining several loss functions, and then, creating loss objects can result in more modular, and more maintainable code. In this book, I’ll mainly use the first way, unless there are compelling reasons to do otherwise.\nOn to the second question."
  },
  {
    "objectID": "loss_functions.html#what-loss-function-should-i-choose",
    "href": "loss_functions.html#what-loss-function-should-i-choose",
    "title": "9  Loss functions",
    "section": "9.2 What loss function should I choose?",
    "text": "9.2 What loss function should I choose?\nIn deep learning, or machine learning overall, most applications aim to do one (or both) of two things: predict a numerical value, or estimate a probability. The regression task of our running example does the former; real-world applications might forecast temperatures, infer employee churn, or predict sales. In the second group, the prototypical task is classification. To categorize, say, an image according to its most salient content, we really compute the respective probabilities. Then, when the probability for “dog” is 0.7, while that for “cat” is 0.3, we say it’s a dog.\n\n9.2.1 Maximum likelihood\nIn both classification and regression, the mostly used loss functions are built on the maximum likelihood principle. Maximum likelihood means: We want to choose model parameters in a way that the data, the things we have observed or could have observed, are maximally likely. This principle is not “just” fundamental, it is also intuitively appealing. Imagine a simple example.\nSay we have the values 7.1, 22.14, and 11.3, and we know that the underlying process follows a normal distribution. Then it is much more likely that these data have been generated by a distribution with mean 14 and standard deviation 7 than by one with mean 20 and standard deviation 1.\n\n\n9.2.2 Regression\nIn regression (that implicitly assumes the target distribution to be normal1), to maximize likelihood, we just keep using mean squared error – the loss we’ve been computing all along. Maximum likelihood estimators have all kinds of desirable statistical properties. However, in concrete applications, there may be reasons to use different ones.\nFor example, say a dataset has outliers where, for some reason, prediction and target are found to be deviating substantially. Mean squared error will allocate high importance to these outliers. In such cases, possible alternatives are mean absolute error (nnf_l1_loss()) and smooth L1 loss (nn_smooth_l1_loss()). The latter is a mixture type that, by default, computes the absolute (L1) error, but switches to squared (L2) error whenever the absolute errors get very small.\n\n\n9.2.3 Classification\nIn classification, we are comparing two distributions. The estimate is a probability by design, and the target can be viewed as one, too. In that light, maximum likelihood estimation is equivalent to minimizing the Kullback-Leibler divergence (KL divergence).\nKL divergence is a measure of how two distributions differ. It depends on two things: the likelihood of the data, as determined by some data-generating process, and the likelihood of the data under the model. In the machine learning scenario, however, we are concerned only with the latter. In that case, the criterion to be minimized reduces to the cross-entropy between the two distributions. And cross-entropy loss is exactly what is commonly used in classification tasks.\nIn torch, there are several variants of loss functions that calculate cross-entropy. With this topic, it’s nice to have a quick reference around; so here is a quick lookup table (tbl. 9.1 abbreviates the – rather long-ish – function names; see tbl. 9.2 for the mapping):\n\n\nTable 9.1: Loss functions, by type of data they work on (binary vs. multi-class) and expected input (raw scores, probabilities, or log probabilities).\n\n\n\n\n\n\n\n\n\n\n\nData\n\nInput\n\n\n\n\n\nbinary\nmulti-class\nraw scores\nprobabilities\nlog probs\n\n\nBCeL\nY\n\nY\n\n\n\n\nCe\n\nY\nY\n\n\n\n\nBCe\nY\n\n\nY\n\n\n\nNll\n\nY\n\n\nY\n\n\n\n\n\n\nTable 9.2: Abbreviations used to refer to torch loss functions.\n\n\nBCeL\nnnf_binary_cross_entropy_with_logits()\n\n\nCe\nnnf_cross_entropy()\n\n\nBCe\nnnf_binary_cross_entropy()\n\n\nNll\nnnf_nll_loss()\n\n\n\n\nTo pick the function applicable to your use case, there are two things to consider.\nFirst, are there just two possible classes (“dog vs. cat”, “person present / person absent”, etc.), or are there several?\nAnd second, what is the type of the estimated values? Are they raw scores (in theory, any value between plus and minus infinity)? Are they probabilities (values between 0 and 1)? Or (finally) are they log probabilities, that is, probabilities to which a logarithm has been applied? (In the final case, all values should be either negative or equal to zero.)\n\n9.2.3.1 Binary data\nStarting with binary data, our example classification vector is a sequence of zeros and ones. When thinking in terms of probabilities, it is most intuitive to imagine the ones standing for presence, the zeros for absence of one of the classes in question – cat or no cat, say.\n\ntarget &lt;- torch_tensor(c(1, 0, 0, 1, 1))\n\nThe raw scores could be anything. For example:\n\nunnormalized_estimate &lt;-\n  torch_tensor(c(3, 2.7, -1.2, 7.7, 1.9))\n\nTo turn these into probabilities, all we need to do is pass them to nnf_sigmoid(). nnf_sigmoid() squishes its argument to values between zero and one:\n\nprobability_estimate &lt;- nnf_sigmoid(unnormalized_estimate)\nprobability_estimate\n\ntorch_tensor\n 0.9526\n 0.9370\n 0.2315\n 0.9995\n 0.8699\n[ CPUFloatType{5} ]\nFrom the above table, we see that given unnormalized_estimate and probability_estimate, we can use both as inputs to a loss function – but we have to choose the appropriate one. Provided we do that, the output has to be the same in both cases.\nLet’s see (raw scores first):\n\nnnf_binary_cross_entropy_with_logits(\n  unnormalized_estimate, target\n)\n\ntorch_tensor\n0.643351\n[ CPUFloatType{} ]\nAnd now, probabilities:\n\nnnf_binary_cross_entropy(probability_estimate, target)\n\ntorch_tensor\n0.643351\n[ CPUFloatType{} ]\nThat worked as expected. What does this mean in practice? It means that when we build a model for binary classification, and the final layer computes an un-normalized score, we don’t need to attach a sigmoid layer to obtain probabilities. We can just call nnf_binary_cross_entropy_with_logits() when training the network. In fact, doing so is the preferred way, also due to reasons of numerical stability.\n\n\n9.2.3.2 Multi-class data\nMoving on to multi-class data, the most intuitive framing now really is in terms of (several) classes, not presence or absence of a single class. Think of classes as class indices (maybe indexing into some look-up table). Being indices, technically, classes start at 1:\n\ntarget &lt;- torch_tensor(c(2, 1, 3, 1, 3), dtype = torch_long())\n\nIn the multi-class scenario, raw scores are a two-dimensional tensor. Each row contains the scores for one observation, and each column corresponds to one of the classes. Here’s how the raw estimates could look:\n\nunnormalized_estimate &lt;- torch_tensor(\n  rbind(c(1.2, 7.7, -1),\n    c(1.2, -2.1, -1),\n    c(0.2, -0.7, 2.5),\n    c(0, -0.3, -1),\n    c(1.2, 0.1, 3.2)\n  )\n)\n\nAs per the above table, given this estimate, we should be calling nnf_cross_entropy() (and we will, when below we compare results).\nSo that’s the first option, and it works exactly as with binary data. For the second, there is an additional step.\nFirst, we again turn raw scores into probabilities, using nnf_softmax(). For most practical purposes, nnf_softmax() can be seen as the multi-class equivalent of nnf_sigmoid(). Strictly though, their effects are not the same. In a nutshell, nnf_sigmoid() treats low-score and high-score values equivalently, while nnf_softmax() exacerbates the distances between the top score and the remaining ones (“winner takes all”).\n\nprobability_estimate &lt;- nnf_softmax(unnormalized_estimate,\n  dim = 2\n)\nprobability_estimate\n\ntorch_tensor\n 0.0015  0.9983  0.0002\n 0.8713  0.0321  0.0965\n 0.0879  0.0357  0.8764\n 0.4742  0.3513  0.1745\n 0.1147  0.0382  0.8472\n[ CPUFloatType{5,3} ]\nThe second step, the one that was not required in the binary case, consists in transforming the probabilities to log probabilities. In our example, this could be accomplished by calling torch_log() on the probability_estimate we just computed. Alternatively, both steps together are taken care of by nnf_log_softmax():\n\nlogprob_estimate &lt;- nnf_log_softmax(unnormalized_estimate,\n  dim = 2\n)\nlogprob_estimate\n\ntorch_tensor\n-6.5017 -0.0017 -8.7017\n-0.1377 -3.4377 -2.3377\n-2.4319 -3.3319 -0.1319\n-0.7461 -1.0461 -1.7461\n-2.1658 -3.2658 -0.1658\n[ CPUFloatType{5,3} ]\nNow that we have estimates in both possible forms, we can again compare results from applicable loss functions. First, nnf_cross_entropy() on the raw scores:\n\nnnf_cross_entropy(unnormalized_estimate, target)\n\ntorch_tensor\n0.23665\n[ CPUFloatType{} ]\nAnd second, nnf_nll_loss() on the log probabilities:\n\nnnf_nll_loss(logprob_estimate, target)\n\ntorch_tensor\n0.23665\n[ CPUFloatType{} ]\nApplication-wise, what was said for the binary case applies here as well: In a multi-class classification network, there is no need to have a softmax layer at the end.\nBefore we end this chapter, let’s address a question that might have come to mind. Is not binary classification a sub-type of the multi-class setup? Should we not, in that case, arrive at the same result, whatever the method chosen?\n\n\n9.2.3.3 Check: Binary data, multi-class method\nLet’s see. We re-use the binary-classification scenario employed above. Here it is again:\n\ntarget &lt;- torch_tensor(c(1, 0, 0, 1, 1))\n\nunnormalized_estimate &lt;- \n  torch_tensor(c(3, 2.7, -1.2, 7.7, 1.9))\n\nprobability_estimate &lt;- nnf_sigmoid(unnormalized_estimate)\n\nnnf_binary_cross_entropy(probability_estimate, target)\n\ntorch_tensor\n0.64335\n[ CPUFloatType{} ]\nWe hope to get the same value doing things the multi-class way. We already have the probabilities (namely, probability_estimate); we just need to put them into the “observation by class” format expected by nnf_nll_loss():\n\n# logits\nmulticlass_probability &lt;- torch_tensor(rbind(\n  c(1 - 0.9526, 0.9526),\n  c(1 - 0.9370, 0.9370),\n  c(1 - 0.2315, 0.2315),\n  c(1 - 0.9995, 0.9995),\n  c(1 - 0.8699, 0.8699)\n))\n\nNow, we still want to apply the logarithm. And there is one other thing to be taken care of: In the binary setup, classes were coded as probabilities (either 0 or 1); now, we’re dealing with indices. This means we add 1 to the target tensor:\n\ntarget &lt;- target + 1\n\nFinally, we can call nnf_nll_loss():\n\nnnf_nll_loss(\n  torch_log(multiclass_probability),\n  target$to(dtype = torch_long())\n)\n\ntorch_tensor\n0.643275\n[ CPUFloatType{} ]\nThere we go. The results are indeed the same."
  },
  {
    "objectID": "loss_functions.html#footnotes",
    "href": "loss_functions.html#footnotes",
    "title": "9  Loss functions",
    "section": "",
    "text": "For cases where that assumption seems unlikely, distribution-adequate loss functions are provided (e.g., Poisson negative log likelihood, available as nnf_poisson_nll_loss() .↩︎"
  },
  {
    "objectID": "optim_2.html#meet-l-bfgs",
    "href": "optim_2.html#meet-l-bfgs",
    "title": "10  Function minimization with L-BFGS",
    "section": "10.1 Meet L-BFGS",
    "text": "10.1 Meet L-BFGS\nSo far, we’ve only talked about the kinds of optimizers often used in deep learning – stochastic gradient descent (SGD), SGD with momentum, and a few classics from the adaptive learning rate family: RMSProp, Adadelta, Adagrad, Adam. All these have in common one thing: They only make use of the gradient, that is, the vector of first derivatives. Accordingly, they are all first-order algorithms. This means, however, that they are missing out on helpful information provided by the Hessian, the matrix of second derivatives.\n\n10.1.1 Changing slopes\nFirst derivatives tell us about the slope of the landscape: Does it go up? Does it go down? How much so? Going a step further, second derivatives encode how much that slope changes.\nWhy should that be important?\nAssume we’re at point \\(\\mathbf{x}_n\\), and have just decided on a suitable descent direction. We take a step, of length determined by some pre-chosen learning rate, all set to arrive at point \\(\\mathbf{x}_{n+1}\\). What we don’t know is how the slope will have changed by the time we’ll have gotten there. Maybe it’s become much flatter in the meantime: In this case, we’ll have gone way too far, overshooting and winding up in a far-off area where anything could have happened in-between (including the slope going up again!).\nWe can illustrate this on a function of a single variable. Take a parabola, such as\n\\[\ny = 10x^2\n\\]\nIts derivative is \\(\\frac{dy}{dx} = 20x\\). If our current \\(x\\) is, say, \\(3\\), and we work with a learning rate of \\(0.1\\), we’ll subtract \\(20 * 3 * 0.1= 6\\), winding up at \\(-3\\).\nBut say we had slowed down at \\(2\\) and inspected the current slope. We’d have seen that there, the slope was less steep; in fact, when at that point, we should just have subtracted \\(20 * 2 * 0.1= 4\\).\nBy sheer luck, this “close-your-eyes-and-jump” strategy can still work out – if we happen to be using just the right learning rate for the function in question. (At the chosen learning rate, this would have been the case for a different parabola, \\(y = 5x^2\\), for example.) But wouldn’t it make sense to include second derivatives in the decision from the outset?\nAlgorithms that do this form the family of Newton methods. First, we look at their “purest” specimen, which best illustrates the principle but seldom is feasible in practice.\n\n\n10.1.2 Exact Newton method\nIn higher dimensions, the exact Newton method multiplies the gradient by the inverse of the Hessian, thus scaling the descent direction coordinate-by-coordinate. Our current example has just a single independent variable; so this means for us: take the first derivative, and divide by the second.\nWe now have a scaled gradient – but what portion of it should we subtract? In its original version, the exact Newton method does not make use of a learning rate, thus freeing us of the familiar trial-and-error game. Let’s see, then: In our example, the second derivative is \\(20\\), meaning that at \\(x=3\\) we have to subtract \\((20 * 3)/20=3\\). Voilà, we end up at \\(0\\), the location of the minimum, in a single step.\nSeeing how that turned out just great, why don’t we do it all the time? For one, it will work perfectly only with quadratic functions, like the one we chose for the demonstration. In other cases, it, too, will normally need some “tuning”, for example, by using a learning rate here as well.\nBut the main reason is another one. In more realistic applications, and certainly in the areas of machine learning and deep learning, computing the inverse of the Hessian at every step is way too costly. (It may, in fact, not even be possible.) This is where approximate, a.k.a. Quasi-Newton, methods come in.\n\n\n10.1.3 Approximate Newton: BFGS and L-BFGS\nAmong approximate Newton methods, probably the most-used is the Broyden-Goldfarb-Fletcher-Shanno algorithm, or BFGS. Instead of continually computing the exact inverse of the Hessian, it keeps an iteratively-updated approximation of that inverse. BFGS is often implemented in a more memory-friendly version, referred to as Limited-Memory BFGS (L-BFGS). This is the one provided as part of the core torch optimizers.\nBefore we get there, though, there is one last conceptual thing to discuss.\n\n\n10.1.4 Line search\nLike their exact counterpart, approximate Newton methods can work without a learning rate. In that case, they compute a descent direction and follow the scaled gradient as-is. We already talked about how, depending on the function in question, this can work more or less well. When it does not, there are two things one could do: Firstly, take small steps, or put differently, introduce a learning rate. And secondly, do a line search.\nWith line search, we spend some time evaluating how far to follow the descent direction. There are two principal ways of doing this.\nThe first, exact line search, involves yet another optimization problem: Take the current point, compute the descent direction, and hard-code them as givens in a second function that depends on the learning rate only. Then, differentiate this function to find its minimum. The solution will be the learning rate that optimizes the step length taken.\nThe alternative strategy is to do an approximate search. By now, you’re probably not surprised: Just as approximate Newton is more realistically-feasible than exact Newton, approximate line search is more practicable than exact line search.\nFor line search, approximating the best solution means following a set of proven heuristics. Essentially, we look for something that is just good enough. Among the most established heuristics are the Strong Wolfe conditions, and this is the strategy implemented in torch’s optim_lbfgs(). In the next section, we’ll see how to use optim_lbfgs() to minimize the Rosenbrock function, both with and without line search."
  },
  {
    "objectID": "optim_2.html#minimizing-the-rosenbrock-function-with-optim_lbfgs",
    "href": "optim_2.html#minimizing-the-rosenbrock-function-with-optim_lbfgs",
    "title": "10  Function minimization with L-BFGS",
    "section": "10.2 Minimizing the Rosenbrock function with optim_lbfgs()",
    "text": "10.2 Minimizing the Rosenbrock function with optim_lbfgs()\nHere is the Rosenbrock function again:\n\nlibrary(torch)\n\na &lt;- 1\nb &lt;- 5\n\nrosenbrock &lt;- function(x) {\n  x1 &lt;- x[1]\n  x2 &lt;- x[2]\n  (a - x1)^2 + b * (x2 - x1^2)^2\n}\n\nIn our manual minimization efforts, the procedure was the following. A one-time action, we first defined the parameter tensor destined to hold the current \\(\\mathbf{x}\\):\n\nx &lt;- torch_tensor(c(-1, 1), requires_grad = TRUE)\n\nThen, we iteratively executed the following operations:\n\nCalculate the function value at the current \\(\\mathbf{x}\\).\nCompute the gradient of that value at the position in question.\nSubtract a fraction of the gradient from the current \\(\\mathbf{x}\\).\n\nHow, if so, does that blueprint change?\nThe first step remains unchanged. We still have\n\nvalue &lt;- rosenbrock(x)\n\nThe second step stays the same, as well. We still call backward() directly on the output tensor:\n\nvalue$backward()\n\nThis is because an optimizer does not compute gradients; it decides what to do with the gradient once it’s been computed.\nWhat changes, thus, is the third step, the one that also was the most cumbersome. Now, it is the optimizer that applies the update. To be able to do that, there is a prerequisite: Prior to starting the loop, the optimizer will need to be told which parameter it is supposed to work on. In fact, this is so important that you can’t even create an optimizer without passing it that parameter:\n\nopt &lt;- optim_lbfgs(x)\n\nIn the loop, we now call the step() method on the optimizer object to update the parameter. There is just one part from our manual procedure that needs to get carried over to the new way: We still need to zero out the gradient on each iteration. Just this time, not on the parameter tensor, x, but the optimizer object itself. In principle, this then yields the following actions to be performed on each iteration:\n\nvalue &lt;- rosenbrock(x)\n\nopt$zero_grad()\nvalue$backward()\n\nopt$step()\n\nWhy “in principle”? In fact, this is what we’d write for every optimizer but optim_lbfgs().\nFor optim_lbfgs(), step() needs to be called passing in an anonymous function, a closure. Zeroing of previous gradients, function call, and gradient calculation, all these happen inside the closure:\n\ncalc_loss &lt;- function() {\n  optimizer$zero_grad()\n  value &lt;- rosenbrock(x_star)\n  value$backward()\n  value\n}\n\nHaving executed those actions, the closure returns the function value. Here is how it is called by step():\n\nfor (i in 1:num_iterations) {\n  optimizer$step(calc_loss)\n}\n\nNow we put it all together, add some logging output, and compare what happens with and without line search.\n\n10.2.1 optim_lbfgs() default behavior\nAs a baseline, we first run without line search. Two iterations are enough. In the below output, you can see that in each iteration, the closure is evaluated several times. This is the technical reason we had to create it in the first place.\n\nnum_iterations &lt;- 2\n\nx &lt;- torch_tensor(c(-1, 1), requires_grad = TRUE)\n\noptimizer &lt;- optim_lbfgs(x)\n\ncalc_loss &lt;- function() {\n  optimizer$zero_grad()\n\n  value &lt;- rosenbrock(x)\n  cat(\"Value is: \", as.numeric(value), \"\\n\")\n\n  value$backward()\n  value\n}\n\nfor (i in 1:num_iterations) {\n  cat(\"\\nIteration: \", i, \"\\n\")\n  optimizer$step(calc_loss)\n}\n\nIteration:  1 \nValue is:  4 \nValue is:  6 \nValue is:  318.0431 \nValue is:  5.146369 \nValue is:  4.443705 \nValue is:  0.8787204 \nValue is:  0.8543001 \nValue is:  2.001667 \nValue is:  0.5656172 \nValue is:  0.400589 \nValue is:  7.726219 \nValue is:  0.3388008 \nValue is:  0.2861604 \nValue is:  1.951176 \nValue is:  0.2071857 \nValue is:  0.150776 \nValue is:  0.411357 \nValue is:  0.08056168 \nValue is:  0.04880721 \nValue is:  0.0302862 \n\nIteration:  2 \nValue is:  0.01697086 \nValue is:  0.01124081 \nValue is:  0.0006622815 \nValue is:  3.300996e-05 \nValue is:  1.35731e-07 \nValue is:  1.111701e-09 \nValue is:  4.547474e-12 \nTo make sure we really have found the minimum, we check x:\n\nx\n\ntorch_tensor\n 1.0000\n 1.0000\n[ CPUFloatType{2} ]\nCan this still be improved upon?\n\n\n10.2.2 optim_lbfgs() with line search\nLet’s see. Below, the only line that’s changed is the one where we construct the optimizer.\n\nnum_iterations &lt;- 2\n\nx &lt;- torch_tensor(c(-1, 1), requires_grad = TRUE)\n\noptimizer &lt;- optim_lbfgs(x, line_search_fn = \"strong_wolfe\")\n\ncalc_loss &lt;- function() {\n  optimizer$zero_grad()\n\n  value &lt;- rosenbrock(x)\n  cat(\"Value is: \", as.numeric(value), \"\\n\")\n\n  value$backward()\n  value\n}\n\nfor (i in 1:num_iterations) {\n  cat(\"\\nIteration: \", i, \"\\n\")\n  optimizer$step(calc_loss)\n}\n\nIteration:  1 \nValue is:  4 \nValue is:  6 \nValue is:  3.802412 \nValue is:  3.680712 \nValue is:  2.883048 \nValue is:  2.5165 \nValue is:  2.064779 \nValue is:  1.38384 \nValue is:  1.073063 \nValue is:  0.8844351 \nValue is:  0.5554555 \nValue is:  0.2501077 \nValue is:  0.8948895 \nValue is:  0.1619074 \nValue is:  0.06823064 \nValue is:  0.01653575 \nValue is:  0.004060207 \nValue is:  0.00353789 \nValue is:  0.000391416 \nValue is:  4.303527e-06 \nValue is:  2.036851e-08 \nValue is:  6.870948e-12 \n\nIteration:  2 \nValue is:  6.870948e-12 \nWith line search, a single iteration is sufficient to reach the minimum. Inspecting the individual losses, we also see that the algorithm reduces the loss nearly every time it probes the function, which without line search, had not been the case."
  },
  {
    "objectID": "network_2.html#data",
    "href": "network_2.html#data",
    "title": "11  Modularizing the neural network",
    "section": "11.1 Data",
    "text": "11.1 Data\nAs a prerequisite, we generate the data, same as last time.\n\n# input dimensionality (number of input features)\nd_in &lt;- 3\n# number of observations in training set\nn &lt;- 100\n\nx &lt;- torch_randn(n, d_in)\ncoefs &lt;- c(0.2, -1.3, -0.5)\ny &lt;- x$matmul(coefs)$unsqueeze(2) + torch_randn(n, 1)"
  },
  {
    "objectID": "network_2.html#network",
    "href": "network_2.html#network",
    "title": "11  Modularizing the neural network",
    "section": "11.2 Network",
    "text": "11.2 Network\nWith two linear layers connected via ReLU activation, the easiest choice is a sequential module, very similar to the one we saw in the introduction to modules:\n\n# dimensionality of hidden layer\nd_hidden &lt;- 32\n# output dimensionality (number of predicted features)\nd_out &lt;- 1\n\nnet &lt;- nn_sequential(\n  nn_linear(d_in, d_hidden),\n  nn_relu(),\n  nn_linear(d_hidden, d_out)\n)"
  },
  {
    "objectID": "network_2.html#training",
    "href": "network_2.html#training",
    "title": "11  Modularizing the neural network",
    "section": "11.3 Training",
    "text": "11.3 Training\nHere is the updated training process. We use the Adam optimizer, a popular choice.\n\nopt &lt;- optim_adam(net$parameters)\n\n### training loop --------------------------------------\n\nfor (t in 1:200) {\n  \n  ### -------- Forward pass --------\n  y_pred &lt;- net(x)\n  \n  ### -------- Compute loss -------- \n  loss &lt;- nnf_mse_loss(y_pred, y)\n  if (t %% 10 == 0)\n    cat(\"Epoch: \", t, \"   Loss: \", loss$item(), \"\\n\")\n  \n  ### -------- Backpropagation --------\n  opt$zero_grad()\n  loss$backward()\n  \n  ### -------- Update weights -------- \n  opt$step()\n\n}\n\nEpoch:  10    Loss:  2.549933 \nEpoch:  20    Loss:  2.422556 \nEpoch:  30    Loss:  2.298053 \nEpoch:  40    Loss:  2.173909 \nEpoch:  50    Loss:  2.0489 \nEpoch:  60    Loss:  1.924003 \nEpoch:  70    Loss:  1.800404 \nEpoch:  80    Loss:  1.678221 \nEpoch:  90    Loss:  1.56143 \nEpoch:  100    Loss:  1.453637 \nEpoch:  110    Loss:  1.355832 \nEpoch:  120    Loss:  1.269234 \nEpoch:  130    Loss:  1.195116 \nEpoch:  140    Loss:  1.134008 \nEpoch:  150    Loss:  1.085828 \nEpoch:  160    Loss:  1.048921 \nEpoch:  170    Loss:  1.021384 \nEpoch:  180    Loss:  1.0011 \nEpoch:  190    Loss:  0.9857832 \nEpoch:  200    Loss:  0.973796 \nIn addition to shortening and streamlining the code, our changes have made a big difference performance-wise."
  },
  {
    "objectID": "network_2.html#whats-to-come",
    "href": "network_2.html#whats-to-come",
    "title": "11  Modularizing the neural network",
    "section": "11.4 What’s to come",
    "text": "11.4 What’s to come\nYou now know a lot about how torch works, and how to use it to minimize a cost function in various settings: for example, to train a neural network. But for real-world applications, there is a lot more torch has to offer. The next – and most voluminous – part of the book focuses on deep learning."
  },
  {
    "objectID": "dl_overview.html",
    "href": "dl_overview.html",
    "title": "12  Overview",
    "section": "",
    "text": "This part of the book is completely dedicated to applications of deep learning. There will be two categories of things to dive into: topics workflow-related, and topics related to domain adaptation.\nRegarding workflow, we’ll see how to:\n\nprepare the input data in a form the model can work with;\neffectively and efficiently train a model, monitoring progress and adjusting hyper-parameters on the fly;\nsave and load models;\nmaking models generalize beyond the training data;\nspeed up training;\nand more.\n\nSecondly – beyond an efficient workflow – the task in question matters. Compositions of linear layers, of the type we used to learn torch in the first part, will not suffice when our goal is to model images or time series. Successful use of deep learning means tailoring model architecture to the domain in question. To that end, we start from concrete tasks, and present applicable architectures directly by example.\nConcretely, the plan is the following. The upcoming two chapters will introduce you to workflow-related techniques that are indispensable in practice. You’ll encounter another package, luz, that endows torch with an important layer of abstraction, and significantly streamlines the workflow. Once you know how to use it, we’re all set to look at a first application: image classification. To improve on our initial results, we then back up and explore two more advanced workflow-related topics: how to improve generalization, and how to speed up training. Equipped with that knowledge, we first return to images, before extending our domain-related skills to tabular data, time series, and audio."
  },
  {
    "objectID": "data.html#data-vs.-dataset-vs.-dataloader-whats-the-difference",
    "href": "data.html#data-vs.-dataset-vs.-dataloader-whats-the-difference",
    "title": "13  Loading data",
    "section": "13.1 Data vs. dataset() vs. dataloader() – what’s the difference?",
    "text": "13.1 Data vs. dataset() vs. dataloader() – what’s the difference?\nIn this book, “dataset” (variable-width font, no parentheses), or just “the data”, usually refers to things like R matrices, data.frames, and what’s contained therein. A dataset() (fixed-width font, parentheses), however, is a torch object that knows how to do one thing: deliver to the caller a single item. That item, usually, will be a list, consisting of one input and one target tensor. (It could be anything, though – whatever makes sense for the task. For example, it could be a single tensor, if input and target are the same. Or more than two tensors, in case different inputs should be passed to different modules.)\nAs long as it fulfills the above-stated contract, a dataset() is free to do whatever needs to be done. It could, for example, download data from the internet, store them in some temporary location, do some pre-processing, and when asked, return bite-sized chunks of data in just the shape expected by a certain class of models. No matter what it does in the background, all its caller cares about is that it return a single item. Its caller, that’s the dataloader().\nA dataloader()’s role is to feed input to the model in batches. One immediate reason is computer memory: Most dataset()s will be far too large to pass them to the model in one go. But there are additional benefits to batching. Since gradients are computed (and model weights updated) once per batch, there is an inherent stochasticity to the process, a stochasticity that helps with model training. We’ll talk more about that in an upcoming chapter."
  },
  {
    "objectID": "data.html#using-datasets",
    "href": "data.html#using-datasets",
    "title": "13  Loading data",
    "section": "13.2 Using dataset()s",
    "text": "13.2 Using dataset()s\ndataset()s come in all flavors, from ready-to-use – and brought to you by some package, torchvision or torchdatasets, say, or any package that chooses to provide access to data in torch-ready form – to fully customized (made by you, that is). Creating dataset()s is straightforward, since they are R6 objects, and there’s just three methods to be implemented. These methods are:\n\ninitialize(...). Parameters to initialize() are passed when a dataset() is instantiated. Possibilities include, but are not limited to, references to R data.frames, filesystem paths, download URLs, and any configurations and parameterizations expected by the dataset().\n.getitem(i). This is the method responsible for fulfilling the contract. Whatever it returns counts as a single item. The parameter, i, is an index that, in many cases, will be used to determine the starting position in the underlying data structure (a data.frame of file system paths, for example). However, the dataset() is not obliged to actually make use of that parameter. With extremely huge dataset()s, for example, or given serious class imbalance, it could instead decide to return items based on sampling.\n.length(). This, usually, is a one-liner, its only purpose being to inform about the number of available items in a dataset().\n\nHere is a blueprint for creating a dataset():\n\nds &lt;- dataset()(\n  initialize = function(...) {\n    ...\n  },\n  .getitem = function(index) {\n    ...\n  },\n  .length = function() {\n    ...\n  }\n)\n\nThat said, let’s compare three ways of obtaining a dataset() to work with, from tailor-made to maximally effortless.\n\n13.2.1 A self-built dataset()\nLet’s say we wanted to build a classifier based on the popular iris alternative, palmerpenguins.\n\nlibrary(torch)\nlibrary(palmerpenguins)\nlibrary(dplyr)\n\npenguins %&gt;% glimpse()\n\n$ species           &lt;fct&gt; Adelie, Adelie, Adelie, Adelie,...\n$ island            &lt;fct&gt; Torgersen, Torgersen, Torgersen,...\n$ bill_length_mm    &lt;dbl&gt; 39.1, 39.5, 40.3, NA, 36.7, 39.3,...\n$ bill_depth_mm     &lt;dbl&gt; 18.7, 17.4, 18.0, NA, 19.3, 20.6,...\n$ flipper_length_mm &lt;int&gt; 181, 186, 195, NA, 193, 190, 181,...\n$ body_mass_g       &lt;int&gt; 3750, 3800, 3250, NA, 3450, 3650,...\n$ sex               &lt;fct&gt; male, female, female, NA, female,...\n$ year              &lt;int&gt; 2007, 2007, 2007, 2007, 2007,...\nIn predicting species, we want to make use of just a subset of columns: bill_length_mm, bill_depth_mm, flipper_length_mm, and body_mass_g. We build a dataset() that returns exactly what is needed:\n\npenguins_dataset &lt;- dataset(\n  name = \"penguins_dataset()\",\n  initialize = function(df) {\n    df &lt;- na.omit(df)\n    self$x &lt;- as.matrix(df[, 3:6]) %&gt;% torch_tensor()\n    self$y &lt;- torch_tensor(\n      as.numeric(df$species)\n    )$to(torch_long())\n  },\n  .getitem = function(i) {\n    list(x = self$x[i, ], y = self$y[i])\n  },\n  .length = function() {\n    dim(self$x)[1]\n  }\n)\n\nOnce we’ve instantiated a penguins_dataset(), we should immediately perform some checks. First, does it have the expected length?\n\nds &lt;- penguins_dataset(penguins)\nlength(ds)\n\n[1] 333\nAnd second, do individual elements have the expected shape and data type? Conveniently, we can access dataset() items like tensor values, through indexing:\n\nds[1]\n\n$x\ntorch_tensor\n   39.1000\n   18.7000\n  181.0000\n 3750.0000\n[ CPUFloatType{4} ]\n\n$y\ntorch_tensor\n1\n[ CPULongType{} ]\nThis also works for items “further down” in the dataset() – it has to: When indexing into a dataset(), what happens in the background is a call to .getitem(i), passing along the desired position i.\nTruth be told, in this case we didn’t really have to build our own dataset(). With so little pre-processing to be done, there is an alternative: tensor_dataset().\n\n\n13.2.2 tensor_dataset()\nWhen you already have a tensor around, or something that’s readily converted to one, you can make use of a built-in dataset() generator: tensor_dataset(). This function can be passed any number of tensors; each batch item then is a list of tensor values:\n\nthree &lt;- tensor_dataset(\n  torch_randn(10), torch_randn(10), torch_randn(10)\n)\nthree[1]\n\n[[1]]\ntorch_tensor\n0.522735\n[ CPUFloatType{} ]\n\n[[2]]\ntorch_tensor\n-0.976477\n[ CPUFloatType{} ]\n\n[[3]]\ntorch_tensor\n-1.14685\n[ CPUFloatType{} ]\nIn our penguins scenario, we end up with two lines of code:\n\npenguins &lt;- na.omit(penguins)\nds &lt;- tensor_dataset(\n  torch_tensor(as.matrix(penguins[, 3:6])),\n  torch_tensor(\n    as.numeric(penguins$species)\n  )$to(torch_long())\n)\n\nds[1]\n\nAdmittedly though, we have not made use of all the dataset’s columns. The more pre-processing you need a dataset() to do, the more likely you are to want to code your own.\nThirdly and finally, here is the most effortless possible way.\n\n\n13.2.3 torchvision::mnist_dataset()\nWhen you’re working with packages in the torch ecosystem, chances are that they already include some dataset()s, be it for demonstration purposes or for the sake of the data themselves. torchvision, for example, packages a number of classic image datasets – among those, that archetype of archetypes, MNIST.\nSince we’re going to talk about image processing in a later chapter, I won’t comment on the arguments to mnist_dataset() here; we do, however, include a quick check that the data delivered conform to what we’d expect:\n\nlibrary(torchvision)\n\ndir &lt;- \"~/.torch-datasets\"\n\nds &lt;- mnist_dataset(\n  root = dir,\n  train = TRUE, # default\n  download = TRUE,\n  transform = function(x) {\n    x %&gt;% transform_to_tensor() \n  }\n)\n\nfirst &lt;- ds[1]\ncat(\"Image shape: \", first$x$shape, \" Label: \", first$y, \"\\n\")\n\nImage shape:  1 28 28  Label:  6 \nAt this point, that is all we need to know about dataset()s – we’ll encounter plenty of them in the course of this book. Now, we move on from the one to the many."
  },
  {
    "objectID": "data.html#using-dataloaders",
    "href": "data.html#using-dataloaders",
    "title": "13  Loading data",
    "section": "13.3 Using dataloader()s",
    "text": "13.3 Using dataloader()s\nContinuing to work with the newly created MNIST dataset(), we instantiate a dataloader() for it. The dataloader() will deliver pairs of images and labels in batches: thirty-two at a time. In every epoch, it will return them in different order (shuffle = TRUE):\n\ndl &lt;- dataloader(ds, batch_size = 32, shuffle = TRUE)\n\nJust like dataset()s, dataloader()s can be queried about their length:\n\nlength(dl)\n\n[1] 1875\nThis time, though, the returned value is not the number of items; it is the number of batches.\nTo loop over batches, we first obtain an iterator, an object that knows how to traverse the elements in this dataloader(). Calling dataloader_next(), we can then access successive batches, one by one:\n\nfirst_batch &lt;- dl %&gt;%\n  # obtain an iterator for this dataloader\n  dataloader_make_iter() %&gt;% \n  dataloader_next()\n\ndim(first_batch$x)\ndim(first_batch$y)\n\n[1] 32  1 28 28\n[1] 32\nIf you compare the batch shape of x – the image part – with the shape of an individual image (as inspected above), you see that now, there is an additional dimension in front, reflecting the number of images in a batch.\nThe next step is passing the batches to a model. This – in fact, this as well as the complete, end-to-end deep-learning workflow – is what the next chapter is about."
  },
  {
    "objectID": "training_with_luz.html#que-haya-luz---que-haja-luz---let-there-be-light",
    "href": "training_with_luz.html#que-haya-luz---que-haja-luz---let-there-be-light",
    "title": "14  Training with luz",
    "section": "14.1 Que haya luz - Que haja luz - Let there be light",
    "text": "14.1 Que haya luz - Que haja luz - Let there be light\nA torch already brings some light, but sometimes in life, there is no too bright. luz was designed to make deep learning with torch as effortless as possible, while at the same time allowing for easy customization. In this chapter, we focus on the overall process; examples of customization will appear in later chapters.\nFor ease of comparison, we take our running example, and add a third version, now using luz. First, we “just” directly port the example; then, we adapt it to a more realistic scenario. In that scenario, we\n\nmake use of separate training, validation, and test sets;\nhave luz compute metrics during training/validation;\nillustrate the use of callbacks to perform custom actions or dynamically change hyper-parameters during training; and\nexplain what is going on with the aforementioned devices."
  },
  {
    "objectID": "training_with_luz.html#porting-the-toy-example",
    "href": "training_with_luz.html#porting-the-toy-example",
    "title": "14  Training with luz",
    "section": "14.2 Porting the toy example",
    "text": "14.2 Porting the toy example\n\n14.2.1 Data\nluz does not just substantially transform the code required to train a neural network; it also adds flexibility on the data side of things. In addition to a reference to a dataloader(), its fit() method accepts dataset()s, tensors, and even R objects, as we’ll be able to verify soon.\nWe start by generating an R matrix and a vector, as before. This time though, we also wrap them in a tensor_dataset(), and instantiate a dataloader(). Instead of just 100, we now generate 1000 observations.\n\nlibrary(torch)\nlibrary(luz)\n\n# input dimensionality (number of input features)\nd_in &lt;- 3\n# number of observations in training set\nn &lt;- 1000\n\nx &lt;- torch_randn(n, d_in)\ncoefs &lt;- c(0.2, -1.3, -0.5)\ny &lt;- x$matmul(coefs)$unsqueeze(2) + torch_randn(n, 1)\n\nds &lt;- tensor_dataset(x, y)\n\ndl &lt;- dataloader(ds, batch_size = 100, shuffle = TRUE)\n\n\n\n14.2.2 Model\nTo use luz, no changes are needed to the model definition. Note, though, that we just define the model architecture; we never actually instantiate a model object ourselves.\n\n# dimensionality of hidden layer\nd_hidden &lt;- 32\n# output dimensionality (number of predicted features)\nd_out &lt;- 1\n\nnet &lt;- nn_module(\n  initialize = function(d_in, d_hidden, d_out) {\n    self$net &lt;- nn_sequential(\n      nn_linear(d_in, d_hidden),\n      nn_relu(),\n      nn_linear(d_hidden, d_out)\n    )\n  },\n  forward = function(x) {\n    self$net(x)\n  }\n)\n\n\n\n14.2.3 Training\nTo train the model, we don’t write loops anymore. luz replaces the familiar iterative style by a declarative one: You tell luz what you want to happen, and like a docile sorcerer’s apprentice, it sets in motion the machinery.\nConcretely, instruction happens in two – required – calls.\n\nIn setup(), you specify the loss function and the optimizer to use.\nIn fit(), you pass reference(s) to the training (and optionally, validation) data, as well as the number of epochs to train for.\n\nIf the model is configurable – meaning, it accepts arguments to initialize() – a third method comes into play: set_hparams(), to be called in-between the other two. (That’s hparams for hyper-parameters.) Using this mechanism, you can easily experiment with, for example, different layer sizes, or other factors suspected to affect performance.\n\nfitted &lt;- net %&gt;%\n  setup(loss = nn_mse_loss(), optimizer = optim_adam) %&gt;%\n  set_hparams(\n    d_in = d_in,\n    d_hidden = d_hidden, d_out = d_out\n  ) %&gt;%\n  fit(dl, epochs = 200)\n\nRunning this code, you should see output approximately like this:\nEpoch 1/200\nTrain metrics: Loss: 3.0343                                                                               \nEpoch 2/200\nTrain metrics: Loss: 2.5387                                                                               \nEpoch 3/200\nTrain metrics: Loss: 2.2758                                                                               \n...\n...\nEpoch 198/200\nTrain metrics: Loss: 0.891                                                                                \nEpoch 199/200\nTrain metrics: Loss: 0.8879                                                                               \nEpoch 200/200\nTrain metrics: Loss: 0.9036 \nAbove, what we passed to fit() was the dataloader(). Let’s check that referencing the dataset() would have been just as fine:\n\nfitted &lt;- net %&gt;%\n  setup(loss = nn_mse_loss(), optimizer = optim_adam) %&gt;%\n  set_hparams(\n    d_in = d_in,\n    d_hidden = d_hidden, d_out = d_out\n  ) %&gt;%\n  fit(ds, epochs = 200)\n\nOr even, torch tensors:\n\nfitted &lt;- net %&gt;%\n  setup(loss = nn_mse_loss(), optimizer = optim_adam) %&gt;%\n  set_hparams(\n    d_in = d_in,\n    d_hidden = d_hidden, d_out = d_out\n  ) %&gt;%\n  fit(list(x, y), epochs = 200)\n\nAnd finally, R objects, which can be convenient when we aren’t already working with tensors.\n\nfitted &lt;- net %&gt;%\n  setup(loss = nn_mse_loss(), optimizer = optim_adam) %&gt;%\n  set_hparams(\n    d_in = d_in,\n    d_hidden = d_hidden, d_out = d_out\n  ) %&gt;%\n  fit(list(as.matrix(x), as.matrix(y)), epochs = 200)\n\nIn the following sections, we’ll always be working with dataloader()s; but in some cases those “shortcuts” may come in handy.\nNext, we extend the toy example, illustrating how to address more complex requirements."
  },
  {
    "objectID": "training_with_luz.html#a-more-realistic-scenario",
    "href": "training_with_luz.html#a-more-realistic-scenario",
    "title": "14  Training with luz",
    "section": "14.3 A more realistic scenario",
    "text": "14.3 A more realistic scenario\n\n14.3.1 Integrating training, validation, and test\nIn deep learning, training and validation phases are interleaved. Every epoch of training is followed by an epoch of validation. Importantly, the data used in both phases have to be strictly disjoint.\nIn each training phase, gradients are computed and weights are changed; during validation, none of that happens. Why have a validation set, then? If, for each epoch, we compute task-relevant metrics for both partitions, we can see if we are overfitting to the training data: that is, drawing conclusions based on training sample specifics not descriptive of the overall population we want to model. All we have to do is two things: instruct luz to compute a suitable metric, and pass it an additional dataloader pointing to the validation data.\nThe former is done in setup(), and for a regression task, common choices are mean squared or mean absolute error (MSE or MAE, resp.). As we’re already using MSE as our loss, let’s choose MAE for a metric:\n\nfitted &lt;- net %&gt;%\n  setup(\n    loss = nn_mse_loss(),\n    optimizer = optim_adam,\n    metrics = list(luz_metric_mae())\n  ) %&gt;%\n  fit(...)\n\nThe validation dataloader is passed in fit() – but to be able to reference it, we need to construct it first! So now (anticipating we’ll want to have a test set, too), we split up the original 1000 observations into three partitions, creating a dataset and a dataloader for each of them.\n\ntrain_ids &lt;- sample(1:length(ds), size = 0.6 * length(ds))\nvalid_ids &lt;- sample(\n  setdiff(1:length(ds), train_ids),\n  size = 0.2 * length(ds)\n)\ntest_ids &lt;- setdiff(\n  1:length(ds),\n  union(train_ids, valid_ids)\n)\n\ntrain_ds &lt;- dataset_subset(ds, indices = train_ids)\nvalid_ds &lt;- dataset_subset(ds, indices = valid_ids)\ntest_ds &lt;- dataset_subset(ds, indices = test_ids)\n\ntrain_dl &lt;- dataloader(train_ds,\n  batch_size = 100, shuffle = TRUE\n)\nvalid_dl &lt;- dataloader(valid_ds, batch_size = 100)\ntest_dl &lt;- dataloader(test_ds, batch_size = 100)\n\nNow, we are ready to start the enhanced workflow:\n\nfitted &lt;- net %&gt;%\n  setup(\n    loss = nn_mse_loss(),\n    optimizer = optim_adam,\n    metrics = list(luz_metric_mae())\n  ) %&gt;%\n  set_hparams(\n    d_in = d_in,\n    d_hidden = d_hidden, d_out = d_out\n  ) %&gt;%\n  fit(train_dl, epochs = 200, valid_data = valid_dl)\n\nEpoch 1/200\nTrain metrics: Loss: 2.5863 - MAE: 1.2832                                       \nValid metrics: Loss: 2.487 - MAE: 1.2365\nEpoch 2/200\nTrain metrics: Loss: 2.4943 - MAE: 1.26                                          \nValid metrics: Loss: 2.4049 - MAE: 1.2161\nEpoch 3/200\nTrain metrics: Loss: 2.4036 - MAE: 1.236                                         \nValid metrics: Loss: 2.3261 - MAE: 1.1962\n...\n...\nEpoch 198/200\nTrain metrics: Loss: 0.8947 - MAE: 0.7504\nValid metrics: Loss: 1.0572 - MAE: 0.8287\nEpoch 199/200\nTrain metrics: Loss: 0.8948 - MAE: 0.7503\nValid metrics: Loss: 1.0569 - MAE: 0.8286\nEpoch 200/200\nTrain metrics: Loss: 0.8944 - MAE: 0.75\nValid metrics: Loss: 1.0579 - MAE: 0.8292\nEven though both training and validation sets come from the exact same distribution, we do see a bit of overfitting. This is a topic we’ll talk about more in the next chapter.\nOnce training has finished, the fitted object above holds a history of epoch-wise metrics, as well as references to a number of important objects involved in the training process. Among the latter is the fitted model itself – which enables an easy way to obtain predictions on the test set:\n\nfitted %&gt;% predict(test_dl)\n\ntorch_tensor\n 0.7799\n 1.7839\n-1.1294\n-1.3002\n-1.8169\n-1.6762\n-0.7548\n-1.2041\n 2.9613\n-0.9551\n 0.7714\n-0.8265\n 1.1334\n-2.8406\n-1.1679\n 0.8350\n 2.0134\n 2.1083\n 1.4093\n 0.6962\n-0.3669\n-0.5292\n 2.0310\n-0.5814\n 2.7494\n 0.7855\n-0.5263\n-1.1257\n-3.3117\n 0.6157\n... [the output was truncated (use n=-1 to disable)]\n[ CPUFloatType{200,1} ]\nWe also want to evaluate performance on the test set:\n\nfitted %&gt;% evaluate(test_dl)\n\nA `luz_module_evaluation`\n── Results \nloss: 0.9271\nmae: 0.7348\nThis workflow of: training and validation in lock-step, then checking and extracting predictions on the test set is something we’ll encounter times and again in this book.\n\n\n14.3.2 Using callbacks to “hook” into the training process\nAt this point, you may feel that what we’ve gained in code efficiency, we may have lost in flexibility. Coding the training loop yourself, you can arrange for all kinds of things to happen: save model weights, adjust the learning rate … whatever you need.\nIn reality, no flexibility is lost. Instead, luz offers a standardized way to achieve the same goals: callbacks. Callbacks are objects that can execute arbitrary R code, at any of the following points in time:\n\nwhen the overall training process starts or ends (on_fit_begin() / on_fit_end());\nwhen an epoch (comprising training and validation) starts or ends (on_epoch_begin() / on_epoch_end());\nwhen during an epoch, the training (validation, resp.) phase starts or ends (on_train_begin() / on_train_end(); on_valid_begin() / on_valid_end());\nwhen during training (validation, resp.), a new batch is either about to be or has been processed (on_train_batch_begin() / on_train_batch_end(); on_valid_batch_begin() / on_valid_batch_end());\nand even at specific landmarks inside the “innermost” training / validation logic, such as “after loss computation”, “after backward()” or “after step()”.\n\nWhile you can implement any logic you wish using callbacks (and we’ll see how to do this in a later chapter), luz already comes equipped with a very useful set. For example:\n\nluz_callback_model_checkpoint() saves model weights after every epoch (or just in case of improvements, if so instructed).\nluz_callback_lr_scheduler() activates one of torch’s learning rate schedulers. Different scheduler objects exist, each following their own logic in dynamically updating the learning rate.\nluz_callback_early_stopping() terminates training once model performance stops to improve. What exactly “stops to improve” should mean is configurable by the user.\n\nCallbacks are passed to the fit() method in a list. For example, augmenting our most recent workflow:\n\nfitted &lt;- net %&gt;%\n  setup(\n    loss = nn_mse_loss(),\n    optimizer = optim_adam,\n    metrics = list(luz_metric_mae())\n  ) %&gt;%\n  set_hparams(d_in = d_in,\n              d_hidden = d_hidden,\n              d_out = d_out) %&gt;%\n  fit(\n    train_dl,\n    epochs = 200,\n    valid_data = valid_dl,\n    callbacks = list(\n      luz_callback_model_checkpoint(path = \"./models/\",\n                                    save_best_only = TRUE),\n      luz_callback_early_stopping(patience = 10)\n    )\n  )\n\nWith this configuration, weights will be saved, but only if validation loss decreases. Training will halt if there is no improvement (again, in validation loss) for ten epochs. With both callbacks, you can pick any other metric to base the decision on, and the metric in question may also refer to the training set.\nHere, we see early stopping happening after 111 epochs:\nEpoch 1/200\nTrain metrics: Loss: 2.5803 - MAE: 1.2547\nValid metrics: Loss: 3.3763 - MAE: 1.4232\nEpoch 2/200\nTrain metrics: Loss: 2.4767 - MAE: 1.229\nValid metrics: Loss: 3.2334 - MAE: 1.3909\n...\n...\nEpoch 110/200\nTrain metrics: Loss: 1.011 - MAE: 0.8034\nValid metrics: Loss: 1.1673 - MAE: 0.8578\nEpoch 111/200\nTrain metrics: Loss: 1.0108 - MAE: 0.8032\nValid metrics: Loss: 1.167 - MAE: 0.8578\nEarly stopping at epoch 111 of 200\n\n\n14.3.3 How luz helps with devices\nFinally, let’s quickly mention how luz helps with device placement. Devices, in a usual environment, are the CPU and perhaps, if available, a GPU. For training, data and model weights need to be located on the same device. This can introduce complexities, and – at the very least – necessitates additional code to keep all pieces in sync.\nWith luz, related actions happen transparently to the user. Let’s take the prediction step from above:\n\nfitted %&gt;% predict(test_dl)\n\nIn case this code was executed on a machine that has a GPU, luz will have detected that, and the model’s weight tensors will already have been moved there. Now, for the above call to predict(), what happened “under the hood” was the following:\n\nluz put the model in evaluation mode, making sure that weights are not updated.\nluz moved the test data to the GPU, batch by batch, and obtained model predictions.\nThese predictions were then moved back to the CPU, in anticipation of the caller wanting to process them further with R. (Conversion functions like as.numeric(), as.matrix() etc. can only act on CPU-resident tensors.)\n\nIn the below appendix, you find a complete walk-through of how to implement the train-validate-test workflow by hand. You’ll likely find this a lot more complex than what we did above – and it does not even bring into play metrics, or any of the functionality afforded by luz callbacks.\nIn the next chapter, we discuss essential ingredients of modern deep learning we haven’t yet touched upon; and following that, we look at specific architectures destined to specifically handle different tasks and domains."
  },
  {
    "objectID": "training_with_luz.html#appendix-a-train-validate-test-workflow-implemented-by-hand",
    "href": "training_with_luz.html#appendix-a-train-validate-test-workflow-implemented-by-hand",
    "title": "14  Training with luz",
    "section": "14.4 Appendix: A train-validate-test workflow implemented by hand",
    "text": "14.4 Appendix: A train-validate-test workflow implemented by hand\nFor clarity, we repeat here the two things that do not depend on whether you’re using luz or not: dataloader() preparation and model definition.\n\n# input dimensionality (number of input features)\nd_in &lt;- 3\n# number of observations in training set\nn &lt;- 1000\n\nx &lt;- torch_randn(n, d_in)\ncoefs &lt;- c(0.2, -1.3, -0.5)\ny &lt;- x$matmul(coefs)$unsqueeze(2) + torch_randn(n, 1)\n\nds &lt;- tensor_dataset(x, y)\n\ndl &lt;- dataloader(ds, batch_size = 100, shuffle = TRUE)\n\ntrain_ids &lt;- sample(1:length(ds), size = 0.6 * length(ds))\nvalid_ids &lt;- sample(setdiff(\n  1:length(ds),\n  train_ids\n), size = 0.2 * length(ds))\ntest_ids &lt;- setdiff(1:length(ds), union(train_ids, valid_ids))\n\ntrain_ds &lt;- dataset_subset(ds, indices = train_ids)\nvalid_ds &lt;- dataset_subset(ds, indices = valid_ids)\ntest_ds &lt;- dataset_subset(ds, indices = test_ids)\n\ntrain_dl &lt;- dataloader(train_ds,\n  batch_size = 100,\n  shuffle = TRUE\n)\nvalid_dl &lt;- dataloader(valid_ds, batch_size = 100)\ntest_dl &lt;- dataloader(test_ds, batch_size = 100)\n\n# dimensionality of hidden layer\nd_hidden &lt;- 32\n# output dimensionality (number of predicted features)\nd_out &lt;- 1\n\nnet &lt;- nn_module(\n  initialize = function(d_in, d_hidden, d_out) {\n    self$net &lt;- nn_sequential(\n      nn_linear(d_in, d_hidden),\n      nn_relu(),\n      nn_linear(d_hidden, d_out)\n    )\n  },\n  forward = function(x) {\n    self$net(x)\n  }\n)\n\nRecall that with luz, now all that separates you from watching how training and validation losses evolve is a snippet like this:\n\nfitted &lt;- net %&gt;%\n  setup(\n    loss = nn_mse_loss(),\n    optimizer = optim_adam\n  ) %&gt;%\n  set_hparams(\n    d_in = d_in,\n    d_hidden = d_hidden, d_out = d_out\n  ) %&gt;%\n  fit(train_dl, epochs = 200, valid_data = valid_dl)\n\nWithout luz, however, things to be taken care of fall into three distinct categories.\nFirst, instantiate the network, and, if CUDA is installed, move its weights to the GPU.\n\ndevice &lt;- torch_device(if\n(cuda_is_available()) {\n  \"cuda\"\n} else {\n  \"cpu\"\n})\n\nmodel &lt;- net(d_in = d_in, d_hidden = d_hidden, d_out = d_out)\nmodel &lt;- model$to(device = device)\n\nSecond, create an optimizer.\n\noptimizer &lt;- optim_adam(model$parameters)\n\nAnd third, the biggest chunk: In each epoch, iterate over training batches as well as validation batches, performing backpropagation when working on the former, while just passively reporting losses when processing the latter.\nFor clarity, we pack training logic and validation logic each into their own functions. train_batch() and valid_batch() will be called from inside loops over the respective batches. Those loops, in turn, will be executed for every epoch.\nWhile train_batch() and valid_batch(), per se, trigger the usual actions in the usual order, note the device placement calls: For the model to be able to take in the data, they have to live on the same device. Then, for mean-squared-error computation to be possible, the target tensors need to live there as well.\n\ntrain_batch &lt;- function(b) {\n  optimizer$zero_grad()\n  output &lt;- model(b[[1]]$to(device = device))\n  target &lt;- b[[2]]$to(device = device)\n\n  loss &lt;- nn_mse_loss(output, target)\n  loss$backward()\n  optimizer$step()\n\n  loss$item()\n}\n\nvalid_batch &lt;- function(b) {\n  output &lt;- model(b[[1]]$to(device = device))\n  target &lt;- b[[2]]$to(device = device)\n\n  loss &lt;- nn_mse_loss(output, target)\n  loss$item()\n}\n\nThe loop over epochs contains two lines that deserve special attention: model$train() and model$eval(). The former instructs torch to put the model in training mode; the latter does the opposite. With the simple model we’re using here, it wouldn’t be a problem if you forgot those calls; however, when later we’ll be using regularization layers like nn_dropout() and nn_batch_norm2d(), calling these methods in the correct places is essential. This is because these layers behave differently during evaluation and training.\n\nnum_epochs &lt;- 200\n\nfor (epoch in 1:num_epochs) {\n  model$train()\n  train_loss &lt;- c()\n\n  # use coro::loop() for stability and performance\n  coro::loop(for (b in train_dl) {\n    loss &lt;- train_batch(b)\n    train_loss &lt;- c(train_loss, loss)\n  })\n\n  cat(sprintf(\n    \"\\nEpoch %d, training: loss: %3.5f \\n\",\n    epoch, mean(train_loss)\n  ))\n\n  model$eval()\n  valid_loss &lt;- c()\n\n  # disable gradient tracking to reduce memory usage\n  with_no_grad({ \n    coro::loop(for (b in valid_dl) {\n      loss &lt;- valid_batch(b)\n      valid_loss &lt;- c(valid_loss, loss)\n    })  \n  })\n  \n  cat(sprintf(\n    \"\\nEpoch %d, validation: loss: %3.5f \\n\",\n    epoch, mean(valid_loss)\n  ))\n}\n\nThis completes our walk-through of manual training, and should have made more concrete my assertion that using luz significantly reduces the potential for casual (e.g., copy-paste) errors."
  },
  {
    "objectID": "image_classification_1.html#what-does-it-take-to-classify-an-image",
    "href": "image_classification_1.html#what-does-it-take-to-classify-an-image",
    "title": "15  A first go at image classification",
    "section": "15.1 What does it take to classify an image?",
    "text": "15.1 What does it take to classify an image?\nThink about how we, as human beings, can say “that’s a cat”, or: “this is a dog”. No conscious processing is required. (Usually, that is.)\nWhy? The neuroscience, and cognitive psychology, involved are definitely out of scope for this book; but on a high level, we can assume that there are at least two prerequisites: First, that our visual system be able to build up complex representations out of lower-level ones, and second, that we have a set of concepts available we can map those high-level representations to. Presumably, then, an algorithm expected to do the same thing needs to be endowed with these same capabilities.\nIn the context of this chapter, dedicated to image classification, the second prerequisite is satisfied gratuitously. Classification being a variant of supervised machine learning, the concepts are given by means of the targets. The first, however, is all-important. We can again distinguish two components: the capability to detect low-level features, and that to successively compose them into higher-level ones.\nTake a simple example. What would be required to identify a rectangle? A rectangle consists of edges: straight-ish borders of sort where something in the visual impression (color, for example) changes. To start with, then, the algorithm would have to be able to identify a single edge. That “edge extractor”, as we might call it, is going to mark all four edges in the image. In this case, no further composition of features is needed; we can directly infer the concept.\nOn the other hand, assume the image were showing a house built of bricks. Then, there would be many rectangles, together forming a wall of the house; another rectangle, the door; and a few further ones, the windows. Maybe there’d be a different arrangement of edges, triangle-shaped, the roof. Meaning, an edge detector is not enough: We also need a “rectangle detector”, a “triangle detector”, a “wall detector”, a “roof detector” … and so on. Evidently, these detectors can’t all be programmed up front. They’ll have to be emergent properties of the algorithm: the neural network, in our case."
  },
  {
    "objectID": "image_classification_1.html#neural-networks-for-feature-detection-and-feature-emergence",
    "href": "image_classification_1.html#neural-networks-for-feature-detection-and-feature-emergence",
    "title": "15  A first go at image classification",
    "section": "15.2 Neural networks for feature detection and feature emergence",
    "text": "15.2 Neural networks for feature detection and feature emergence\nThe way we’ve spelled out the requirements, a neural network for image classification needs to (1) be able to detect features, and (2) build up a hierarchy of such. Networks being networks, we can safely assume that (1) will be taken care of by a specialized layer (module), while (2) will be made possible by chaining several layers.\n\n15.2.1 Detecting low-level features with cross-correlation\nThis chapter is about “convolutional” neural networks; the specialized module in question is the “convolutional” one. Why, then, am I talking about cross-correlation? It’s because what neural-network people refer to as convolution, technically is cross-correlation. (Don’t worry – I’ll be making the distinction just here, in the conceptual introduction; afterwards I’ll be saying “convolution”, just like everyone else.)\nSo why am I insisting? It is for two reasons. First, this book actually has a chapter on convolution – the “real one”; it figures in part three right between matrix operations and the Discrete Fourier Transform. Second, while in a formal sense the difference may be small, semantically as well as in terms of mathematical status, convolution and cross-correlation are decidedly distinct. In broad strokes:\nConvolution may well be the most fundamental operation in all of signal processing, fundamental in the way addition and multiplication are. It can act as a filter, a signal-space transformation intended to achieve a desired result. For example, a moving average filter can be programmed as a convolution. So can, however, something quite the opposite: a filter that emphasizes differences. (An edge enhancer would be an example of the latter.)\nCross-correlation, in contrast, is more specialized. It finds things, or put differently: It spots similarities. This is what is needed in image recognition. To demonstrate how it works, we start in a single dimension.\n\n15.2.1.1 Cross-correlation in one dimension\nAssume we have a signal – a univariate time series – that looks like this: 0,1,1,1,-1,0,-1,-1,1,1,1,-1. We want to find locations where a one occurs three times in a row. To that end, we make use of a filter that, too, has three ones in a row: 1,1,1.\nThat filter, also called a kernel, is going to slide over the input sequence, producing an output value at every location. To be precise: The output value in question will be mapped to the input value co-located with the kernel’s central value. How, then, can we obtain an output for the very first input value, which has no way of being mapped to the center of the kernel? In order for this to work, the input sequence is padded with zeroes: one in front, and one at the end. The new signal looks like this: 0,0,1,1,1,-1,0,-1,-1,1,1,1,-1,0 .\nNow, we have the kernel sliding over the signal. Like so:\n0,0,1,1,1,-1,0,-1,-1,1,1,1,-1,0\n1,1,1\n\n0,0,1,1,1,-1,0,-1,-1,1,1,1,-1,0\n   1,1,1\n\n0,0,1,1,1,-1,0,-1,-1,1,1,1,-1,0\n      1,1, 1\nAnd so on.\nAt every position, products are computed between the mapped input and kernel values, and then, those products are added up, to yield the output value at the central position. For example, this is what gets computed at the very first matching: 0*1 + 0*1 + 1*1 = 1. Appending the outputs, we get a new sequence: 1,2,3,1,0,-2,-2,-1,1,3,1,0 .\nHow does this help in finding three consecutive ones? Well, a three can only result when the kernel has found such a location. Thus, with that choice of kernel, we take every occurrence of 3 in the output as the center of the target sequence we’re looking for.\n\n\n15.2.1.2 Cross-correlation in two dimensions\nThis chapter is about images; how does that logic carry over to two dimensions?\nEverything works just the same; it’s just that now, the input signal extends over two dimensions, and the kernel is two-dimensional, as well. Again, the input is padded; with a kernel of size 3 x 3, for example, one row is added on top and bottom each, and one column, on the left and the right. Again, the kernel slides over the image, row by row and column by column. At each point it computes an aggregate that is the sum of point-wise products. Mathematically, that’s a dot product.\nTo get a feel for how this works, we look at a bare-bones example: a white square on black background (fig. 15.1).\n\n\n\nFigure 15.1: White square on black background.\n\n\nNicely, the open-source graphics program Gimp has a feature that allows one to experiment with custom filters (“Filters” -&gt; “Custom” -&gt; “Convolution matrix”). We can construct kernels and directly examine their effects.\nSay we want to find the left edge of the square. We are looking for locations where the color changes, horizontally, from black to white. This can be achieved with a 3x3 kernel that looks like this (fig. 15.2):\n 0 0 0\n-1 1 0\n 0 0 0\nThis kernel is similar to the edge type we’re interested in in that it has, in the second row, a horizontal transition from -1 to 1.\nAnalogously, kernels can be constructed that extract the right (fig. 15.3), top (fig. 15.4), and bottom (fig. 15.5) edges.\n\n\n\nFigure 15.2: Gimp convolution matrix that detects the left edge.\n\n\n\n\n\nFigure 15.3: Gimp convolution matrix that detects the right edge.\n\n\n\n\n\nFigure 15.4: Gimp convolution matrix that detects the top edge.\n\n\n\n\n\nFigure 15.5: Gimp convolution matrix that detects the bottom edge.\n\n\nTo understand this numerically, we can simulate a tiny image (fig. 15.6, left). The numbers represent a grayscale image with values ranging from 0 to 255. To its right, we have the kernel; this is the one we used to detect the left edge. As a result of having that kernel slide over the image, we obtain the “image” on the right. 0 being the lowest possible value, negative pixels end up black, and we obtain a white edge on black background, just like we saw with Gimp.\n\n\n\nFigure 15.6: Input image, filter, and result as pixel values. Negative pixel values being impossible, -255 will end up as 0.\n\n\nNow, we’ve talked a lot about constructing kernels. Neural networks are all about learning feature detectors, not having them programmed up-front. Naturally, then, learning a filter means having a layer type whose weights embody this logic.\n\n\n15.2.1.3 Convolutional layers in torch\nSo far, the only layer type we’ve seen that learns weights is nn_linear(). nn_linear() performs an affine operation: It takes an input tensor, matrix-multiplies it by its weight matrix \\(\\mathbf{W}\\), and adds the bias vector \\(\\mathbf{b}\\). While there is just a single bias per layer, independently of the number of neurons it has, this is not the case for the weights: There is a unique connection between each feature in the input tensor and each of the layer’s neurons.\nThis is not true for nn_conv2d(), torch’s (two-dimensional) convolution1 layer.\nBack to how convolutional layers differ from linear ones. We’ve already seen what the layer’s effect is supposed to be: A kernel should slide over its input, generating an output value at each location. Now the kernel, for a convolutional layer, is exactly its weight matrix. The kernel sliding over an input image means that weights are re-used every time it shifts its position. Thus, the number of weights is determined by the size of the kernel, not the size of the input. As a consequence, a convolutional layer is way more economical than a linear one.\nAnother way to express this is the following.\nConceptually, we are looking for the same thing, wherever it appears in the image. Take the most standard of standard image classification benchmarks, MNIST. It is about classifying images of the Arabic numerals 0-9. Say we want to learn the shape for a 2. The 2 could be right in the middle of the image, or it could be shifted to the left (say). An algorithm should be able to recognize it no matter where. Additional requirements depend on the task. If all we need to be able to do is say “that’s a 2”, we’re good to use an algorithm that is translation-invariant: It outputs the same thing independently of any shifts that may have occurred. For classification, that’s just fine: A 2 is a 2 is a 2.\nAnother important task, though, is image segmentation (something we’ll look at in an upcoming chapter). In segmentation, we want to mark all pixels in an image according to whether they are part of some object or not. Think tumor cells, for example. The 2 is still a 2, but we do need the information where in the image it is located. The algorithm to use now has to be translation-equivariant: If a shift has occurred, the target is still detected, but at a new location. And thinking about the convolution algorithm, translation-equivariant is exactly what it is.\nSo now, we have an idea how torch lets us detect individual features in an image. This gives us the first in our list of desiderates. The second is about combining feature detectors, that is, building up a hierarchy, in order to discern more and more specialized types of objects. This means that from a single layer, we move on to a network of layers.\n\n\n\n15.2.2 Build up feature hierarchies\nA prototypical convolutional neural network for image classification will chain blocks composed of three types of layers: convolutional ones (nn_conv1d(), nn_conv2d(), or nn_conv3d(), depending on the dimension we’re in), activation layers (e.g., nn_relu()), and pooling layers (e.g., nn_maxpool1d(), nn_maxpool2d(), nn_maxpool3d()).\nThe only type we haven’t talked about yet are the pooling layers. Just like activation layers, these don’t have any parameters; what they do is aggregate neighboring tensor values. The size of the region to summarize is specified in the layer constructor’s parameters. Various types of aggregation are available: nn_maxpool&lt;n&gt;d() picks the highest value, while nn_avg_pool&lt;n&gt;d() computes the average.\nWhy would one want to perform these kinds of aggregation? Practically speaking, one has to if one wants to arrive at a per-image (as opposed to per-pixel) output. But we can’t just choose any way of aggregating spatially-arranged values. Picture, for example, an average where the interior pixels of an image patch were weighted higher than the exterior ones. Then, it would make a difference where in the patch some object was located. But for classification, this should not be the case. For classification, as opposed to segmentation, we want translation invariance – not just equivariance, the property we just said convolution has. And translation-invariant is just what layers like nn_maxpool2d(), nn_avgpool2d(), etc. are.\n\n15.2.2.1 A prototypical convnet\nA template for a convolutional network, called “convnet” from now on, could thus look as below. To preempt any possible confusion: Even though, above, I was talking about three types of layers, there really is just one type in the code: the convolutional one. For brevity, both ReLU activation and max pooling are realized as functions instead.\nHere is a possible template. It is not intended as a recommendation (as to number of filters, kernel size, or other hyperparameters, for example) – just to illustrate the mechanics. More detailed comments follow.\n\nlibrary(torch)\n\nconvnet &lt;- nn_module(\n  \"convnet\",\n  \n  initialize = function() {\n    \n    # nn_conv2d(in_channels, out_channels, kernel_size)\n    self$conv1 &lt;- nn_conv2d(1, 16, 3)\n    self$conv2 &lt;- nn_conv2d(16, 32, 3)\n    self$conv3 &lt;- nn_conv2d(32, 64, 3)\n    \n    self$output &lt;- nn_linear(2304, 3)\n\n  },\n  \n  forward = function(x) {\n    \n    x %&gt;% \n      self$conv1() %&gt;% \n      nnf_relu() %&gt;%\n      nnf_max_pool2d(2) %&gt;%\n      self$conv2() %&gt;% \n      nnf_relu() %&gt;%\n      nnf_max_pool2d(2) %&gt;%\n      self$conv3() %&gt;% \n      nnf_relu() %&gt;%\n      nnf_max_pool2d(2) %&gt;%\n      torch_flatten(start_dim = 2) %&gt;%\n      self$output()\n      \n  }\n)\n\nmodel &lt;- convnet()\n\nTo understand what is going on, we need to know how images are represented in torch. By itself, an image is represented as a three-dimensional tensor, with one dimension indexing into available channels (package luz)} (one for gray-scale images, three for RGB, possibly more for different kinds of imaging outputs), and the other two, corresponding to the two spatial axes, height (rows) and width (columns). In deep learning, we work with batches; thus, there is an additional dimension – the very first one – that refers to batch number.\nLet’s look at an example image that may be used with the above template:\n\nimg &lt;- torch_randn(1, 1, 64, 64)\n\nWhat we have here is an image, or more precisely, a batch containing a single image, that has a single channel, and is of size 64 x 64.\nThat said, the above template assumes the following:\n\nThe input image has one channel. That’s why the first argument to nn_conv2d() is 1 when we construct the first of the conv layers. (No assumptions are made, on the other hand, about the size of the input image.)\nWe want to distinguish between three different target classes. This means that the output layer, a linear module, needs to have three output channels.\n\nTo test the code, we can call the un-trained model on our example image:\n\nmodel(img)\n\ntorch_tensor\n0.01 *\n 6.4821  3.4166 -5.6050\n[ CPUFloatType{1,3} ][ grad_fn = &lt;AddmmBackward0&gt; ]\nOne final note about that template. When you were reading the code above, one line that might have stood out is the following:\n\nself$output &lt;- nn_linear(2304, 3)\n\nHow did that 2304, the number of input connections to nn_linear(), come about? It is the result of (1) a number of operations that each reduce spatial resolution, plus (2) a flattening operation that removes all dimensional information besides the batch dimension. This will make more sense once we’ve discussed the arguments to the layers in question. But one thing needs to be said upfront: If this sounds like magic, there is a simple means to make the magic go away. Namely, a simple way to find out about tensor shapes at any stage in a network is to comment all subsequent actions in forward(), and call the modified model. Naturally, this should not replace understanding, but it’s a great way not to lose one’s nerves when encountering shape errors.\nNow, about layer arguments.\n\n\n15.2.2.2 Arguments to nn_conv2d()\nAbove, we passed three arguments to nn_conv2d(): in_channels, out_channels, and kernel_size. This is not an exhaustive list of parameters, though. The remaining ones all have default values, but it is important to know about their existence. We’re going to elaborate on three of them, all of whom you’re likely to play with applying the template to some concrete task. All of them affect output size. So do two of the three mandatory arguments, out_channels and kernel_size:\n\nout_channels refers to the number of kernels (often called filters, in this context) learned. Its value affects the second of the four dimensions of the output tensor; it does not affect spatial resolution, though. Learning more filters adds capacity to the network, as it increases the number of weights.\nkernel_size, on the other hand, does alter spatial resolution – unless its value is 1, in which case the kernel never exceeds image boundaries. Like out_channels, it is a candidate for experimentation. In general, though, it is advisable to keep kernel size rather small, and chain a larger number of convolutional layers, instead of enlarging kernel size in a “shallow” network.\n\nNow for the three non-mandatory arguments to explore.\n\npadding is something we’ve encountered before. Any kernel that extends over more than a single pixel will move outside the valid region when sliding over an image; the more, the bigger the kernel. General options are to (1) either pad the image (with zeroes, for example), or (2) compute the dot product only where possible. In the latter case, spatial resolution will decrease. That need not in itself be a problem; like so many things, it’s a matter of experimentation. By default, torch does not pad images; however by passing a value greater than 0 for padding, you can ensure that spatial resolution is preserved, whatever the kernel size. Compare fig. 15.7, reproduced from a nice compilation by Dumoulin and Visin (2016), to see the effect of padding.\nstride refers to the way a kernel moves over the image. With a stride greater than 1, it takes “leaps” of sorts – see fig. 15.8. This results in fewer “snapshots” being taken. As a result, spatial resolution decreases.\nA setting of dilation greater than 1, too, results in fewer snapshots, but for a different reason. Now, it’s not that the kernel moves faster. Instead, the pixels it is applied to are not adjacent anymore. They’re spread out – how much, depends on the argument’s value. See fig. 15.9.\n\n\n\n\nFigure 15.7: Convolution, and the effect of padding. Copyright Dumoulin and Visin (2016), reproduced under MIT license.\n\n\n\n\n\nFigure 15.8: Convolution, and the effect of strides. Copyright Dumoulin and Visin (2016), reproduced under MIT license.\n\n\n\n\n\nFigure 15.9: Convolution, and the effect of dilation. Copyright Dumoulin and Visin (2016), reproduced under MIT license.\n\n\nFor non-mandatory arguments padding, stride, and dilation, tbl. 15.1 has a summary of defaults and effects.\n\n\nTable 15.1: Arguments to nn_conv_2d() you may want to experiment with – default values and non-default actions.\n\n\n\n\n\n\n\nArgument\nDefault\nAction (if non-default)\n\n\n\n\npadding\n0\nvirtual rows/columns added around the image\n\n\nstride\n1\nkernel moves across image at bigger step size (“jumps” over pixels)\n\n\ndilation\n1\nkernel is applied to spread-out image pixels (“holes” in kernel)\n\n\n\n\n\n\n15.2.2.3 Arguments to pooling layers\nPooling layers compute aggregates over neighboring pixels. The number of pixels of aggregate over in every dimension is specified in the layer constructor’s first argument (alternatively, the corresponding function’s second argument). Slightly misleadingly, that argument is called kernel_size, although there are no weights involved: For example, in the above template, we were unconditionally taking the maximum pixel value over regions of size 2 x 2.\nIn analogy to convolution layers, pooling layers also accept arguments padding and stride. However, they are seldom used.\n\n\n15.2.2.4 Zooming out\nWe’ve talked a lot about layers and their arguments. Let’s zoom out and think back about the general template, and what it is supposed to achieve.\nWe are chaining blocks that, each, perform a convolution, apply a non-linearity, and spatially aggregate the result. Each block’s weights act as feature detectors, and every block but the first receives as input something that already is the result of applying one or more feature detectors. The magical thing that happens, and the reason behind the success of convnets, is that by chaining layers, a hierarchy of features is built. Early layers detect edges and textures, later ones, patterns of various complexity, and the final ones, objects and parts of objects (see fig. 15.10, a beautiful visualization reproduced from Olah, Mordvintsev, and Schubert (2017)).\n\n\n\nFigure 15.10: Feature visualization on a subset of layers of GoogleNet. Figure from Olah, Mordvintsev, and Schubert (2017), reproduced under Creative Commons Attribution CC-BY 4.0 without modification.\n\n\nWe now know enough about coding convnets and how they work to explore a real example."
  },
  {
    "objectID": "image_classification_1.html#classification-on-tiny-imagenet",
    "href": "image_classification_1.html#classification-on-tiny-imagenet",
    "title": "15  A first go at image classification",
    "section": "15.3 Classification on Tiny Imagenet",
    "text": "15.3 Classification on Tiny Imagenet\nBefore we start coding, let me anchor your expectations. In this chapter, we design and train a basic convnet from scratch. As to data pre-processing, we do what is needed, not more. In the next two chapters, we’ll learn about common techniques used to improve model training, in terms of quality as well as speed. Once we’ve covered those, we’ll pick up right where this chapter ended, and apply a few of those techniques to the present task. Therefore, what we’re doing here is build a baseline, to be used in comparison with more sophisticated approaches. This is “just” a beginning.\n\n15.3.1 Data pre-processing\nIn addition to torch and luz, we load a third package from the torch ecosystem: torchvision. torchvision provides operations on images, as well as a set of pre-trained models and common benchmark datasets.\n\nlibrary(torch)\nlibrary(torchvision)\nlibrary(luz)\n\nThe dataset we use is “Tiny Imagenet”. Tiny Imagenet is a subset of ImageNet, a gigantic collection of more than fourteen million images, initially made popular through the “ImageNet Large Scale Visual Recognition Challenge” that was run between 2012 and 2017. In the challenge, the most popular task was multi-class classification, with one thousand different classes to choose from.\nOne thousand classes is a lot; and with images typically being processed at a resolution of 256 x 256, training a model takes a lot of time, even on luxurious hardware. For that reason, a more manageable version was created as part of a popular Stanford class on deep learning for images, Convolutional Neural Networks for Visual Recognition (CS231n). The condensed dataset has two hundred classes, with five hundred training images per class. Two hundred classes, that’s still a lot! (Most introductory examples will do “cats vs. dogs”, or some other binary problem.) Thus, it’s not an easy task.\nWe start by downloading the data.\n\nset.seed(777)\ntorch_manual_seed(777)\n\ndir &lt;- \"~/.torch-datasets\"\n\ntrain_ds &lt;- tiny_imagenet_dataset(\n  dir,\n  download = TRUE,\n  transform = function(x) {\n    x %&gt;%\n      transform_to_tensor() \n  }\n)\n\nvalid_ds &lt;- tiny_imagenet_dataset(\n  dir,\n  split = \"val\",\n  transform = function(x) {\n    x %&gt;%\n      transform_to_tensor()\n  }\n)\n\nNotice how tiny_imagenet_dataset() takes an argument called transform. This is used to specify operations to be performed as part of the input pipeline. Here, not much is happening: We just convert images to something we can work with, tensors. However, very soon we’ll see this argument used to specify sequences of transformations such as resizing, cropping, rotation, and more.\nWhat remains to be done is create the data loaders.\n\ntrain_dl &lt;- dataloader(train_ds,\n  batch_size = 128,\n  shuffle = TRUE\n)\nvalid_dl &lt;- dataloader(valid_ds, batch_size = 128)\n\nImages are RGB, and of size 64 x 64:\n\nbatch &lt;- train_dl %&gt;%\n  dataloader_make_iter() %&gt;%\n  dataloader_next()\n\ndim(batch$x)\n\n[1] 128   3  64  64\nClasses are integers between 1 to 200:\n\nbatch$y\n\ntorch_tensor\n 172\n  17\n  76\n  78\n 111\n  57\n   8\n 166\n 146\n 114\n  41\n  28\n 138\n  98\n  57\n  98\n  25\n 148\n 166\n 135\n  31\n 182\n  48\n 184\n 160\n 166\n  40\n 115\n 161\n  21\n... [the output was truncated (use n=-1 to disable)]\n[ CPULongType{128} ]\nNow we define a convnet, and train it with luz.\n\n\n15.3.2 Image classification from scratch\nHere is a prototypical convnet, modeled after our template, but more powerful.\nIn addition to what we’ve seen already, the code illustrates a way of modularizing the code, arranging layers into three groups:\n\na (large) feature detector that, as a whole, is shift-equivariant;\na shift-invariant pooling layer (nn_adaptive_avg_pool2d()) that allows us to specify a desired output resolution; and\na feed-forward neural network that takes the computed features and uses them to produce final scores: two hundred values, corresponding to two hundred classes, for each item in the batch.\n\n\nconvnet &lt;- nn_module(\n  \"convnet\",\n  initialize = function() {\n    self$features &lt;- nn_sequential(\n      nn_conv2d(3, 64, kernel_size = 3, padding = 1),\n      nn_relu(),\n      nn_max_pool2d(kernel_size = 2),\n      nn_conv2d(64, 128, kernel_size = 3, padding = 1),\n      nn_relu(),\n      nn_max_pool2d(kernel_size = 2),\n      nn_conv2d(128, 256, kernel_size = 3, padding = 1),\n      nn_relu(),\n      nn_max_pool2d(kernel_size = 2),\n      nn_conv2d(256, 512, kernel_size = 3, padding = 1),\n      nn_relu(),\n      nn_max_pool2d(kernel_size = 2),\n      nn_conv2d(512, 1024, kernel_size = 3, padding = 1),\n      nn_relu(),\n      nn_adaptive_avg_pool2d(c(1, 1))\n    )\n    self$classifier &lt;- nn_sequential(\n      nn_linear(1024, 1024),\n      nn_relu(),\n      nn_linear(1024, 1024),\n      nn_relu(),\n      nn_linear(1024, 200)\n    )\n  },\n  forward = function(x) {\n    x &lt;- self$features(x)$squeeze()\n    x &lt;- self$classifier(x)\n    x\n  }\n)\n\nNow, we train the network. The classifier outputs raw logits, not probabilities; this means we need to make use of nn_cross_entropy_loss(). We train for fifty epochs:\n\nfitted &lt;- convnet %&gt;%\n  setup(\n    loss = nn_cross_entropy_loss(),\n    optimizer = optim_adam,\n    metrics = list(\n      luz_metric_accuracy()\n    )\n  ) %&gt;%\n  fit(train_dl,\n      epochs = 50,\n      valid_data = valid_dl,\n      verbose = TRUE\n      )\n\nAfter fifty epochs, this resulted in accuracy values of 0.92 and 0.22, on the training and test sets, respectively. This is quite a difference! On the training set, this model is near-perfect; on the test set, it only gets up to every fourth image correct.\nEpoch 1/50\nTrain metrics: Loss: 5.0822 - Acc: 0.0146                                     \nValid metrics: Loss: 4.8564 - Acc: 0.0269\nEpoch 2/50\nTrain metrics: Loss: 4.5545 - Acc: 0.0571                                     \nValid metrics: Loss: 4.2592 - Acc: 0.0904\nEpoch 3/50\nTrain metrics: Loss: 4.0727 - Acc: 0.1122                                     \nValid metrics: Loss: 3.9097 - Acc: 0.1381\n...\n...\nEpoch 48/50\nTrain metrics: Loss: 0.3033 - Acc: 0.9064                                     \nValid metrics: Loss: 10.2999 - Acc: 0.2188\nEpoch 49/50\nTrain metrics: Loss: 0.2932 - Acc: 0.9098                                     \nValid metrics: Loss: 10.7348 - Acc: 0.222\nEpoch 50/50\nTrain metrics: Loss: 0.2733 - Acc: 0.9152                                     \nValid metrics: Loss: 10.641 - Acc: 0.2204\nWith two hundred options to choose from, “every fourth” does not even seem so bad; however, looking at the enormous difference between both metrics, something is not quite right. The model has severely overfitted to the training set – memorized the training samples, in other words. Overfitting is not specific to deep learning; it is the nemesis of all of machine learning. We’ll consecrate the whole next chapter to this topic.\nBefore we end, though, let’s see how we would use luz to obtain predictions:\n\npreds &lt;- last %&gt;% predict(valid_dl)\n\npredict() directly returns what is output by the model: two hundred non-normalized scores for each item. That’s because the model’s last layer is a linear module, with no activation applied. (Remember how the loss function, nn_cross_entropy_loss(), applies a softmax operation before calculating cross-entropy.)\nNow, we could certainly call nnf_softmax() ourselves, converting outputs from predict() to probabilities:\n\npreds &lt;- nnf_softmax(preds, dim = 2)\n\nHowever, if we’re just interested in determining the most likely class, we can as well skip the normalization step, and directly pick the highest value for each batch item:\n\ntorch_argmax(preds, dim = 2)\n\ntorch_tensor\n  55\n   1\n   1\n   1\n   1\n   1\n   1\n  89\n  45\n   1\n   1\n  19\n   1\n 190\n  14\n   1\n 185\n   1\n   1\n 150\n  77\n  37\n 131\n 193\n  80\n   1\n   1\n  45\n   1\n 131\n... [the output was truncated (use n=-1 to disable)]\n[ CUDALongType{10000} ]\nWe could now go on to compare predictions with actual classes, looking for inspiration on what could be done better. But at this stage, there is still a lot that can be done better! We will return to this application in due time, but first, we need to learn about overfitting, and ways to speed up model training.\n\n\n\n\nDumoulin, Vincent, and Francesco Visin. 2016. “A guide to convolution arithmetic for deep learning.” arXiv e-Prints, March, arXiv:1603.07285. https://arxiv.org/abs/1603.07285.\n\n\nOlah, Chris, Alexander Mordvintsev, and Ludwig Schubert. 2017. “Feature Visualization.” Distill. https://doi.org/10.23915/distill.00007."
  },
  {
    "objectID": "image_classification_1.html#footnotes",
    "href": "image_classification_1.html#footnotes",
    "title": "15  A first go at image classification",
    "section": "",
    "text": "Like I said above, I’ll be using the established term “convolution” from now on. Actually – given that weights are learned – it does not matter that much anyway.↩︎"
  },
  {
    "objectID": "overfitting.html#the-royal-road-more-and-more-representative-data",
    "href": "overfitting.html#the-royal-road-more-and-more-representative-data",
    "title": "16  Making models generalize",
    "section": "16.1 The royal road: more – and more representative! – data",
    "text": "16.1 The royal road: more – and more representative! – data\nDepending on your situation, you may be forced to work with pre-existing datasets. In that case, you may still have the option of supplementing external sources with data you’ve collected yourself, albeit of lesser quantity.\nIn the next chapter, we’ll encounter the technique of transfer learning. Transfer learning means making use of pre-trained models, employing them as feature extractors for your “downstream” task. Often, these models have been trained on huge datasets. In consequence, they are not just “very good at what they do”, but they also generalize to unseen data – provided the new data are similar to those they have been trained on. What if they’re not? For many tasks, it will still be possible to make use of a pre-trained model: Take it the way it comes, and go on training it – but now, adding in the kinds of data you want it to generalize to.\nOf course, being able to add in any data of your own may still be a dream. In that case, all you can do is think hard about how your results will be biased, and be honest about it.\nNow, let’s say you’ve thought it through, and are confident that there are no systematic deficiencies in your training data preventing generalization to use cases in the wild. Or maybe you’ve restricted the application domain of your model as required. Then, if you have a small training set and want it to generalize as much as possible, what can you do?"
  },
  {
    "objectID": "overfitting.html#pre-processing-stage-data-augmentation",
    "href": "overfitting.html#pre-processing-stage-data-augmentation",
    "title": "16  Making models generalize",
    "section": "16.2 Pre-processing stage: Data augmentation",
    "text": "16.2 Pre-processing stage: Data augmentation\nData augmentation means taking the data you have and modifying them, so as to force the algorithm to abstract over some things. What things? It depends on the domain operated upon. This should become clear by means of a concrete example.\nIn this chapter, I’ll introduce two popular variants of data augmentation: one I’ll refer to as “classic”, the other one going by the name of “mixup”.\n\n16.2.1 Classic data augmentation\nClassically, when people talk about (image) data augmentation, it is the following they’re having in mind. You take an image, and apply some random transformation to it. That transformation could be geometric, such as when rotating, translating, or scaling the image. Alternatively, instead of moving things around, the operation could affect the colors, as when changing brightness or saturation. Other options include blurring the image, or, quite the contrary, sharpening it. Technically, you are free to implement whatever algorithm you desire – you don’t have to use any of the (numerous!) transformations provided by torchvision. In practice, though, you’ll likely find much of what you need among the transformations already available.\nIn our running example, we’ll work with MNIST, the Hello World of image-classification datasets we’ve already made quick use of before. It contains 70,000 images of the digits 0 to 9, split into training and test sets in a ratio of six to one. Like before, we get the data from torchvision.\nTo see how the digits appear without any data augmentation, take a look at the first thirty-two images in the test set (fig. 16.1):\n\nlibrary(torch)\nlibrary(torchvision)\nlibrary(luz)\n\ndir &lt;- \"~/.torch-datasets\"\n\nvalid_ds &lt;- mnist_dataset(\n  dir,\n  download = TRUE,\n  train = FALSE,\n  transform = transform_to_tensor\n)\n\nvalid_dl &lt;- dataloader(valid_ds, batch_size = 128)\n\n# a convenient way to obtain individual images without \n# manual iteration\ntest_images &lt;- coro::collect(\n  valid_dl, 1\n)[[1]]$x[1:32, 1, , ] %&gt;% as.array()\n\npar(mfrow = c(4, 8), mar = rep(0, 4), mai = rep(0, 4))\ntest_images %&gt;%\n  purrr::array_tree(1) %&gt;%\n  purrr::map(as.raster) %&gt;%\n  purrr::iwalk(~ {\n    plot(.x)\n  })\n\n\n\n\nFigure 16.1: MNIST: The first thirty-two images in the test set.\n\n\nNow, we use the training set to experiment with data augmentation. Like the dogs_vs_cats_dataset() we made use of in the last chapter – in fact, like all torchvision datasets – mnist_dataset() takes a transform argument, allowing you to pass in arbitrary transformations to be performed on the input images. Of the four transformations that appear in the code snippet below, one we have already seen: transform_to_tensor(), used to convert from R doubles to torch tensors. The other three, that all share the infix _random_, each trigger non-deterministic data augmentation: be it by flipping the image horizontally (transform_random_horizontal_flip()) or vertically (transform_random_vertical_flip()), or through rotations and translations(transform_random_affine()). In all cases, the amount of distortion is configurable.\n\ntrain_ds &lt;- mnist_dataset(\n  dir,\n  download = TRUE,\n  transform = . %&gt;%\n    transform_to_tensor() %&gt;%\n    # flip horizontally, with a probability of 0.5\n    transform_random_horizontal_flip(p = 0.5) %&gt;%\n    # flip vertically, with a probability of 0.5\n    transform_random_vertical_flip(p = 0.5) %&gt;%\n    # (1) rotate to the left or the right,\n    #     up to respective angles of 45 degrees\n    # (2) translate vertically or horizontally,\n    #     not exceeding 10% of total image width/height\n    transform_random_affine(\n      degrees = c(-45, 45),\n      translate = c(0.1, 0.1)\n    )\n)\n\nAgain, let’s look at a sample result (fig. 16.2).\n\ntrain_dl &lt;- dataloader(\n  train_ds,\n  batch_size = 128,\n  shuffle = TRUE\n)\n\ntrain_images &lt;- coro::collect(\n  train_dl, 1\n)[[1]]$x[1:32, 1, , ] %&gt;% as.array()\n\npar(mfrow = c(4, 8), mar = rep(0, 4), mai = rep(0, 4))\ntrain_images %&gt;%\n  purrr::array_tree(1) %&gt;%\n  purrr::map(as.raster) %&gt;%\n  purrr::iwalk(~ {\n    plot(.x)\n  })\n\n\n\n\nFigure 16.2: MNIST, with random rotations, translations, and flips.\n\n\nThe effects are clearly visible, and the ranges chosen for rotations and translations seem sensible. But let’s think about the flips. Does it actually make sense to include them?\nIn general, this would depend on the dataset, and more even, on the task. Think of a cat, comfortably residing on a fluffy sofa. If the cat were looking to the right instead of to the left, it would still be a cat; if it was positioned upside-down, we’d probably assume the image had been loaded incorrectly. Neither transformation would affect its “catness”. It’s different with digits, though. A flipped \\(1\\) is not the same as a \\(1\\), at least not in a default context. Thus, for MNIST, I’d rather go with rotations and translations only:\n\ntrain_ds &lt;- mnist_dataset(\n  dir,\n  download = TRUE,\n  transform = . %&gt;%\n    transform_to_tensor() %&gt;%\n    transform_random_affine(\n      degrees = c(-45, 45), translate = c(0.1, 0.1)\n    )\n)\n\ntrain_dl &lt;- dataloader(train_ds,\n  batch_size = 128,\n  shuffle = TRUE\n)\n\nNow, to compare what happens with and without augmentation, you’d separately train a model for both an augmented and a non-augmented version of the training set. Here is an example setup:\n\nconvnet &lt;- nn_module(\n  \"convnet\",\n  initialize = function() {\n    # nn_conv2d(in_channels, out_channels, kernel_size, stride)\n    self$conv1 &lt;- nn_conv2d(1, 32, 3, 1)\n    self$conv2 &lt;- nn_conv2d(32, 64, 3, 2)\n    self$conv3 &lt;- nn_conv2d(64, 128, 3, 1)\n    self$conv4 &lt;- nn_conv2d(128, 256, 3, 2)\n    self$conv5 &lt;- nn_conv2d(256, 10, 3, 2)\n  },\n  forward = function(x) {\n    x %&gt;%\n      self$conv1() %&gt;%\n      nnf_relu() %&gt;%\n      self$conv2() %&gt;%\n      nnf_relu() %&gt;%\n      self$conv3() %&gt;%\n      nnf_relu() %&gt;%\n      self$conv4() %&gt;%\n      nnf_relu() %&gt;%\n      self$conv5() %&gt;%\n      torch_squeeze()\n  }\n)\n\nfitted &lt;- convnet %&gt;%\n  setup(\n    loss = nn_cross_entropy_loss(),\n    optimizer = optim_adam,\n    metrics = list(\n      luz_metric_accuracy()\n    )\n  ) %&gt;%\n  fit(train_dl, epochs = 5, valid_data = valid_dl)\n\nWith MNIST, we have at our disposition a huge training set, and at the same time, we’re dealing with a very homogeneous domain. Those are exactly the conditions where we don’t expect to see much overfitting2.\nHowever, even with MNIST, you’ll notice that with augmentation, as well as the other “overfitting antidotes” to be introduced, it takes a lot longer to achieve better performance on the training than on the test set (if ever there is better performance on the training set!). For example, in the setup described above, with no data augmentation applied, training-set accuracy surpassed that on the test set from the third epoch onwards; whereas with augmentation, there was no sign of overfitting during all five epochs I trained for.\nNext – still in the realm of data augmentation – we look at a technique that is domain-independent; that is, it can be applied to all kinds of data, not just images.\n\n\n16.2.2 Mixup\nClassic data augmentation, whatever it may be doing to the entities involved – move them, distort them, blur them – it leaves them intact. A rotated cat is still a cat. Mixup (Zhang et al. 2017), on the other hand, takes two entities and “mixes them together”. With mixup, we may have something that’s half-cat and half-squirrel. Or rather, in practice, with strongly unequal mixing weights used, ninety percent squirrel and ten percent cat.\nAs an idea, mixup generalizes to any domain. We could mix time series, say, or categorical data of any kind. We could also mix numerical data, although it’s not clear why we would do it. After all, mixup is nothing else but linear combination: take two values \\(x1\\) and \\(x2\\), and construct \\(x3 = w1 x1 + w2 x2\\), where \\(w1\\) and \\(w2\\) are weights summing to one. In neural networks, linear combination of numerical values (“automatic mixup”) happens all the time, so that normally, we wouldn’t expect “manual mixup” to add much value.\nFor visual demonstration, however, images are still best. Starting from MNIST’s test set, we can apply mixup with different weight patterns – equal, very unequal, and somewhere in-between – and see what happens. luz has a function, called nnf_mixup(), that lets you play around with this.\n(By the way, I’m introducing this function just so you can picture (literally!) what is going on. To actually use mixup, all that is required is to pass the appropriate callback to fit(), and let setup() know which loss function you want to use.)\nBesides the input and target batches, nnf_mixup() expects to be passed the mixing weights, one value per batch item. We start with the most “tame” variant: with weights that are very unequal between classes. Every resultant image will be composed, to ninety percent, of the original item at that position, and to ten percent, of a randomly-chosen different one (fig. 16.3):\n\nfirst_batch &lt;- coro::collect(valid_dl, 1)[[1]]\n\nmixed &lt;- nnf_mixup(x = first_batch$x,\n                   y = first_batch$y,\n                   weight = torch_tensor(rep(0.9, 128)))\n\n\n\n\nFigure 16.3: Mixing up MNIST, with mixing weights of 0.9.\n\n\nDo you agree that the mixed-in digits are just barely visible, if at all? Still, the callback’s default configuration results in mixing ratios pretty close to this one. For MNIST, this probably is too cautious a choice. But think of datasets where the objects are less shape-like, less sharp-edged. Mixing two landscapes, at an equal-ish ratio, would result in total gibberish. And here, too, the task plays a role; not just the dataset per se. Mixing apples and oranges one-to-one can make sense if we’re looking for a higher-level concept – a superset, of sorts. But if all we’re looking for is to correctly discern oranges that look a bit like an apple, or apples that have something “orange-y” in them, then a ratio such as 9:1 might be just fine.\nTo develop an idea of what would happen for other proportions, let’s successively make the mixing ratio more equal.\nFirst, here (fig. 16.4) is 0.7:\n\nmixed &lt;- nnf_mixup(x = first_batch$x,\n                   y = first_batch$y,\n                   weight = torch_tensor(rep(0.7, 128)))\n\n\n\n\nFigure 16.4: Mixing up MNIST, with mixing weights of 0.7.\n\n\nAnd here (fig. 16.5), 0.5:\n\nmixed &lt;- nnf_mixup(x = first_batch$x,\n                   y = first_batch$y,\n                   weight = torch_tensor(rep(0.5, 128)))\n\n\n\n\nFigure 16.5: Mixing up MNIST, with mixing weights of 0.5.\n\n\nTo use mixup while training, all you need to do is:\n\nAdd luz_callback_mixup() to the list of callbacks passed to luz::fit(). The callback takes an optional parameter, alpha, used in determining the mixing ratios. I’d recommend starting with the default, though, and start tweaking from there.\nWrap the loss you’re using for the task (here, cross entropy) in nn_mixup_loss().\nUse the loss, not the accuracy metric, for monitoring training progress, since accuracy in this case is not well defined.\n\nHere is an example:\n\n# redefine the training set to not use augmentation\ntrain_ds &lt;- mnist_dataset(\n  dir,\n  download = TRUE,\n  transform = transform_to_tensor\n)\n\ntrain_dl &lt;- dataloader(train_ds,\n  batch_size = 128,\n  shuffle = TRUE\n)\n\nfitted &lt;- convnet %&gt;%\n  setup(\n    loss = nn_mixup_loss(torch::nn_cross_entropy_loss()),\n    optimizer = optim_adam\n  ) %&gt;%\n  fit(\n    train_dl,\n    epochs = 5,\n    valid_data = valid_dl,\n    callbacks = list(luz_callback_mixup())\n  )\n\nMixup is an appealing technique that makes a lot of intuitive sense. If you feel like, go ahead and experiment with it on different tasks and different types of data.\nNext, we move on to the next stage in the workflow: model definition."
  },
  {
    "objectID": "overfitting.html#modeling-stage-dropout-and-regularization",
    "href": "overfitting.html#modeling-stage-dropout-and-regularization",
    "title": "16  Making models generalize",
    "section": "16.3 Modeling stage: dropout and regularization",
    "text": "16.3 Modeling stage: dropout and regularization\nInside a neural network, there are two kinds of “data”: activations – tensors propagated from one layer to the next – and weights, tensors associated with individual layers. From the two techniques we’ll look at in this section, one (dropout) affects the former; the other (regularization), the latter.\n\n16.3.1 Dropout\nDropout (Srivastava et al. 2014) happens during training only. At each forward pass, individual activations – single values in the tensors being passed on – are dropped (meaning: set to zero), with configurable probability. Due to randomness, actual positions of zeroed-out values in a tensor vary from pass to pass.\nPut differently: Dynamically and reversibly, individual inter-neuron connections are “cut off”. Why would this help to avoid overfitting?\nIf different connections between neurons could be dropped out at unforeseeable times, the network as a whole had better not get too dependent on cooperation between individual units. But it is just this kind of inter-individual cooperation that results in strong memorization of examples presented during training. If that is made impossible, the network as a whole has to focus on more general features, ones that that emerge from more distributed, more random cooperation. Put differently, we’re introducing randomness, or noise, and no hyper-specialized model will be able to deal with that.\nApplication of this technique in torch is very straightforward. A dedicated layer takes care of it: nn_dropout(). By “takes care” I mean:\n\nIn its forward() method, the layer checks whether we’re in training or test mode. If the latter, nothing happens.\nIf we’re training, it uses the dropout probability \\(p\\) it’s been initialized with to zero out parts of the input tensor.\nThe partly-zeroed tensor is scaled up by the inverse of \\(1-p\\), to keep the overall magnitude of the tensor unchanged. The result is then passed on to the next layer.\n\nHere is our convnet from above, with a few dropout layers interspersed:\n\nconvnet &lt;- nn_module(\n  \"convnet\",\n  \n  initialize = function() {\n    # nn_conv2d(in_channels, out_channels, kernel_size, stride)\n    self$conv1 &lt;- nn_conv2d(1, 32, 3, 1)\n    self$conv2 &lt;- nn_conv2d(32, 64, 3, 2)\n    self$conv3 &lt;- nn_conv2d(64, 128, 3, 1)\n    self$conv4 &lt;- nn_conv2d(128, 256, 3, 2)\n    self$conv5 &lt;- nn_conv2d(256, 10, 3, 2)\n    \n    self$drop1 &lt;- nn_dropout(p = 0.2)\n    self$drop2 &lt;- nn_dropout(p = 0.2)\n  },\n  forward = function(x) {\n    x %&gt;% \n      self$conv1() %&gt;% \n      nnf_relu() %&gt;%\n      self$conv2() %&gt;%\n      nnf_relu() %&gt;% \n      self$drop1() %&gt;%\n      self$conv3() %&gt;% \n      nnf_relu() %&gt;% \n      self$conv4() %&gt;% \n      nnf_relu() %&gt;% \n      self$drop2() %&gt;%\n      self$conv5() %&gt;%\n      torch_squeeze()\n  }\n)\n\nAs always, experimentation will help in determining a good dropout rate, as well as the number of dropout layers introduced. You may be wondering, though – how does this technique go together with the data-related ones we presented before?\nIn practice (in the “real world”), you would basically always use data augmentation. As to dropout, it probably is the go-to technique in this area. A priori, there is no reason to not use them together – all the more since both are configurable. One way to see it is like this: You have a fixed budget of randomness; every technique you use that adds randomness will take up some of that budget. How do you know if you’ve exceeded it? By seeing no (or insufficient) progress on the training set. There is a clear ranking of priorities here: It’s no use worrying about generalization to the test set as long as the model is not learning at all. The number one requirement always is to get the model to learn in the first place.\n\n\n16.3.2 Regularization\nBoth dropout and regularization affect how the model’s inner workings, but they are very different in spirit.\nDropout introduces randomness. Looking for analogies in machine learning overall, it has something in common with ensemble modeling. (By the way, the idea of ensembling is as applicable, in theory, to neural networks, as to other algorithms. It’s just not that popular because training a neural network takes a long time.)\nRegularization, on the other hand, is similar to – regularization. If you know what is meant by this term in machine learning in general, you know what is meant in deep learning. In deep learning, though, it is often referred to by a different name: “weight decay”. Personally, I find this a little misleading. “Decay” seems to hint at some sort of temporal development; in fact, this is exactly its meaning in “learning rate decay”, a training strategy that makes use of decreasing learning rates over time.\nIn weight decay, or regularization, however, there’s no dynamics involved at all. Instead, we follow a fixed rule. That rule is: When computing the loss, add to it a quantity proportional to the aggregate size of the weights. The idea is to keep the weights small and homogeneous, to prevent sharp cliffs and canyons in the loss function.\nIn deep learning, regularization as a strategy is nowhere as central as data augmentation or dropout, which is why I’m not going to go into great detail. If you want to learn more, check out one of the many great introductions to statistical learning around.\nAmong data scientists, regularization probably is most often associated with different variants of linear regression. Variants differ in what they understand by a penalty “proportional” to the weights. In ridge regression, this quantity will be a fraction of the sum of the squared weights; in the Lasso, their absolute values. It is only the former algorithm that is implemented in torch.\nAlthough semantically, regularization forms part of the “business logic” – which is why I’m listing it in the “model” section – it technically is implemented as part of an optimizer object. All of the classic optimizers – SGD, RMSProp, Adam, and relatives – take a weight_decay argument used to indicate what fraction of the sum of squared weights you’d like to have added to the loss.\nIn our example, you’d pass this argument to luz::set_opt_hparams(), like so:\n\nfitted &lt;- convnet %&gt;%\n  setup(\n    loss = nn_cross_entropy_loss(),\n    optimizer = optim_adam,\n    metrics = list(luz_metric_accuracy())\n    ) %&gt;%\n  set_opt_hparams(weight_decay = 0.00001) %&gt;%\n  fit(train_dl, epochs = 5, valid_data = valid_dl)\n\nAs already hinted at above, regularization is not seen that often in the context of neural networks. Nevertheless, given the wide range of problems deep learning is applied to, it’s good to be aware of its availability."
  },
  {
    "objectID": "overfitting.html#training-stage-early-stopping",
    "href": "overfitting.html#training-stage-early-stopping",
    "title": "16  Making models generalize",
    "section": "16.4 Training stage: Early stopping",
    "text": "16.4 Training stage: Early stopping\nWe conclude this chapter with an example of an anything-but-sophisticated, but very effective training technique: early stopping. So far, in our running example, we’ve always let the model learn for a pre-determined number of epochs. Thanks to the existence of callbacks – those “hooks” into the training process that let you modify “almost everything” dynamically – however, training duration does not have to be fixed from the outset.\nOne of several callbacks related to the learning rate – besides, e.g., luz_callback_lr_scheduler() , that allows you to dynamically adjust learning rate while training – luz_callback_early_stopping() will trigger an early exit once some configurable condition is satisfied.\nCalled without parameters, it will monitor loss on the validation set, and once validation loss stops decreasing, it will immediately cause training to stop. However, less strict policies are possible. For example, luz_callback_early_stopping(patience = 2) will allow for two consecutive epochs without improvement before triggering an exit.\nTo make use of luz_callback_early_stopping(), you add it to the callbacks list in fit():\n\nfitted &lt;- convnet %&gt;%\n  setup(\n    loss = nn_cross_entropy_loss(),\n    optimizer = optim_adam,\n    metrics = list(luz_metric_accuracy())\n  ) %&gt;%\n  fit(train_dl,\n    epochs = 5,\n    valid_data = valid_dl,\n    callbacks = list(\n      luz_callback_early_stopping()\n    )\n  )\n\nIn deep learning, early stopping is ubiquitous; it’s hard to imagine why one would not want to use it.\nHaving discussed overfitting, we now go on to a complementary aspect of model training: True, we want our models to generalize; but we also want them to learn fast. That’s what the next chapter is dedicated to.\n\n\n\n\nSrivastava, Nitish, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. 2014. “Dropout: A Simple Way to Prevent Neural Networks from Overfitting.” J. Mach. Learn. Res. 15 (1): 1929–58.\n\n\nZhang, Hongyi, Moustapha Cisse, Yann N. Dauphin, and David Lopez-Paz. 2017. “mixup: Beyond Empirical Risk Minimization.” arXiv e-Prints, October, arXiv:1710.09412. https://arxiv.org/abs/1710.09412."
  },
  {
    "objectID": "overfitting.html#footnotes",
    "href": "overfitting.html#footnotes",
    "title": "16  Making models generalize",
    "section": "",
    "text": "Meaning: “overly fitting” the model to the training data.↩︎\nI chose MNIST for this overview because it allows to easily discern the effects of various augmentation techniques (especially the next one we’re going to discuss).↩︎"
  },
  {
    "objectID": "training_efficiency.html#batch-normalization",
    "href": "training_efficiency.html#batch-normalization",
    "title": "17  Speeding up training",
    "section": "17.1 Batch normalization",
    "text": "17.1 Batch normalization\nThe idea behind batch normalization (Ioffe and Szegedy (2015)) directly follows from the basic mechanism of backpropagation.\nIn backpropagation, each layer’s weights are adapted, from the very last to the very first. Now, let’s focus on layer 17. When the time comes for the next forward pass, it will have updated its weights in a way that made sense, given the previous batch. However – the layer right before it will also have updated its weights. As will the one preceding its predecessor, the one before that … you get the picture. And so, due to all prior layers now handling their inputs differently, layer 17 will not quite get what it expects. In consequence, the strategy that seemed optimal before might not be.\nWhile the problem per se is algorithm-inherent, it is more likely to surface the deeper the model. Due to the resulting instability, you need to train with lower learning rates. And this, in turn, means that training will take more time.\nThe solution Ioffe and Szegedy proposed was the following. At each pass, and for every layer, normalize the activations. If that were all, however, some sort of levelling would occur. That’s because each layer now has to adjust its activations so they have a mean of zero and a standard deviation of one. In fact, such a requirement would not just act as an equalizer between layers, but also, within: meaning, it would make it harder, for each individual layer, to create sharp internal distinctions.\nFor that reason, mean and standard deviation are not simply computed, but learned. In other words, they become model parameters.\nSo far, we’ve been talking about this conceptually, suggesting an implementation where each layer took care of this itself. This is not how it’s implemented, however. Rather, we have dedicated layers, batchnorm layers, that normalize and re-scale their inputs. It is them who have mean and standard deviation as learnable parameters.\nTo use batch normalization in our MNIST example, we intersperse batchnorm layers throughout the network, one after each convolution block. There are three types of them, one for each of one-, two-, and three-dimensional inputs (time series, images, and video, say). All of them compute statistics individually per channel, and the number of input channels is the only required argument to their constructors.\n\nlibrary(torch)\nlibrary(torchvision)\nlibrary(luz)\n\nconvnet &lt;- nn_module(\n  \"convnet\",\n  initialize = function() {\n    # nn_conv2d(in_channels, out_channels, kernel_size, stride)\n    self$conv1 &lt;- nn_conv2d(1, 32, 3, 1)\n    self$conv2 &lt;- nn_conv2d(32, 64, 3, 2)\n    self$conv3 &lt;- nn_conv2d(64, 128, 3, 1)\n    self$conv4 &lt;- nn_conv2d(128, 256, 3, 2)\n    self$conv5 &lt;- nn_conv2d(256, 10, 3, 2)\n\n    self$bn1 &lt;- nn_batch_norm2d(32)\n    self$bn2 &lt;- nn_batch_norm2d(64)\n    self$bn3 &lt;- nn_batch_norm2d(128)\n    self$bn4 &lt;- nn_batch_norm2d(256)\n  },\n  forward = function(x) {\n    x %&gt;%\n      self$conv1() %&gt;%\n      nn_relu() %&gt;%\n      self$bn1() %&gt;%\n      self$conv2() %&gt;%\n      nn_relu() %&gt;%\n      self$bn2() %&gt;%\n      self$conv3() %&gt;%\n      nn_relu() %&gt;%\n      self$bn3() %&gt;%\n      self$conv4() %&gt;%\n      nn_relu() %&gt;%\n      self$bn4() %&gt;%\n      self$conv5() %&gt;%\n      torch_squeeze()\n  }\n)\n\nOne thing you may be wondering though: What happens during testing? The whole notion of testing would be carried to absurdity, were we to apply the same logic there as well. Instead, during evaluation we use the mean and standard deviation determined on the training set. So, batch normalization shares with dropout the fact that they behave differently across phases.\nBatch normalization can be stunningly successful, especially in image processing. It’s a technique you should always consider. What’s more, it has often been found to help with generalization, as well."
  },
  {
    "objectID": "training_efficiency.html#dynamic-learning-rates",
    "href": "training_efficiency.html#dynamic-learning-rates",
    "title": "17  Speeding up training",
    "section": "17.2 Dynamic learning rates",
    "text": "17.2 Dynamic learning rates\nYou won’t be surprised to hear that the learning rate is central to training performance. In backpropagation, layer weights are modified in a direction given by the current loss; the learning rate affects the size of the update.\nWith very small updates, the network might move in the right direction, to eventually arrive at a satisfying local minimum of the loss function. But the journey will be long. The bigger the updates, on the other hand, the likelier it gets that it’ll “jump over” that minimum. Imagine moving down one leg of a parabola. Maybe the update is so big that we don’t just end up on the other leg (with equivalent loss), but at a “higher” place (loss) even. Then the next update will send us back to the other leg, to a yet higher location. It won’t take long until loss becomes infinite – the dreaded NaN, in R.\nThe goal is easily stated: We’d like to train with the highest-viable learning rate, while avoiding to ever “overshoot”. There are two aspects to this.\nFirst, we should know what would constitute too high a rate. To that purpose, we use something called a learning rate finder. This technique owes a lot of its popularity to the fast.ai library, and the deep learning classes taught by its creators. The learning rate finder gets called once, before training proper.\nSecond, we want to organize training in a way that at each time, the optimal learning rate is used. Views differ on what is an optimal, stage-dependent rate. torch offers a set of so-called learning rate schedulers implementing various widely-established techniques. Schedulers differ not just in strategy, but also, in how often the learning rate is adapted.\n\n17.2.1 Learning rate finder\nThe idea of the learning rate finder is the following. You train the network for a single epoch, starting with a very low rate. Looping through the batches, you keep increasing it, until you arrive at a very high value. During the loop, you keep track of rates as well as corresponding losses. Experiment finished, you plot rates and losses against each other. You then pick a rate lower, but not very much lower, than the one at which loss was minimal. The recommendation usually is to choose a value one order of magnitude smaller than the one at minimum. For example, if the minimum occurred at 0.01, you would go with 0.001.\nNicely, we don’t need to code this ourselves: luz::lr_finder() will run the experiment for us. All we need to do is inspect the resulting graph – and make the decision!\nTo demonstrate, let’s first copy some prerequisites from the last chapter. We use MNIST, with data augmentation. Model-wise, we build on the default version of the CNN, and add in batch normalization. lr_finder() then expects the model to have been setup() with a loss function and an optimizer:\n\nlibrary(torch)\nlibrary(torchvision)\nlibrary(luz)\n\ndir &lt;- \"~/.torch-datasets\" \n\ntrain_ds &lt;- mnist_dataset(\n  dir,\n  download = TRUE,\n  transform = . %&gt;%\n    transform_to_tensor() %&gt;%\n    transform_random_affine(\n      degrees = c(-45, 45), translate = c(0.1, 0.1)\n    )\n)\n\ntrain_dl &lt;- dataloader(train_ds, batch_size = 128, shuffle = TRUE)\n\nvalid_ds &lt;- mnist_dataset(\n  dir,\n  train = FALSE,\n  transform = transform_to_tensor\n)\n\nvalid_dl &lt;- dataloader(valid_ds, batch_size = 128)\n\nconvnet &lt;- nn_module(\n  \"convnet\",\n  initialize = function() {\n    # nn_conv2d(in_channels, out_channels, kernel_size, stride)\n    self$conv1 &lt;- nn_conv2d(1, 32, 3, 1)\n    self$conv2 &lt;- nn_conv2d(32, 64, 3, 2)\n    self$conv3 &lt;- nn_conv2d(64, 128, 3, 1)\n    self$conv4 &lt;- nn_conv2d(128, 256, 3, 2)\n    self$conv5 &lt;- nn_conv2d(256, 10, 3, 2)\n\n    self$bn1 &lt;- nn_batch_norm2d(32)\n    self$bn2 &lt;- nn_batch_norm2d(64)\n    self$bn3 &lt;- nn_batch_norm2d(128)\n    self$bn4 &lt;- nn_batch_norm2d(256)\n  },\n  forward = function(x) {\n    x %&gt;%\n      self$conv1() %&gt;%\n      nnf_relu() %&gt;%\n      self$bn1() %&gt;%\n      self$conv2() %&gt;%\n      nnf_relu() %&gt;%\n      self$bn2() %&gt;%\n      self$conv3() %&gt;%\n      nnf_relu() %&gt;%\n      self$bn3() %&gt;%\n      self$conv4() %&gt;%\n      nnf_relu() %&gt;%\n      self$bn4() %&gt;%\n      self$conv5() %&gt;%\n      torch_squeeze()\n  }\n)\n\nmodel &lt;- convnet %&gt;%\n  setup(\n    loss = nn_cross_entropy_loss(),\n    optimizer = optim_adam,\n    metrics = list(luz_metric_accuracy())\n    )\n\nWhen called with default parameters, lr_finder() will start with a learning rate of 1e-7, and increase that, over one hundred steps, until it arrives at 0.1. All of these values – minimum learning rate, number of steps, and maximum learning rate – can be modified. For MNIST, I knew that higher learning rates should be feasible; so I shifted that range a bit to the right:\n\nrates_and_losses &lt;- model %&gt;%\n  lr_finder(train_dl, start_lr = 0.0001, end_lr = 0.3)\n\nPlotting the recorded losses against their rates, we get both the exact values (one for each of the steps), and an exponentially-smoothed version (fig. 17.1).\n\nrates_and_losses %&gt;% plot()\n\n\n\n\nFigure 17.1: Output of luz’s learning rate finder, run on MNIST.\n\n\nHere, we see that when rates exceed a value of about 0.01, losses become noisy, and increase. The definitive explosion, though, seems to be triggered only when the rate surpasses 0.1. In consequence, you might decide to not exactly follow the “one order of magnitude” recommendation, and try a learning rate of 0.01 – at least in case you do what I’ll be doing in the next section, namely, use the so-determined rate not as a fixed-in-time value, but as a maximal one.\n\n\n17.2.2 Learning rate schedulers\nOnce we have an idea where to upper-bound the learning rate, we can make use of one of torch’s learning rate schedulers to orchestrate rates over training. We will decide on a scheduler object, and pass that to a dedicated luz callback: callback_lr_scheduler().\nClassically, a popular, intuitively appealing scheme used to be the following. In early stages of training, try a reasonably high learning rate, in order to make quick progress; once that has happened, though, slow down, making sure you don’t zig-zag around (and away from) a presumably-found local minimum.\nIn the meantime, more sophisticated schemes have been developed.\nOne family of ideas keeps periodically turning up and down the learning rate. Members of this family are known as, for example, “cyclical learning rates” (Smith (2015)), or (some form of) “annealing with restarts” (e.g., Loshchilov and Hutter (2016)). What differs between members of the family is the shape of the resulting learning rate curve, and the frequency of restarts (meaning, how often you turn up the rate again, to begin a new period of descent). In torch, popular representatives of this family are, for example, lr_cyclic() and lr_cosine_annealing_warm_restarts().\nA very different approach is represented by the one-cycle learning rate strategy (Smith and Topin (2017)). In this scheme, we start from some initial – low-ish – learning rate, increase that up to some user-specified maximum, and from there, decrease again, until we’ve arrived at a rate significantly lower than the one we started with. In torch, this is available as lr_one_cycle(), and this is the strategy I was referring to above.\nlr_one_cycle() allows for user-side tweaking in a number of ways, and in real-life projects, you may want to play around a bit with its many parameters. Here, we use the defaults. All we need to do, then, is pass in the maximum rate we determined, and decide on how often we want the learning rate to be updated. The logical way seems to be to do it once per batch, something that will happen if we pass in number of epochs and number of steps per epoch.\nIn the code snippet below, note that the arguments max_lr , epochs, and steps_per_epoch really “belong to” lr_one_cycle(). We have to pass them to the callback, though, because it is the callback that will instantiate the scheduler.\ncall_on, however, genuinely forms part of the callback logic. This is a harmless-looking argument that, nevertheless, we need to pay attention to. Schedulers differ in whether their period is defined in epochs, or in batches. lr_one_cycle() “wakes up” once per batch; but there are others – lr_step(), for example - that check whether an update is due once per epoch only. The default value of call_on is on_epoch_end; so for lr_one_cycle(), we have to override the default.\n\nnum_epochs &lt;- 5 \n\n# the model has already been setup(), we continue from there\nfitted &lt;- model %&gt;%\n  fit(train_dl,\n    epochs = num_epochs,\n    valid_data = valid_dl,\n    callbacks = list(\n      luz_callback_lr_scheduler(\n        lr_one_cycle,\n        max_lr = 0.01,\n        epochs = num_epochs,\n        steps_per_epoch = length(train_dl),\n        call_on = \"on_batch_end\"\n      )\n    )\n  )\n\nAt this point, we wrap up the topic of learning rate optimization. As with so many things in deep learning, research progresses at a rapid rate, and most likely, new scheduling strategies will continue to be added. Now though, for a total change in scope."
  },
  {
    "objectID": "training_efficiency.html#transfer-learning",
    "href": "training_efficiency.html#transfer-learning",
    "title": "17  Speeding up training",
    "section": "17.3 Transfer learning",
    "text": "17.3 Transfer learning\nTransfer, as a general concept, is what happens when we have learned to do one thing, and benefit from those skills in learning something else. For example, we may have learned how to make some move with our left leg; it will then be easier to learn how to do the same with our right leg. Or, we may have studied Latin and then, found that it helped us a lot in learning French. These, of course, are straightforward examples; analogies between domains and skills can be a lot more subtle.\nIn comparison, the typical usage of “transfer learning” in deep learning seems rather narrow, at first glance. Concretely, it refers to making use of huge, highly effective models (often provided as part of some library), that have already been trained, for a long time, on a huge dataset. Typically, you would load the model, remove its output layer, and add on a small-ish sequential module that takes the model’s now-last layer to the kind of output you require. Often, in example tasks, this will go from the broad to the narrow – as in the below example, where we use a model trained on one thousand categories of images to distinguish between ten types of digits.\nBut it doesn’t have to be like that. In deep learning, too, models trained on one task can be built upon in tasks that have different, but not necessarily more domain-constrained, requirements. As of today, popular examples for this are found mostly in natural language processing (NLP), a topic we don’t cover in this book. There, you find models trained to predict how a sentence continues – resulting in general “knowledge” about a language – used in logically dependent tasks like translation, question answering, or text summarization. Transfer learning, in that general sense, is something we’ll certainly see more and more of in the near future.\nThere is another, very important aspect to the popularity of transfer learning, though. When you build on a pre-trained model, you’ll incorporate all of what it has learned - including its biases and preconceptions. How much that matters, in your context, will depend on what exactly you’re doing. For us, who are classifying digits, it will not matter whether the pre-trained model performs a lot better on cats and dogs than on e-scooters, smart fridges, or garbage cans. But think about this whenever models concern people. Typically, these high-performing models have been trained either on benchmark datasets, or data massively scraped from the web. The former have, historically, been very little concerned with questions of stereotypes and representation. (Hopefully, that will change in the future.) The latter are, by definition, subject to availability bias, as well as idiosyncratic decisions made by the dataset creators. (Hopefully, these circumstances and decisions will have been carefully documented. That is something you’ll need to check out.)\nWith our running example, we’ll be in the former category: We’ll be downstream users of a benchmark dataset. The benchmark dataset in question is ImageNet, the well-known collection of images we already encountered in our first experience with Tiny Imagenet, two chapters ago.\nIn torchvision, we find a number of ready-to-use models that have been trained on ImageNet. Among them is ResNet-18 (He et al. (2015)). The “Res” in ResNet stands for “residual”, or “residual connection”. Here residual is used, as is common in statistics, to designate an error term. The idea is to have some layers predict, not something entirely new, but the difference between a target and the previous layer’s prediction – the error, so to say. If this sounds confusing, don’t worry. For us, what matters is that due to their architecture, ResNets can afford to be very deep, without becoming excessively hard to train. And that in turn means they’re very performant, and often used as pre-trained feature detectors.\nThe first time you use a pre-trained model, its weights are downloaded, and cached in an operating-system-specific location.\n\nresnet &lt;- model_resnet18(pretrained = TRUE)\nresnet\n\nAn `nn_module` containing 11,689,512 parameters.\n\n── Modules ───────────────────────────────────────\n• conv1: &lt;nn_conv2d&gt; #9,408 parameters\n• bn1: &lt;nn_batch_norm2d&gt; #128 parameters\n• relu: &lt;nn_relu&gt; #0 parameters\n• maxpool: &lt;nn_max_pool2d&gt; #0 parameters\n• layer1: &lt;nn_sequential&gt; #147,968 parameters\n• layer2: &lt;nn_sequential&gt; #525,568 parameters\n• layer3: &lt;nn_sequential&gt; #2,099,712 parameters\n• layer4: &lt;nn_sequential&gt; #8,393,728 parameters\n• avgpool: &lt;nn_adaptive_avg_pool2d&gt; #0 parameters\n• fc: &lt;nn_linear&gt; #513,000 parameters\nHave a look at the last module, a linear layer:\n\nresnet$fc\n\nFrom the weights, we can see that this layer maps tensors with 512 features to ones with 1000 - the thousand different image categories used in the ImageNet challenge. To adapt this model to our purposes, we simply replace the very last layer with one that outputs feature vectors of length ten:\n\nresnet$fc &lt;- nn_linear(resnet$fc$in_features, 10)\nresnet$fc\n\nAn `nn_module` containing 5,130 parameters.\n\n── Parameters ───────────────────────────────────────────────────────────────────────────────────────────────\n• weight: Float [1:10, 1:512]\n• bias: Float [1:10]\nWhat will happen if we now train the modified model on MNIST? Training will progress with the speed of a Zenonian tortoise, since gradients need to be propagated across a huge network. Not only is that a waste of time; it is useless, as well. It could, if we were very patient, even be harmful: We could destroy the intricate feature hierarchy learned by the pre-trained model. Of course, in classifying digits we will make use of just a tiny subset of learned higher-order features, but that is not a problem. In any case, with the resources available to mere mortals, we are unlikely to improve on ResNet’s digit-discerning capabilities.\nWhat we do, thus, is set all layer weights to non-trainable, apart from just that last layer we replaced.\nPutting it all together, we arrive at the following, concise definition of a model:\n\nconvnet &lt;- nn_module(\n  initialize = function() {\n    self$model &lt;- model_resnet18(pretrained = TRUE)\n    for (par in self$parameters) {\n      par$requires_grad_(FALSE)\n    }\n    self$model$fc &lt;- nn_linear(self$model$fc$in_features, 10)\n  },\n  forward = function(x) {\n    self$model(x)\n  }\n)\n\nThis we can now train with luz, like before. There’s just one further step required, and that’s just because I’m using MNIST to illustrate. Since ResNet has been trained on RGB images, its first layer expects three channels, not one. We can work around this by multiplexing the single grayscale channel into three identical ones, using torch_expand(). For important real-life tasks, this may not be an optimal solution; but it will do well enough for MNIST.\nA convenient place to perform the expansion is as part of the data pre-processing pipeline, repeated here in modified form.\n\ntrain_ds &lt;- mnist_dataset(\n  dir,\n  download = TRUE,\n  transform = . %&gt;%\n    transform_to_tensor() %&gt;%\n    (function(x) x$expand(c(3, 28, 28))) %&gt;%\n    transform_random_affine(\n      degrees = c(-45, 45), translate = c(0.1, 0.1)\n    )\n)\n\ntrain_dl &lt;- dataloader(\n  train_ds,\n  batch_size = 128,\n  shuffle = TRUE\n)\n\nvalid_ds &lt;- mnist_dataset(\n  dir,\n  train = FALSE,\n  transform = . %&gt;%\n    transform_to_tensor %&gt;%\n    (function(x) x$expand(c(3, 28, 28)))\n)\n\nvalid_dl &lt;- dataloader(valid_ds, batch_size = 128)\n\nThe code for training then looks as usual.\n\nmodel &lt;- convnet %&gt;%\n  setup(\n    loss = nn_cross_entropy_loss(),\n    optimizer = optim_adam,\n    metrics = list(luz_metric_accuracy())\n    ) %&gt;%\n  fit(train_dl,\n      epochs = 5,\n      valid_data = valid_dl)\n\nBefore we wrap up both section and chapter, one additional comment. The way we proceeded, above – replacing the very last layer with a single module outputting the final scores – is just the easiest, most straightforward thing to do.\nFor MNIST, this is good enough. Maybe, on inspection, we’d find that single digits already form part of ResNet’s feature hierarchy; but even if not, a linear layer with ~ 5000 parameters should suffice to learn them. However, the more there is “still to be learned” – equivalently, the more either dataset or task differ from what was used (done, resp.) in model pre-training – the more powerful a sub-module we will want to chain on. We’ll see an example of this in the next chapter.\n\n\n\n\nHe, Kaiming, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2015. “Deep Residual Learning for Image Recognition.” CoRR abs/1512.03385. http://arxiv.org/abs/1512.03385.\n\n\nIoffe, Sergey, and Christian Szegedy. 2015. “Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift.” https://arxiv.org/abs/1502.03167.\n\n\nLoshchilov, Ilya, and Frank Hutter. 2016. “SGDR: Stochastic Gradient Descent with Restarts.” CoRR abs/1608.03983. http://arxiv.org/abs/1608.03983.\n\n\nSmith, Leslie N. 2015. “No More Pesky Learning Rate Guessing Games.” CoRR abs/1506.01186. http://arxiv.org/abs/1506.01186.\n\n\nSmith, Leslie N., and Nicholay Topin. 2017. “Super-Convergence: Very Fast Training of Residual Networks Using Large Learning Rates.” CoRR abs/1708.07120. http://arxiv.org/abs/1708.07120."
  },
  {
    "objectID": "image_classification_2.html#data-input-common-for-all",
    "href": "image_classification_2.html#data-input-common-for-all",
    "title": "18  Image classification, take two: Improving performance",
    "section": "18.1 Data input (common for all)",
    "text": "18.1 Data input (common for all)\nAll three runs use the same data input pipeline. Compared with our first go at telling apart the two hundred classes in Tiny Imagenet, two things are new.\nFirst, we now apply data augmentation to the training set: rotations and translations, to be precise.\nSecond, input tensors are normalized, channel-wise, to a set of given means and standard deviations. This really is required for the third run (using ResNet) only; we just do to our images what was done in training ResNet. (The same goes for most of the pre-trained models trained on ImageNet.) There really is no problem, though, in doing the same for runs one and two; so normalization is part of the common pre-processing pipeline.\n\nlibrary(torch)\nlibrary(torchvision)\nlibrary(torchdatasets)\nlibrary(luz)\n\nset.seed(777)\ntorch_manual_seed(777)\n\ndir &lt;- \"~/.torch-datasets\"\n\ntrain_ds &lt;- tiny_imagenet_dataset(\n  dir,\n  download = TRUE,\n  transform = . %&gt;%\n    transform_to_tensor() %&gt;%\n    transform_random_affine(\n      degrees = c(-30, 30), translate = c(0.2, 0.2)\n    ) %&gt;%\n    transform_normalize(\n      mean = c(0.485, 0.456, 0.406),\n      std = c(0.229, 0.224, 0.225)\n    )\n)\n\nvalid_ds &lt;- tiny_imagenet_dataset(\n  dir,\n  split = \"val\",\n  transform = function(x) {\n    x %&gt;%\n      transform_to_tensor() %&gt;%\n      transform_normalize(\n        mean = c(0.485, 0.456, 0.406),\n        std = c(0.229, 0.224, 0.225))\n  }\n)\n\ntrain_dl &lt;- dataloader(\n  train_ds,\n  batch_size = 128,\n  shuffle = TRUE\n)\nvalid_dl &lt;- dataloader(valid_ds, batch_size = 128)\n\nNext, we compare three different configurations."
  },
  {
    "objectID": "image_classification_2.html#run-1-dropout",
    "href": "image_classification_2.html#run-1-dropout",
    "title": "18  Image classification, take two: Improving performance",
    "section": "18.2 Run 1: Dropout",
    "text": "18.2 Run 1: Dropout\nIn run one, we take the convnet we were using, and add dropout layers.\n\nconvnet &lt;- nn_module(\n  \"convnet\",\n  initialize = function() {\n    self$features &lt;- nn_sequential(\n      nn_conv2d(3, 64, kernel_size = 3, padding = 1),\n      nn_relu(),\n      nn_max_pool2d(kernel_size = 2),\n      nn_dropout2d(p = 0.05),\n      nn_conv2d(64, 128, kernel_size = 3, padding = 1),\n      nn_relu(),\n      nn_max_pool2d(kernel_size = 2),\n      nn_dropout2d(p = 0.05),\n      nn_conv2d(128, 256, kernel_size = 3, padding = 1),\n      nn_relu(),\n      nn_max_pool2d(kernel_size = 2),\n      nn_dropout2d(p = 0.05),\n      nn_conv2d(256, 512, kernel_size = 3, padding = 1),\n      nn_relu(),\n      nn_max_pool2d(kernel_size = 2),\n      nn_dropout2d(p = 0.05),\n      nn_conv2d(512, 1024, kernel_size = 3, padding = 1), \n      nn_relu(),\n      nn_adaptive_avg_pool2d(c(1, 1)),\n      nn_dropout2d(p = 0.05),\n    )\n    self$classifier &lt;- nn_sequential(\n      nn_linear(1024, 1024),\n      nn_relu(),\n      nn_dropout(p = 0.05),\n      nn_linear(1024, 1024),\n      nn_relu(),\n      nn_dropout(p = 0.05),\n      nn_linear(1024, 200)\n    )\n  },\n  forward = function(x) {\n    x &lt;- self$features(x)$squeeze()\n    x &lt;- self$classifier(x)\n    x\n  }\n)\n\nNext, we run the learning rate finder (fig. 18.1).\n\nmodel &lt;- convnet %&gt;%\n  setup(\n    loss = nn_cross_entropy_loss(),\n    optimizer = optim_adam,\n    metrics = list(\n      luz_metric_accuracy()\n    )\n  ) \n    \nrates_and_losses &lt;- model %&gt;% lr_finder(train_dl)\nrates_and_losses %&gt;% plot()\n\n\n\n\nFigure 18.1: Learning rate finder, run on Tiny Imagenet. Convnet with dropout layers.\n\n\nWe already know that discerning between two hundred classes is a task that takes time; it’s thus not surprising to see a flat-ish loss curve during most of learning rate increase. We can conclude, though, that we had better not exceed a learning rate of 0.01.\nAs in all further configurations, we now train with the one-cycle learning rate scheduler, and early stopping enabled.\n\nfitted &lt;- model %&gt;%\n  fit(train_dl, epochs = 50, valid_data = valid_dl,\n      callbacks = list(\n        luz_callback_early_stopping(patience = 2),\n        luz_callback_lr_scheduler(\n          lr_one_cycle,\n          max_lr = 0.01,\n          epochs = 50,\n          steps_per_epoch = length(train_dl),\n          call_on = \"on_batch_end\"),\n        luz_callback_model_checkpoint(path = \"cpt_dropout/\"),\n        luz_callback_csv_logger(\"logs_dropout.csv\")\n        ),\n      verbose = TRUE)\n\nFor me, training stopped after thirty-five epochs, at a validation accuracy of 0.4, and a training accuracy that was just slightly higher: 0.44.\nEpoch 1/50\nTrain metrics: Loss: 5.116 - Acc: 0.0128                                      \nValid metrics: Loss: 4.9144 - Acc: 0.0217\nEpoch 2/50\nTrain metrics: Loss: 4.7217 - Acc: 0.042                                      \nValid metrics: Loss: 4.4143 - Acc: 0.067\nEpoch 3/50\nTrain metrics: Loss: 4.3681 - Acc: 0.0791                                     \nValid metrics: Loss: 4.1145 - Acc: 0.105\n...\n...\nEpoch 33/50\nTrain metrics: Loss: 2.3006 - Acc: 0.4304                                     \nValid metrics: Loss: 2.5863 - Acc: 0.4025\nEpoch 34/50\nTrain metrics: Loss: 2.2717 - Acc: 0.4365                                     \nValid metrics: Loss: 2.6377 - Acc: 0.3889\nEpoch 35/50\nTrain metrics: Loss: 2.2456 - Acc: 0.4402                                     \nValid metrics: Loss: 2.6208 - Acc: 0.4043\nEarly stopping at epoch 35 of 50\nComparing with the initial approach, where after fifty epochs, we were left with accuracies of 0.22 for validation, and 0.92 for training, we see an impressive reduction in overfitting. Of course, we cannot really say anything about the respective merits of dropout and data augmentation here. If you’re curious, please go ahead and find out!"
  },
  {
    "objectID": "image_classification_2.html#run-2-batch-normalization",
    "href": "image_classification_2.html#run-2-batch-normalization",
    "title": "18  Image classification, take two: Improving performance",
    "section": "18.3 Run 2: Batch normalization",
    "text": "18.3 Run 2: Batch normalization\nIn configuration number two, dropout is replaced by batch normalization.\n\nconvnet &lt;- nn_module(\n  \"convnet\",\n  initialize = function() {\n    self$features &lt;- nn_sequential(\n      nn_conv2d(3, 64, kernel_size = 3, padding = 1),\n      nn_batch_norm2d(64),\n      nn_relu(),\n      nn_max_pool2d(kernel_size = 2),\n      nn_conv2d(64, 128, kernel_size = 3, padding = 1),\n      nn_batch_norm2d(128),\n      nn_relu(),\n      nn_max_pool2d(kernel_size = 2),\n      nn_conv2d(128, 256, kernel_size = 3, padding = 1),\n      nn_batch_norm2d(256),\n      nn_relu(),\n      nn_max_pool2d(kernel_size = 2),\n      nn_conv2d(256, 512, kernel_size = 3, padding = 1),\n      nn_batch_norm2d(512),\n      nn_relu(),\n      nn_max_pool2d(kernel_size = 2),\n      nn_conv2d(512, 1024, kernel_size = 3, padding = 1), \n      nn_batch_norm2d(1024),\n      nn_relu(),\n      nn_adaptive_avg_pool2d(c(1, 1)),\n    )\n    self$classifier &lt;- nn_sequential(\n      nn_linear(1024, 1024),\n      nn_relu(),\n      nn_batch_norm1d(1024),\n      nn_linear(1024, 1024),\n      nn_relu(),\n      nn_batch_norm1d(1024),\n      nn_linear(1024, 200)\n    )\n  },\n  forward = function(x) {\n    x &lt;- self$features(x)$squeeze()\n    x &lt;- self$classifier(x)\n    x\n  }\n)\n\nAgain, we run the learning rate finder (fig. 18.2):\n\nmodel &lt;- convnet %&gt;%\n  setup(\n    loss = nn_cross_entropy_loss(),\n    optimizer = optim_adam,\n    metrics = list(\n      luz_metric_accuracy()\n    )\n  ) \n\nrates_and_losses &lt;- model %&gt;% lr_finder(train_dl)\nrates_and_losses %&gt;% plot()\n\n\n\n\nFigure 18.2: Learning rate finder, run on Tiny Imagenet. Convnet with batchnorm layers.\n\n\nThis looks surprisingly different! Of course, this is in part due to the scale on the loss axis; the loss does not explode as much, and thus, we get better resolution in the early and middle stages. The loss not exploding is an interesting finding in itself; the conclusion for us to draw from this plot is to be a bit more careful with the learning rate. This time, we’ll choose 0.001 for the maximum.\n\nfitted &lt;- model %&gt;%\n  fit(train_dl, epochs = 50, valid_data = valid_dl,\n      callbacks = list(\n        luz_callback_early_stopping(patience = 2),\n        luz_callback_lr_scheduler(\n          lr_one_cycle,\n          max_lr = 0.001,\n          epochs = 50,\n          steps_per_epoch = length(train_dl),\n          call_on = \"on_batch_end\"),\n        luz_callback_model_checkpoint(path = \"cpt_batchnorm/\"),\n        luz_callback_csv_logger(\"logs_batchnorm.csv\")\n        ),\n      verbose = TRUE)\n\nCompared with scenario one, I saw slightly more overfitting with batchnorm.\nEpoch 1/50\nTrain metrics: Loss: 4.5434 - Acc: 0.0862                                     \nValid metrics: Loss: 4.0914 - Acc: 0.1332\nEpoch 2/50\nTrain metrics: Loss: 3.9534 - Acc: 0.161                                      \nValid metrics: Loss: 3.7865 - Acc: 0.1809\nEpoch 3/50\nTrain metrics: Loss: 3.6425 - Acc: 0.2054                                     \nValid metrics: Loss: 3.5965 - Acc: 0.2115\n...\n...\nEpoch 19/50\nTrain metrics: Loss: 2.1063 - Acc: 0.4859                                     \nValid metrics: Loss: 2.621 - Acc: 0.3912\nEpoch 20/50\nTrain metrics: Loss: 2.0514 - Acc: 0.4987                                     \nValid metrics: Loss: 2.6334 - Acc: 0.3914\nEpoch 21/50\nTrain metrics: Loss: 1.9982 - Acc: 0.5069                                     \nValid metrics: Loss: 2.6603 - Acc: 0.3932\nEarly stopping at epoch 21 of 50"
  },
  {
    "objectID": "image_classification_2.html#run-3-transfer-learning",
    "href": "image_classification_2.html#run-3-transfer-learning",
    "title": "18  Image classification, take two: Improving performance",
    "section": "18.4 Run 3: Transfer learning",
    "text": "18.4 Run 3: Transfer learning\nFinally, the setup including transfer learning. A pre-trained ResNet is used for feature extraction, and a small sequential model takes care of classification. During training, all of ResNets weights are left untouched.\n\nconvnet &lt;- nn_module(\n  initialize = function() {\n    self$model &lt;- model_resnet18(pretrained = TRUE)\n    for (par in self$parameters) {\n      par$requires_grad_(FALSE)\n    }\n    self$model$fc &lt;- nn_sequential(\n      nn_linear(self$model$fc$in_features, 1024),\n      nn_relu(),\n      nn_linear(1024, 1024),\n      nn_relu(),\n      nn_linear(1024, 200)\n    )\n  },\n  forward = function(x) {\n    self$model(x)\n  }\n)\n\nAs always, we run the learning rate finder (fig. 18.3).\n\nmodel &lt;- convnet %&gt;%\n  setup(\n    loss = nn_cross_entropy_loss(),\n    optimizer = optim_adam,\n    metrics = list(\n      luz_metric_accuracy()\n    )\n  ) \n\nrates_and_losses &lt;- model %&gt;% lr_finder(train_dl)\nrates_and_losses %&gt;% plot()\n\n\n\n\nFigure 18.3: Learning rate finder, run on Tiny Imagenet. Convnet with transfer learning (ResNet).\n\n\nA maximal rate of 0.01 looks like it could be on the edge, but I decided to give it a try.\n\nfitted &lt;- model %&gt;%\n  fit(train_dl, epochs = 50, valid_data = valid_dl,\n      callbacks = list(\n        luz_callback_early_stopping(patience = 2),\n        luz_callback_lr_scheduler(\n          lr_one_cycle,\n          max_lr = 0.01,\n          epochs = 50,\n          steps_per_epoch = length(train_dl),\n          call_on = \"on_batch_end\"),\n        luz_callback_model_checkpoint(path = \"cpt_resnet/\"),\n        luz_callback_csv_logger(\"logs_resnet.csv\")\n        ),\n      verbose = TRUE)\n\nFor me, this configuration resulted in early stopping after nine epochs already, and yielded the best results by far: Final accuracy on the validation set was 0.48. Interestingly, in this setup, accuracy ended up worse for training than for validation.\nEpoch 1/50\nTrain metrics: Loss: 3.4036 - Acc: 0.2322                                     \nValid metrics: Loss: 2.5491 - Acc: 0.3884\nEpoch 2/50\nTrain metrics: Loss: 2.7911 - Acc: 0.3436                                     \nValid metrics: Loss: 2.417 - Acc: 0.4233\nEpoch 3/50\nTrain metrics: Loss: 2.6423 - Acc: 0.3726                                     \nValid metrics: Loss: 2.3492 - Acc: 0.4431\n...\n...\nValid metrics: Loss: 2.1822 - Acc: 0.4868\nEpoch 7/50\nTrain metrics: Loss: 2.4031 - Acc: 0.4198                                     \nValid metrics: Loss: 2.1413 - Acc: 0.4889\nEpoch 8/50\nTrain metrics: Loss: 2.3759 - Acc: 0.4252                                     \nValid metrics: Loss: 2.149 - Acc: 0.4958\nEpoch 9/50\nTrain metrics: Loss: 2.3447 - Acc: 0.433                                      \nValid metrics: Loss: 2.1888 - Acc: 0.484\nEarly stopping at epoch 9 of 50\nIn the next chapter, we stay with the domain – images – but vary the task: We move on from classification to segmentation."
  },
  {
    "objectID": "image_segmentation.html#segmentation-vs.-classification",
    "href": "image_segmentation.html#segmentation-vs.-classification",
    "title": "19  Image segmentation",
    "section": "19.1 Segmentation vs. classification",
    "text": "19.1 Segmentation vs. classification\nBoth classification and segmentation are about labeling – but both don’t label the same thing. In classification, it’s complete images that are categorized; in segmentation, it’s individual pixels. For each and every pixel, we ask: Which object, or what kind of object, is this pixel part of? No longer are we just interested in saying “this is a cat”; this time, we need to know where exactly that cat is.\nNote the qualifier: exactly. That’s what constitutes the difference to object detection, where the class instances found are surrounded by a so-called bounding box. The type of localization hint provided by such boxes (rectangles, basically) is not sufficient for many tasks in, say, health care, biology, or the earth sciences. For example, in segmenting cell tissue, we need to see the actual boundaries between clusters of cell types, not their straightened versions.\nObject detection is not something we discuss in this book; but we’ve built up quite some experience with classification. Compared to classification, then, what changes?\nRemember how, in the initial chapter on image classification, we talked about translation invariance and translation equivariance. If an operator is translation-invariant, it will return the same measurement when applied to location \\(x\\) and to \\(x=x+n\\). If, on the other hand, it is translation-equivariant, it will return a measurement adapted to the new location. In classification, the difference did not really matter. Now though, in segmentation, we want to avoid transformations that are not shift-equivariant. We can’t afford to lose location-related information anymore.\nTechnically, this means that no layer type should be used that abstracts over location. However, we just saw how successful the typical convolutional architecture is at learning about images. We certainly want to to re-use as much of that architecture as we can.\nWe will avoid pooling layers, since those destroy information about location. But how, then, are we going to build up a hierarchy of features? For that hierarchy to emerge, it would seem like some form of spatial downsizing has to occur – no matter how we make it happen. And for sure, that representational hierarchy is something we can’t sacrifice: Whatever the downstream task, the features are required for the model to develop an “understanding” of sorts about what is displayed in the image. But: If we need to label every single pixel, the network must output an image exactly equal, in resolution, to the input! These are conflicting goals – can they be combined?"
  },
  {
    "objectID": "image_segmentation.html#u-net-a-classic-in-image-segmentation",
    "href": "image_segmentation.html#u-net-a-classic-in-image-segmentation",
    "title": "19  Image segmentation",
    "section": "19.2 U-Net, a “classic” in image segmentation",
    "text": "19.2 U-Net, a “classic” in image segmentation\nThe general U-Net architecture, first described in Ronneberger, Fischer, and Brox (2015), has been used for countless tasks involving image segmentation, as a sub-module of numerous composite models as well as in various standalone forms. To explain the name “U-Net”, there is no better way than to reproduce a figure from the paper (fig. 19.1):\n\n\n\nFigure 19.1: U-Net architecture from Ronneberger, Fischer, and Brox (2015), reproduced with the principal author’s permission.\n\n\nThe left “leg” of the U shows a sequence of steps implementing a successive decrease in spatial resolution, accompanied by an increase in number of filters. The right leg illustrates the opposite mechanism: While number of filters goes down, spatial size increases right until we reach the original resolution of the input. (This is achieved via upsampling, a technique we’ll talk about below.)\nIn consequence, we do in fact build up a feature hierarchy, and at the same time, we are able to classify individual pixels. But the upsampling process should result in significant loss of spatial information – are we really to expect sensible results?\nWell, probably not, were it not for the mechanism, also displayed in the above schematic, of channeling lower-level feature maps through the system. This is the constitutive U-Net idea: In the “down” sequence, when creating higher-level feature maps, we don’t throw away the lower-level ones; instead we keep them, to be eventually fed back into the “up” sequence. In the “up” sequence, once some small-resolution input has been upsampled, the matching-in-size features from the “down” process are appended. This means that each “up” step works on an ensemble of feature maps: ones kept while downsizing, and ones incorporating higher-level information.\nThe fact that U-Net-based architectures are so pervasively used speaks to the power of this idea."
  },
  {
    "objectID": "image_segmentation.html#u-net-a-torch-implementation",
    "href": "image_segmentation.html#u-net-a-torch-implementation",
    "title": "19  Image segmentation",
    "section": "19.3 U-Net – a torch implementation",
    "text": "19.3 U-Net – a torch implementation\nOur torch implementation follows the general U-Net idea. As always in this book, kernel sizes, number of filters, as well as hyper-parameters should be seen as subject to experimentation.\nThe implementation is modular, emphasizing the fact that we can distinguish two phases, an encoding and a decoding phase. Unlike in many other encoder-decoder architectures, these are coupled: Since the decoder needs to incorporate “messages” from the encoder, namely, the conserved feature maps, it will have to know about their sizes.\n\n19.3.1 Encoder\nIn the last chapter, we’ve seen how using a pre-trained feature extractor speeds up training. Now that we need to keep feature maps on our way “down”, can we still apply this technique? We can, and we’ll see how to do it shortly. First, let’s talk about the pre-trained model we’ll be using.\nMobileNet v2 (Sandler et al. (2018)) features a convolutional architecture optimized for mobile use. We won’t go into details here, but there is one thing we’d like to check: Does it lose translation equivariance; for example, by using local pooling? Let’s poke around a bit to find out.\n\nlibrary(torch)\nlibrary(torchvision)\nlibrary(luz)\n\nmodel &lt;- model_mobilenet_v2(pretrained = TRUE)\nmodel\n\nAn `nn_module` containing 3,504,872 parameters.\n\n── Modules ──────────────────────────────────────────────────────────────────────────────────\n• features: &lt;nn_sequential&gt; #2,223,872 parameters\n• classifier: &lt;nn_sequential&gt; #1,281,000 parameters\nImmediately, we see that model_mobilenet_v2() is a wrapping a sequence of two modules: a container called features (the feature detector, evidently), and another called classifier (again, with a telling name). It’s the former we’re interested in.\n\nmodel$features\n\nAn `nn_module` containing 2,223,872 parameters.\n\n── Modules ──────────────────────────────────────────────────────────────────────────────────\n• 0: &lt;conv_bn_activation&gt; #928 parameters\n• 1: &lt;inverted_residual&gt; #896 parameters\n• 2: &lt;inverted_residual&gt; #5,136 parameters\n• 3: &lt;inverted_residual&gt; #8,832 parameters\n• 4: &lt;inverted_residual&gt; #10,000 parameters\n• 5: &lt;inverted_residual&gt; #14,848 parameters\n• 6: &lt;inverted_residual&gt; #14,848 parameters\n• 7: &lt;inverted_residual&gt; #21,056 parameters\n• 8: &lt;inverted_residual&gt; #54,272 parameters\n• 9: &lt;inverted_residual&gt; #54,272 parameters\n• 10: &lt;inverted_residual&gt; #54,272 parameters\n• 11: &lt;inverted_residual&gt; #66,624 parameters\n• 12: &lt;inverted_residual&gt; #118,272 parameters\n• 13: &lt;inverted_residual&gt; #118,272 parameters\n• 14: &lt;inverted_residual&gt; #155,264 parameters\n• 15: &lt;inverted_residual&gt; #320,000 parameters\n• 16: &lt;inverted_residual&gt; #320,000 parameters\n• 17: &lt;inverted_residual&gt; #473,920 parameters\n• 18: &lt;conv_bn_activation&gt; #412,160 parameters\nThus MobileNet is mostly made up of a bunch of “inverted residual” blocks. What do these consist of? Some further poking around tells us:\n\nmodel$features[2]$`0`$conv\n\nAn `nn_module` containing 896 parameters.\n\n── Modules ──────────────────────────────────────────────────────────────────────────────────\n• 0: &lt;conv_bn_activation&gt; #352 parameters\n• 1: &lt;nn_conv2d&gt; #512 parameters\n• 2: &lt;nn_batch_norm2d&gt; #32 parameters\nIf we want to be paranoid, we still need to check the first of these modules:\n\nmodel$features[2]$`0`$conv[1]$`0`\n\nAn `nn_module` containing 352 parameters.\n\n── Modules ─────────────────────────────────────────────────────────────────────────────────────────────\n• 0: &lt;nn_conv2d&gt; #288 parameters\n• 1: &lt;nn_batch_norm2d&gt; #64 parameters\n• 2: &lt;nn_relu6&gt; #0 parameters\nIt seems like there really is no pooling applied. The question then is, how would one obtain – and keep around – feature maps from different stages? Here is how the encoder does it:\n\nencoder &lt;- nn_module(\n  initialize = function() {\n    model &lt;- model_mobilenet_v2(pretrained = TRUE)\n    self$stages &lt;- nn_module_list(list(\n      nn_identity(),\n      model$features[1:2],\n      model$features[3:4],\n      model$features[5:7],\n      model$features[8:14],\n      model$features[15:18]\n    ))\n    for (par in self$parameters) {\n      par$requires_grad_(FALSE)\n    }\n  },\n  forward = function(x) {\n    features &lt;- list()\n    for (i in 1:length(self$stages)) {\n      x &lt;- self$stages[[i]](x)\n      features[[length(features) + 1]] &lt;- x\n    }\n    features\n  }\n)\n\nThe encoder splits up MobileNet v2’s feature extraction blocks into several stages, and applies one stage after the other. Respective results are saved in a list.\nWe can construct an example, and inspect the sizes of the feature maps obtained. For three-channel input of resolution 224 x 224 pixels, we see:\n\nsample &lt;- torch_randn(1, 3, 224, 224)\nsample_features &lt;- encoder()(sample)\npurrr::map(sample_features, purrr::compose(dim, as.array))\n\n[[1]]\n[1]   1   3 224 224\n\n[[2]]\n[1]   1  16 112 112\n\n[[3]]\n[1]   1  24  56  56\n\n[[4]]\n[1]   1  32  28  28\n\n[[5]]\n[1]   1  96  14  14\n\n[[6]]\n[1]   1 320   7   7\nNext, we look at the decoder, which is a bit more complex.\n\n\n19.3.2 Decoder\nThe decoder is made up of configurable blocks. A block receives two input tensors: one that is the result of applying the previous decoder block, and one that holds the feature map produced in the matching encoder stage. In the forward pass, first the former is upsampled, and passed through a nonlinearity. The intermediate result is then prepended to the second argument, the channeled-through feature map. On the resultant tensor, a convolution is applied, followed by another nonlinearity.\n\ndecoder_block &lt;- nn_module(\n  initialize = function(in_channels,\n                        skip_channels,\n                        out_channels) {\n    self$upsample &lt;- nn_conv_transpose2d(\n      in_channels = in_channels,\n      out_channels = out_channels,\n      kernel_size = 2,\n      stride = 2\n    )\n    self$activation &lt;- nn_relu()\n    self$conv &lt;- nn_conv2d(\n      in_channels = out_channels + skip_channels,\n      out_channels = out_channels,\n      kernel_size = 3,\n      padding = \"same\"\n    )\n  },\n  forward = function(x, skip) {\n    x &lt;- x %&gt;%\n      self$upsample() %&gt;%\n      self$activation()\n    input &lt;- torch_cat(list(x, skip), dim = 2)\n    input %&gt;%\n      self$conv() %&gt;%\n      self$activation()\n  }\n)\n\nWe now look closer at how upsampling is achieved. Technically, what is applied is a so-called transposed convolution – hence the name of the layer, nn_conv_transpose2d().(If you’re wondering about the transpose: Quite literally, the kernel is the transpose of a corresponding one that performs downsampling.) However, it’s more intuitive to picture the operation like this: First zeroes are inserted between individual tensor values, and then, a convolution with strides greater than 1 is applied. See fig. 19.2 for a visualization.\n\n\n\nFigure 19.2: Transposed convolution. Copyright Dumoulin and Visin (2016), reproduced under MIT license.\n\n\nEven though we won’t go into technical details, we can do a quick check that really, convolution and transposed convolution affect resolution in opposite ways.\nWe start from a 1 x 1 “image”, and apply a 3 x 3 filter with a stride of 2. Together with padding, this results in an output tensor of size 3 x 3.\n\nimg &lt;- torch_randn(1, 1, 5, 5)\n\nconv &lt;- nn_conv2d(\n  in_channels = 1,\n  out_channels = 1,\n  kernel_size = 3,\n  stride = 2,\n  padding = 1\n)\n\nconvolved &lt;- conv(img)\nconvolved\n\ntorch_tensor\n(1,1,.,.) = \n -0.4996 -0.2898  0.4643\n  0.6608  1.2109  0.8377\n  0.3615  0.5400  0.1567\n[ CPUFloatType{1,1,3,3} ][ grad_fn = &lt;ConvolutionBackward0&gt; ]\nIf we take that output, and now apply a transposed convolution, with the same kernel size, strides, and padding as the above convolution, we are back to the original resolution of 5 x 5:\n\ntransposed_conv &lt;- nn_conv_transpose2d(\n  in_channels = 1,\n  out_channels = 1,\n  kernel_size = 3,\n  stride = 2,\n  padding = 1\n)\n\nupsampled &lt;- transposed_conv(convolved)\nupsampled\n\ntorch_tensor\n(1,1,.,.) = \n  0.4076  0.0940  0.3424  0.1920  0.1078\n  0.2416  0.6456  0.2473  0.6500  0.2643\n  0.0467  0.5028 -0.1243  0.6425 -0.0083\n  0.2682  0.5003  0.2812  0.4150  0.2720\n  0.1398  0.3832  0.0843  0.4155  0.2035\n[ CPUFloatType{1,1,5,5} ][ grad_fn = &lt;ConvolutionBackward0&gt; ]\nAfter that quick check, back to the decoder block. What is the outcome of its very first application?\nAbove, we saw that at the “bottom of the U”, we will have a tensor of size 7 x 7, with 320 channels. This tensor will be upsampled, and concatenated with feature maps from the previous “down” stage. At that stage, there had been 96 channels. That makes two thirds of the information needed to instantiate a decoder block (in_channels and skip_channels). The missing third, out_channels, really is up to us. Here we choose 256.\nWe can thus instantiate a decoder block:\n\nfirst_decoder_block &lt;- decoder_block(\n  in_channels = 320,\n  skip_channels = 96,\n  out_channels = 256\n)\n\nTo do a forward pass, the block needs to be passed two tensors: the maximally-processed features, and their immediate precursors. Let’s check that our understanding is correct:\n\nfirst_decoder_block(\n  sample_features[[6]],\n  sample_features[[5]]\n) %&gt;%\n  dim()\n\n[1]   1 256  14  14\nLet me remark in passing that the purpose of exercises like this is not just to explain some concrete architecture. They’re also supposed to illustrate how with torch, instead of having to rely on some overall idea of what a piece of code will probably do, you can usually find a way to know.\nNow that we’ve talked at length about the decoder blocks, we can quickly characterize their “manager” of sorts, the decoder module. It “merely” instantiates and runs through the blocks.\n\ndecoder &lt;- nn_module(\n  initialize = function(\n    decoder_channels = c(256, 128, 64, 32, 16),\n    encoder_channels = c(16, 24, 32, 96, 320)) {\n    encoder_channels &lt;- rev(encoder_channels)\n    skip_channels &lt;- c(encoder_channels[-1], 3)\n    in_channels &lt;- c(encoder_channels[1], decoder_channels)\n\n    depth &lt;- length(encoder_channels)\n\n    self$blocks &lt;- nn_module_list()\n    for (i in seq_len(depth)) {\n      self$blocks$append(decoder_block(\n        in_channels = in_channels[i],\n        skip_channels = skip_channels[i],\n        out_channels = decoder_channels[i]\n      ))\n    }\n  },\n  forward = function(features) {\n    features &lt;- rev(features)\n    x &lt;- features[[1]]\n    for (i in seq_along(self$blocks)) {\n      x &lt;- self$blocks[[i]](x, features[[i + 1]])\n    }\n    x\n  }\n)\n\n\n\n19.3.3 The “U”\nBefore we take the last step and look at the top-level module, let’s see how the U-shape comes about in our case. Here (tbl. 19.1) is a table, displaying “image” sizes at every step of the “down” and “up” passes, as well as the actors responsible for shape manipulations:\n\n\nTable 19.1: Tensor sizes at various stages of the encoding and decoding chains.\n\n\n\n\n\n\n\nEncoder steps\n\nDecoder steps\n\n\n\n\nInput: 224 x 224 (channels: 3)\n\nOutput: 224 x 224 (channels: 16)\n\n\n\n\n\n\n\n\\(\\Downarrow\\) convolve (MobileNet)\n\nupsample \\(\\Uparrow\\)\n\n\n\n\n\n\n\n112 x 112 (channels: 16)\nappend \\(\\Rightarrow\\)\n112 x 112 (channels: 32)\n\n\n\n\n\n\n\n\\(\\Downarrow\\) convolve (MobileNet)\n\nupsample \\(\\Uparrow\\)\n\n\n\n\n\n\n\n56 x 56 (channels: 24)\nappend \\(\\Rightarrow\\)\n56 x 56 (channels: 64)\n\n\n\n\n\n\n\n\\(\\Downarrow\\) convolve (MobileNet)\n\nupsample \\(\\Uparrow\\)\n\n\n\n\n\n\n\n28 x 28 (channels: 32)\nappend \\(\\Rightarrow\\)\n28 x 28 (channels: 128)\n\n\n\n\n\n\n\n\\(\\Downarrow\\) convolve (MobileNet)\n\nupsample \\(\\Uparrow\\)\n\n\n\n\n\n\n\n14 x 14 (channels: 96)\nappend \\(\\Rightarrow\\)\n14 x 14 (channels: 256)\n\n\n\n\n\n\n\n\\(\\Downarrow\\) convolve (MobileNet)\n\nupsample \\(\\Uparrow\\)\n\n\n\n\n\n\n\n7 x 7 (channels: 320)\nuse as input \\(\\Rightarrow\\)\n7 x 7 (channels: 320)\n\n\n\n\nDid you notice that the final output has sixteen channels? In the end, we want to use the channels dimension for class scores; so really, we’ll need to have as many channels as there are different “pixel classes”. This, of course, is task-dependent, so it makes sense to have a dedicated module take care of it. The top-level module will then be a composition of the “U” part and a final, score-generating layer.\n\n\n19.3.4 Top-level module\nIn our task, there will be three pixel classes. The score-producing submodule can then just be a final convolution, producing three channels:\n\nmodel &lt;- nn_module(\n  initialize = function() {\n    self$encoder &lt;- encoder()\n    self$decoder &lt;- decoder()\n    self$output &lt;- nn_conv2d(\n      in_channels = 16,\n      out_channels = 3,\n      kernel_size = 3,\n      padding = \"same\"\n    )\n  },\n  forward = function(x) {\n    x %&gt;%\n      self$encoder() %&gt;%\n      self$decoder() %&gt;%\n      self$output()\n  }\n)\n\nNow that I’ve already mentioned that there’ll be three pixel classes to tell apart, it’s time for full disclosure: What, then, is the task we’ll use this model on?"
  },
  {
    "objectID": "image_segmentation.html#dogs-and-cats",
    "href": "image_segmentation.html#dogs-and-cats",
    "title": "19  Image segmentation",
    "section": "19.4 Dogs and cats",
    "text": "19.4 Dogs and cats\nThis time, we need an image dataset that has each individual pixel tagged. One of those, the Oxford Pet Dataset, is an ensemble of cats and dogs. As provided by torchdatasets, it comes with three types of target data to choose from: the overall class (cat or dog), the individual breed (there are thirty-seven of them), and a pixel-level segmentation with three categories: foreground, boundary, and background. The default is exactly the type of target we need: the segmentation map.\n\nlibrary(torchvision)\nlibrary(torchdatasets)\n\ndir &lt;- \"~/.torch-datasets\"\n\nds &lt;- oxford_pet_dataset(root = dir, download = TRUE)\n\nImages come in different sizes. As in the previous chapter, we want all of them to share the same resolution; one that also fulfills the requirements of MobileNet v2. The masks will need to be resized in the same way. So far, then, this looks like a case for the transform = and target_transform = arguments we encountered in the last chapter. But if we also want to apply data augmentation, things get more complex.\nImagine we make use of random flipping. An input image will be flipped – or not – according to some probability. But if the image is flipped, the mask better had be, as well! Input and target transformations are not independent, in this case.\nA solution is to create a wrapper around oxford_pet_dataset() that lets us “hook into” the .getitem() method, like so:\n\npet_dataset &lt;- torch::dataset(\n  inherit = oxford_pet_dataset,\n  initialize = function(...,\n                        size,\n                        normalize = TRUE,\n                        augmentation = NULL) {\n    self$augmentation &lt;- augmentation\n    input_transform &lt;- function(x) {\n      x &lt;- x %&gt;%\n        transform_to_tensor() %&gt;%\n        transform_resize(size)\n      if (normalize) {\n        x &lt;- x %&gt;%\n          transform_normalize(\n            mean = c(0.485, 0.456, 0.406),\n            std = c(0.229, 0.224, 0.225)\n          )\n      }\n      x\n    }\n    target_transform &lt;- function(x) {\n      x &lt;- torch_tensor(x, dtype = torch_long())\n      x &lt;- x[newaxis, ..]\n      # interpolation = 0 makes sure we\n      # still end up with integer classes\n      x &lt;- transform_resize(x, size, interpolation = 0)\n      x[1, ..]\n    }\n    super$initialize(\n      ...,\n      transform = input_transform,\n      target_transform = target_transform\n    )\n  },\n  .getitem = function(i) {\n    item &lt;- super$.getitem(i)\n    if (!is.null(self$augmentation)) {\n      self$augmentation(item)\n    } else {\n      list(x = item$x, y = item$y)\n    }\n  }\n)\n\nWith this wrapper, all we have to do is create a custom function that decides what augmentation to apply once per input-target pair, and manually calls the respective transformation functions.\nHere, we flip every second image (on average). And if we do, we flip the mask as well.\n\naugmentation &lt;- function(item) {\n  vflip &lt;- runif(1) &gt; 0.5\n\n  x &lt;- item$x\n  y &lt;- item$y\n\n  if (vflip) {\n    x &lt;- transform_vflip(x)\n    y &lt;- transform_vflip(y)\n  }\n\n  list(x = x, y = y)\n}\n\nSince we’re at it, let me mention types of augmentation that should be helpful with slightly different formulations of the task. Why don’t we try (small-ish) random rotations, or translations, or both?\nLike this:\n\nangle &lt;- runif(1, -12, 12)\nx &lt;- transform_rotate(x, angle)\n\n# same effect as interpolation = 0, above\ny &lt;- transform_rotate(y, angle, resample = 0)\n\nAs-is, that piece of code does not work. This is because a rotation will introduce black pixels, or – technically speaking – zeroes in the tensor. Before, possible target classes went from 1 to 3. Now there’s an additional class, 0. As a consequence, loss computation will expect the model output to have four, not three, slots in the second dimension – and fail.\nThere are several possible workarounds. First, we could take the risk and assume that in nearly all cases, this will affect only background pixels. Under that assumption, we can just set all values that have turned to 0 to 2, the background class.\nAnother thing we can do is upsize the image, trigger the rotation, and downsize again:\n\nangle &lt;- runif(1, -12, 12)\n\nx &lt;- transform_resize(x, size = c(268, 268))\ny &lt;- transform_resize(\n  y,\n  size = c(268, 268),\n  interpolation = 0\n)\n\nx &lt;- transform_rotate(x, angle)\ny &lt;- transform_rotate(y, angle, resample = 0)\n\nx &lt;- transform_center_crop(x, size = c(224, 224))\ny &lt;- transform_center_crop(y, size = c(224, 224))\n\nIn this specific case, there still is a problem, though. In my experiments, training performance got a lot worse. This could be because we have “boundary” class. The whole râison d’être of a boundary is to be sharp; the downside of sharp boundaries is their not dealing well with resizings.\nHowever, in the real world, segmentation requirements will vary widely. Maybe you have just two classes, foreground and background. Maybe there are many. Experimenting with rotations (and translations, that also will introduce black pixels) cannot hurt.\nBack on the main track, we can now make use of the dataset wrapper, pet_dataset(), to instantiate the training and validation sets:\n\ntrain_ds &lt;- pet_dataset(root = dir,\n                        split = \"train\",\n                        size = c(224, 224),\n                        augmentation = augmentation)\n\nvalid_ds &lt;- pet_dataset(root = dir,\n                        split = \"valid\",\n                        size = c(224, 224))\n\nWe create the data loaders, and run the learning rate finder (fig. 19.3):\n\ntrain_dl &lt;- dataloader(\n  train_ds,\n  batch_size = 32,\n  shuffle = TRUE\n)\nvalid_dl &lt;- dataloader(valid_ds, batch_size = 32)\n\nmodel &lt;- model %&gt;%\n  setup(\n    optimizer = optim_adam,\n    loss = nn_cross_entropy_loss()\n  )\n\nrates_and_losses &lt;- model %&gt;% lr_finder(train_dl)\nrates_and_losses %&gt;% plot()\n\n\n\n\nFigure 19.3: Learning rate finder, run on the Oxford Pet Dataset.\n\n\nFrom this plot, we conclude that a maximum learning rate of 0.01, at least when run with a one-cycle strategy, should work fine.\n\nfitted &lt;- model %&gt;%\n  fit(train_dl, epochs = 20, valid_data = valid_dl,\n      callbacks = list(\n        luz_callback_early_stopping(patience = 2),\n        luz_callback_lr_scheduler(\n          lr_one_cycle,\n          max_lr = 0.01,\n          epochs = 20,\n          steps_per_epoch = length(train_dl),\n          call_on = \"on_batch_end\")\n      ),\n      verbose = TRUE)\n\nEpoch 1/20\nTrain metrics: Loss: 0.6782                                               \nValid metrics: Loss: 0.4433\nEpoch 2/20\nTrain metrics: Loss: 0.3705\nValid metrics: Loss: 0.3331\nEpoch 3/20\nTrain metrics: Loss: 0.315\nValid metrics: Loss: 0.2999\n...\n...\nEpoch 11/20\nTrain metrics: Loss: 0.1803\nValid metrics: Loss: 0.2161\nEpoch 12/20\nTrain metrics: Loss: 0.1751\nValid metrics: Loss: 0.2191\nEpoch 13/20\nTrain metrics: Loss: 0.1708\nValid metrics: Loss: 0.2203\nEarly stopping at epoch 13 of 20\nThis looks like decent improvement; but – as always – we want to see what the model actually says. To that end, we generate segmentation masks for the first eight observations in the validation set, and plot them overlayed on the images. (We probably don’t need to also display the ground truth, humans being rather familiar with cats and dogs.)\nA convenient way to plot an image and superimpose a mask is provided by the raster package.\n\nlibrary(raster)\n\nPixel intensities have to be between zero and one, which is why in the dataset wrapper, we have made it so normalization can be switched off. To plot the actual images, we just instantiate a clone of valid_ds that leaves the pixel values unchanged. (The predictions, on the other hand, will still have to be obtained from the original validation set.)\n\nvalid_ds_4plot &lt;- pet_dataset(\n  root = dir,\n  split = \"valid\",\n  size = c(224, 224),\n  normalize = FALSE\n)\n\nFinally, the predictions are generated, and overlaid over the images one-by-one (fig. 19.4):\n\nindices &lt;- 1:8\n\npreds &lt;- predict(\n  fitted,\n  dataloader(dataset_subset(valid_ds, indices))\n)\n\npng(\n  \"pet_segmentation.png\",\n  width = 1200,\n  height = 600,\n  bg = \"black\"\n)\n\npar(mfcol = c(2, 4), mar = rep(2, 4))\n\nfor (i in indices) {\n  mask &lt;- as.array(\n    torch_argmax(preds[i, ..], 1)$to(device = \"cpu\")\n  )\n  mask &lt;- raster::ratify(raster::raster(mask))\n\n  img &lt;- as.array(valid_ds_4plot[i][[1]]$permute(c(2, 3, 1)))\n  cond &lt;- img &gt; 0.99999\n  img[cond] &lt;- 0.99999\n  img &lt;- raster::brick(img)\n\n  # plot image\n  raster::plotRGB(img, scale = 1, asp = 1, margins = TRUE)\n  # overlay mask\n  plot(\n    mask,\n    alpha = 0.4,\n    legend = FALSE,\n    axes = FALSE,\n    add = TRUE\n  )\n}\n\ndev.off()\n\n\n\n\nFigure 19.4: Cats and dogs: Sample images and predicted segmentation masks.\n\n\nThis looks pretty reasonable!\nNow, it’s time we let images be, and look at different application domains. In the next chapter, we explore deep learning on tabular data.\n\n\n\n\nDumoulin, Vincent, and Francesco Visin. 2016. “A guide to convolution arithmetic for deep learning.” arXiv e-Prints, March, arXiv:1603.07285. https://arxiv.org/abs/1603.07285.\n\n\nRonneberger, Olaf, Philipp Fischer, and Thomas Brox. 2015. “U-Net: Convolutional Networks for Biomedical Image Segmentation.” CoRR abs/1505.04597. http://arxiv.org/abs/1505.04597.\n\n\nSandler, Mark, Andrew G. Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen. 2018. “Inverted Residuals and Linear Bottlenecks: Mobile Networks for Classification, Detection and Segmentation.” CoRR abs/1801.04381. http://arxiv.org/abs/1801.04381."
  },
  {
    "objectID": "tabular_data.html#types-of-numerical-data-by-example",
    "href": "tabular_data.html#types-of-numerical-data-by-example",
    "title": "20  Tabular data",
    "section": "20.1 Types of numerical data, by example",
    "text": "20.1 Types of numerical data, by example\nA classic distinction is that between interval, ordinal, and categorical data. Assume that for three individuals, we’re told that, for some feature, their respective scores are 2, 1, and 3. What are we to make of this?\nFirst, the numbers could just be encoding “artifacts”. That would be the case if, say, 1 stood for apple, 2 for orange, and 3, for pineapple. Then, we’d have categorical data.\nSecond, the numbers could represent grades, 1 denoting the best, 2 the second-best, 3 the one thereafter … and so on. In this case, there is a ranking, or an ordering, between values. We have no reason to assume, though, that the distance between 1 and 2 is the same as that between 2 and 3. These are ordinal data.\nFinally, maybe those distances are the same. Now, we’re dealing with interval data.\nPut in this way, the distinction may seem trivial. However, with real-world datasets, it is not always easy to know what type of data you’re dealing with.\nTo illustrate, we now inspect the popular heart disease dataset, available on the UCI Machine Learning Repository. It is this dataset we are going to build a classifier for, in this chapter.\nIn what follows, the aspiration is not to be completely sure we’re “getting it right”; instead, it’s about showing how to approach the task in a conscientious (if I may say so) way.\nWe start by loading the dataset. Here heart_disease is the target, and missing values are indicated by a question mark.\n\nlibrary(torch)\nlibrary(luz)\n\nlibrary(purrr)\nlibrary(readr)\nlibrary(dplyr)\n\nuci &lt;- \"https://archive.ics.uci.edu\"\nds_path &lt;- \"ml/machine-learning-databases/heart-disease\"\nds_file &lt;- \"processed.cleveland.data\"\n\ndownload.file(\n file.path(uci, ds_path, ds_file),\n destfile = \"resources/tabular-heart.csv\"\n)\n\nheart_df &lt;- read_csv(\n  \"resources/tabular-heart.csv\",\n  col_names = c(\n    \"age\", \n    # 1 = male; 0 = female\n    \"sex\", \n    # chest pain type\n    # (1 = typical angina, 2 = atypical angina,\n    #  3 = non-anginal pain, 4 = asymptomatic)\n    \"pain_type\", \n    # in mm Hg on admission\n    \"resting_blood_pressure\", \n    # serum cholesterol in mg/dl\n    \"chol\", \n    # &gt; 120 mg/dl, true (1) or false (0)\n    \"fasting_blood_sugar\", \n    # 0 = normal, 1 = ST-T wave abnormality\n    # (T wave inversions and/or ST elevation\n    # or depression of &gt; 0.05 mV),\n    # 2 = probable or definite left ventricular\n    # hypertrophy by Estes' criteria\n    \"rest_ecg\", \n    # during exercise\n    \"max_heart_rate\", \n     # exercise induced angina (1 = yes, 0 = no),\n    \"ex_induced_angina\",\n    # ST depression induced by exercise relative to rest \n    \"old_peak\", \n    # slope of the peak exercise ST segment\n    # (1 = upsloping, 2 = flat, 3 = downsloping) \n    \"slope\", \n    # number of major vessels (0-3) colored by fluoroscopy\n    \"ca\", \n    # 3 = normal; 6 = fixed defect; 7 = reversible defect\n    \"thal\", \n    # 1-4 = yes; 0 = no\n    \"heart_disease\" \n  ),\n  na = \"?\")\n\nAs you see, I’ve annotated the features with information given by the dataset creators.\nBased on this information, as well as the actual values in the dataset, the following look like interval data to me:\n\nage,\nresting_blood_pressure,\nchol,\nmax_heart_rate, and\nold_peak .\n\nAt the other end of the dimension, some features are clearly conceived as binary here: namely, the predictors sex, fasting_blood_sugar, and ex_induced_angina, as well as the target, heart_disease. Predictors that are binary, with values either zero or one, are usually treated as numerical. No information is lost that way. An alternative would be to represent them by length-two vectors: either [0, 1] (when the given value is 0) or [1, 0] (when the given value is 1). This is called one-hot encoding, “one-hot” referring to just a single position in the vector being non-zero (namely, the one corresponding to the category in question). Normally, this is only done when there are more than two categories, and we’ll get back to this technique when we implement the dataset() for this problem.\nWe now have just pain_type, rest_ecg, slope, ca, and thal remaining. Of those, pain_type , rest_ecg, and thal look categorical, or maybe ordinal, to some degree. Normally in machine learning, ordinal data are treated as categorical, and unless the number of different categories is high, this should not result in significant loss of information.\nWhat about slope? To a machine learning person, the naming alone seems to suggest a continuous feature; and values as nicely ordered as 1, 2, 3 might make you think that the variable must be at least ordinal. However, a quick web search already turns up a different, and much more complicated, reality.1 We’ll thus definitely want to treat this variable as categorical. (If this were a real-world task, we should try consulting a domain expert, to find out whether there’s a better solution.)\nFinally, for ca, we – or me, actually – don’t know the implication of how many major blood vessels (zero to four) got colored by fluoroscopy. (Again, if you have access to a domain expert, make use of the opportunity and ask them.) The safest way is to not assume an equal distance between measurements, but treat this feature as merely ordinal, and thus, categorical.\nNow that we’ve discussed what to do, we can implement our data provider: heart_dataset()."
  },
  {
    "objectID": "tabular_data.html#a-torch-dataset-for-tabular-data",
    "href": "tabular_data.html#a-torch-dataset-for-tabular-data",
    "title": "20  Tabular data",
    "section": "20.2 A torch dataset for tabular data",
    "text": "20.2 A torch dataset for tabular data\nBeyond adequate feature representation, there is one additional thing to take care of. The dataset contains unknown values, which we coded as NA:\n\nwhich(is.na(heart_df), arr.ind = TRUE)\n\n     row col\n[1,] 167  12\n[2,] 193  12\n[3,] 288  12\n[4,] 303  12\n[5,]  88  13\n[6,] 267  13\nFortunately, these occur in two columns only: thal and ca.\n\nheart_df %&gt;% group_by(thal) %&gt;% summarise(n())\n\ntibble: 4 × 2\nthal `n()`\n&lt;dbl&gt; &lt;int&gt;\n  1      3   166\n  2      6    18\n  3      7   117\n  4     NA     2\n\nheart_df %&gt;% group_by(ca) %&gt;% summarise(n())\n\nA tibble: 5 × 2\nca `n()`\n&lt;dbl&gt; &lt;int&gt;\n  1     0   176\n  2     1    65\n  3     2    38\n  4     3    20\n  5    NA     4\nSeeing how we’re totally clueless as to what caused these missing values, and considering that – conveniently – both features in question are categorical and of low cardinality, it seems easiest to just have those NAs represented by an additional factor value.\nThat decided, we can implement heart_dataset(). Numerical features will be scaled, a measure that should always be taken when they’re of different orders of magnitude. This will significantly speed up training.\nAs to the categorical features, one thing we could do is one-hot encode them. For example:\n\nnnf_one_hot(\n  torch_tensor(\n    heart_df$slope,\n    dtype = torch_long()\n  )\n) %&gt;% print(n = 5)\n\ntorch_tensor\n 0  0  1\n 0  1  0\n 0  1  0\n 0  0  1\n 1  0  0\n... [the output was truncated (use n=-1 to disable)]\n[ CPULongType{303,3} ]\nWith one-hot encoding, we guarantee that each feature value differs from all others exactly to the same degree. However, we can do better. We can use a technique called embedding to represent these feature values in a space where they are not all equally distinct from one another.\nWe’ll see how that works when we build the model, but now, we need to make sure the prerequisites are satisfied. Namely, torch’s embedding modules expect their input to be integers, not one-hot encoded vectors (or anything else). So what we’ll do is convert the categorical features to factors, and from there, to integers.\nPutting it all together, we arrive at the following dataset() definition:\n\nheart_dataset &lt;- dataset(\n  initialize = function(df) {\n    self$x_cat &lt;- self$get_categorical(df)\n    self$x_num &lt;- self$get_numerical(df)\n    self$y &lt;- self$get_target(df)\n  },\n  .getitem = function(i) {\n    x_cat &lt;- self$x_cat[i, ]\n    x_num &lt;- self$x_num[i, ]\n    y &lt;- self$y[i]\n    list(x = list(x_cat, x_num), y = y)\n  },\n  .length = function() {\n    dim(self$y)[1]\n  },\n  get_target = function(df) {\n    heart_disease &lt;- ifelse(df$heart_disease &gt; 0, 1, 0)\n    heart_disease\n  },\n  get_numerical = function(df) {\n    df %&gt;%\n      select(\n        -(c(\n          heart_disease, pain_type,\n          rest_ecg, slope, ca, thal\n        ))\n      ) %&gt;%\n      mutate(across(.fns = scale)) %&gt;%\n      as.matrix()\n  },\n  get_categorical = function(df) {\n    df$ca &lt;- ifelse(is.na(df$ca), 999, df$ca)\n    df$thal &lt;- ifelse(is.na(df$thal), 999, df$thal)\n    df %&gt;%\n      select(\n        pain_type, rest_ecg, slope, ca, thal\n      ) %&gt;%\n      mutate(\n        across(.fns = compose(as.integer, as.factor))\n      ) %&gt;%\n      as.matrix()\n  }\n)\n\nLet’s see if the output it produces matches our expectations.\n\nds &lt;- heart_dataset(heart_df)\nds[1]\n\n$x\n$x[[1]]\npain_type  rest_ecg     slope        ca      thal \n        1         3         3         1         2 \n\n$x[[2]]\n                   age                    sex \n            0.94715962             0.68506916 \nresting_blood_pressure                   chol \n            0.75627397            -0.26446281 \n   fasting_blood_sugar         max_heart_rate \n            2.39048352             0.01716893 \n     ex_induced_angina               old_peak \n           -0.69548004             1.08554229 \n\n\n$y\n[1] 0\nIt does.\nThis is a small dataset, so we’ll forego creation of separate test and validation sets.\n\ntrain_indices &lt;- sample(\n  1:nrow(heart_df), size = floor(0.8 * nrow(heart_df)))\nvalid_indices &lt;- setdiff(\n  1:nrow(heart_df), train_indices)\n\ntrain_ds &lt;- dataset_subset(ds, train_indices)\ntrain_dl &lt;- train_ds %&gt;% \n  dataloader(batch_size = 256, shuffle = TRUE)\n\nvalid_ds &lt;- dataset_subset(ds, valid_indices)\nvalid_dl &lt;- valid_ds %&gt;% \n  dataloader(batch_size = 256, shuffle = FALSE)\n\nWe’re ready to move on to the model. But first, let’s talk about embeddings."
  },
  {
    "objectID": "tabular_data.html#embeddings-in-deep-learning-the-idea",
    "href": "tabular_data.html#embeddings-in-deep-learning-the-idea",
    "title": "20  Tabular data",
    "section": "20.3 Embeddings in deep learning: The idea",
    "text": "20.3 Embeddings in deep learning: The idea\nThe main idea behind embeddings, the way this term is used in deep learning, is to go beyond the default “all are equally distinct from each other” representation of categorical values.\nAs in one-hot encoding, scalars get mapped to vectors. But this time, there’s no restriction as to how many slots may be non-empty, or as to the values they may take. For example, integers 1, 2, and 3 could get mapped to the following tensors:\n\none &lt;- torch_tensor(c(1.555, 0.21, -3.33, 0.0007, 0.07))\ntwo &lt;- torch_tensor(c(0.33, -0.03, -2.177, 1.1, 0.0005))\nthree &lt;- torch_tensor(c(-0.33, 2.99, 1.77, 1.08, 3.001))\n\nNow, we can use nnf_cosine_similarity() to find out how close to each other these vectors are. Working, for convenience, in two dimensions, say we have two parallel vectors, pointing in the same direction. The angle between them is zero, and its cosine is one:\n\nnnf_cosine_similarity(\n  torch_ones(2),\n  torch_ones(2) * 2.5,\n  dim = 1\n)\n\ntorch_tensor\n1\n[ CPUFloatType{} ]\nThus, a value of one indicates maximal similarity. In contrast, now take them to still be parallel, but pointing in opposite directions. The angle is one-hundred-eighty degrees, and the cosine is minus one:\n\nnnf_cosine_similarity(\n  torch_ones(2),\n  -1.5 * torch_ones(2),\n  dim = 1\n)\n\ntorch_tensor\n-1\n[ CPUFloatType{} ]\nThese vectors are maximally dissimilar. In-between, we have angles around ninety degrees, with vectors being (approximately) orthogonal; or “independent”, in common parlance. With an angle of exactly ninety degrees, the cosine is zero:\n\nnnf_cosine_similarity(\n  torch_tensor(c(1, 0)),\n  torch_tensor(c(0, 1)),\n  dim = 1\n)\n\ntorch_tensor\n0\n[ CPUFloatType{} ]\nThings work analogously in higher dimensions. So, we can determine which of the above vectors one, two, and three are closest to each other:\n\nnnf_cosine_similarity(one, two, dim = 1)\nnnf_cosine_similarity(one, three, dim = 1)\nnnf_cosine_similarity(two, three, dim = 1)\n\ntorch_tensor\n0.855909\n[ CPUFloatType{} ]\n\ntorch_tensor\n-0.319886\n[ CPUFloatType{} ]\n\ntorch_tensor\n-0.245948\n[ CPUFloatType{} ]\nLooking at how those were defined, these values make sense."
  },
  {
    "objectID": "tabular_data.html#embeddings-in-deep-learning-implementation",
    "href": "tabular_data.html#embeddings-in-deep-learning-implementation",
    "title": "20  Tabular data",
    "section": "20.4 Embeddings in deep learning: Implementation",
    "text": "20.4 Embeddings in deep learning: Implementation\nBy now you probably agree that embeddings are useful. But how do we get them?\nConveniently, these vectors are learned as part of model training. Embedding modules are modules that take integer inputs and learn to map them to vectors.\nWhen creating such a module, you specify how many different integers there are (num_embeddings), and how long you want the learned vectors to be (embedding_dim). Together, these parameters tell the module how its weight matrix should look. The weight matrix is nothing but a look-up table (a mutable one, though!) that maps integers to corresponding vectors:\n\nmodule &lt;- nn_embedding(num_embeddings = 3, embedding_dim = 5)\nmodule$weight\n\ntorch_tensor\n 2.3271  0.0894  0.6558 -0.5836 -0.1074\n 0.0367  0.1822 -0.0446  0.2059 -0.7540\n-0.7577 -1.7773 -0.6619  1.2884  0.3946\n[ CPUFloatType{3,5} ][ requires_grad = TRUE ]\nAt module creation, these mappings are initialized randomly. Still, we can test that the code does what we want by calling the module on some feature – slope, say:\n\n# slope\nmodule(ds[1]$x[[1]][3])\n\ntorch_tensor\n-0.7577 -1.7773 -0.6619  1.2884  0.3946\n[ CPUFloatType{1,5} ][ grad_fn = &lt;EmbeddingBackward0&gt; ]\nNow, you could say that we glossed over, with a certain nonchalance, the question of how these mappings are optimized. Technically, this works like for any module: by means of backpropagation. But there is an implication: It follows that the overall model, and even more importantly, the given task, will determine how “good” those learned mappings are.\nAnd yet, there is something even more important: the data.\nSure, that goes without saying, you may think. But when embeddings are used, the learned mappings are sometimes presented as an additional outcome, a surplus benefit, of sorts. For example, the model itself might be a classifier, predicting whether people are going to default on a loan or not. Now assume that there is an input feature – ethnicity, say – that is processed using embeddings. Once the model’s been trained, the acquired representation is extracted. In turn, that representation will reflect all problems – biases, injustices, distortions – that are present in the training dataset.\nBelow, I will show how to obtain and plot such a representation. To a domain expert, this representation may or may not seem adequate; in any case, no harm is likely to be caused in this example. However, when working on real-world tasks, we always have to be aware of possible harms, and rigorously analyse any biases and assumptions inherent in the training workflow.\nGetting back to the implementation, here is the embedding module we use for our task. Actually, there is no single embedding module; there is one for each categorical feature. The wrapper, embedding_module(), keeps them all in an nn_module_list(), and when called, iterates over them and concatenates their outputs:\n\nembedding_module &lt;- nn_module(\n  initialize = function(cardinalities, embedding_dim) {\n    self$embeddings &lt;- nn_module_list(\n      lapply(\n        cardinalities,\n        function(x) {\n          nn_embedding(\n            num_embeddings = x, embedding_dim = embedding_dim\n          )\n        }\n      )\n    )\n  },\n  forward = function(x) {\n    embedded &lt;- vector(\n      mode = \"list\",\n      length = length(self$embeddings)\n    )\n    for (i in 1:length(self$embeddings)) {\n      embedded[[i]] &lt;- self$embeddings[[i]](x[, i])\n    }\n    torch_cat(embedded, dim = 2)\n  }\n)\n\nThis wrapper – let’s call it “embedder” – will be one of the modules that make up the top-level model."
  },
  {
    "objectID": "tabular_data.html#model-and-model-training",
    "href": "tabular_data.html#model-and-model-training",
    "title": "20  Tabular data",
    "section": "20.5 Model and model training",
    "text": "20.5 Model and model training\nThe top-level module’s logic is straightforward. For the categorical part of its input, it delegates to the embedder, and to the embeddings obtained it appends the numerical part. The resulting tensor is then passed through a sequence of linear modules:\n\nmodel &lt;- nn_module(\n  initialize = function(cardinalities,\n                        num_numerical,\n                        embedding_dim,\n                        fc1_dim,\n                        fc2_dim) {\n    self$embedder &lt;- embedding_module(\n      cardinalities,\n      embedding_dim\n    )\n    self$fc1 &lt;- nn_linear(\n      embedding_dim * length(cardinalities) + num_numerical,\n      fc1_dim\n    )\n    self$drop1 &lt;- nn_dropout(p = 0.7)\n    self$fc2 &lt;- nn_linear(fc1_dim, fc2_dim)\n    self$drop2 &lt;- nn_dropout(p = 0.7)\n    self$output &lt;- nn_linear(fc2_dim, 1)\n  },\n  forward = function(x) {\n    embedded &lt;- self$embedder(x[[1]])\n    all &lt;- torch_cat(list(embedded, x[[2]]), dim = 2)\n    score &lt;- all %&gt;%\n      self$fc1() %&gt;%\n      nnf_relu() %&gt;%\n      self$drop1() %&gt;%\n      self$fc2() %&gt;%\n      nnf_relu() %&gt;%\n      self$drop2() %&gt;%\n      self$output()\n    score[, 1]\n  }\n)\n\nLooking at the final output, you see that these are raw scores, not probabilities. With a binary target, this means we’ll make use of nn_bce_with_logits_loss() to train the model.\nNow, we still need some housekeeping and configuration:\n\n# cardinalities of categorical features\ncardinalities &lt;- heart_df %&gt;%\n  select(pain_type, rest_ecg, slope, ca, thal) %&gt;%\n  mutate(across(.fns = as.factor)) %&gt;%\n  summarise(across(.fns = nlevels))\n\n# cardinalities of categorical features,\n# adjusted for presence of NAs in ca and thal\ncardinalities &lt;- cardinalities + c(0, 0, 0, 1, 1) \n\n# number of numerical features\nnum_numerical &lt;- ncol(heart_df) - length(cardinalities) - 1\n\nembedding_dim &lt;- 7\n\nfc1_dim &lt;- 32\nfc2_dim &lt;- 32\n\nNote here the requested embedding dimension(s), embedding_dim.\nUsual best practice would choose lower values, corresponding, roughly, to half a feature’s cardinality. For example, if there were thirty different values for some category, we might go with a vector length of about fifteen. And definitely, this is what I’d do if I had thirty values. But in this example, cardinalities are much lower: two, three, four, or five. (And that’s already with NA taken as an additional factor value.) Halving those numbers would hardly leave any representational capacity. So here, I went the opposite way: give the model a significantly bigger space to play with. (Beyond seven, the chosen value, I didn’t see further training improvements.)\nAll preparatory work done, we can train the model. Normally, at this point we’d run the learning rate finder. Here, the dataset really is too small for that to make sense, at least without substantially tweaking the learning rate finder’s default settings. Also, with a dataset as tiny as this, experimentation takes very little time; and a few quick experiments are what the learning rate chosen is based on.\n\nfitted &lt;- model %&gt;%\n  setup(\n    optimizer = optim_adam,\n    loss = nn_bce_with_logits_loss(),\n    metrics = luz_metric_binary_accuracy_with_logits()\n  ) %&gt;%\n  set_hparams(\n    cardinalities = cardinalities,\n    num_numerical = num_numerical,\n    embedding_dim = embedding_dim,\n    fc1_dim = fc1_dim, fc2_dim\n  ) %&gt;%\n  set_opt_hparams(lr = 0.001) %&gt;%\n  fit(train_dl,\n    epochs = 200,\n    valid_data = valid_dl,\n    callbacks = list(\n      luz_callback_early_stopping(patience = 10)\n    ),\n    verbose = TRUE\n  )\n\n# Epoch 1/200\n# Train metrics: Loss: 0.7445 - Acc: 0.4091\n# Valid metrics: Loss: 0.6988 - Acc: 0.541\n# Epoch 2/200\n# Train metrics: Loss: 0.7036 - Acc: 0.5248\n# Valid metrics: Loss: 0.6966 - Acc: 0.5246\n# Epoch 3/200\n# Train metrics: Loss: 0.7029 - Acc: 0.5124\n# Valid metrics: Loss: 0.6946 - Acc: 0.5082\n# ...\n# ...\n# Epoch 124/200\n# Train metrics: Loss: 0.3884 - Acc: 0.8512\n# Valid metrics: Loss: 0.4026 - Acc: 0.8525\n# Epoch 125/200\n# Train metrics: Loss: 0.3961 - Acc: 0.8471\n# Valid metrics: Loss: 0.4023 - Acc: 0.8525\n# Epoch 126/200\n# Train metrics: Loss: 0.359 - Acc: 0.8554\n# Valid metrics: Loss: 0.4019 - Acc: 0.8525\n# Early stopping at epoch 126 of 200\n\nfitted %&gt;% plot()\n\n\n\n\nFigure 20.1: Losses and accuracies (training and validation, resp.) for binary heart disease classification.\n\n\nAs you see (fig. 20.1), model training went smoothly, and yielded good accuracy.\nBefore we leave the topic of tabular data, here’s how to extract, post-process, and visualize the learned representations."
  },
  {
    "objectID": "tabular_data.html#embedding-generated-representations-by-example",
    "href": "tabular_data.html#embedding-generated-representations-by-example",
    "title": "20  Tabular data",
    "section": "20.6 Embedding-generated representations by example",
    "text": "20.6 Embedding-generated representations by example\nHere, example-wise, is the weight matrix for slope.\n\nembedding_weights &lt;- vector(mode = \"list\")\n\nfor (i in 1:length(fitted$model$embedder$embeddings)) {\n  embedding_weights[[i]] &lt;-\n    fitted$model$embedder$embeddings[[i]]$\n    parameters$weight$to(device = \"cpu\")\n}\n\nslope_weights &lt;- embedding_weights[[3]]\nslope_weights\n\ntorch_tensor\n-0.9226 -1.0282  0.8935  0.3152  0.5481  0.8376  0.9990\n 0.0604  0.1904  0.6788  0.8542  0.8007  1.5226 -0.1789\n 1.2504 -0.0827 -0.7259  1.2885 -1.7847  0.1813  0.4418\n[ CPUFloatType{3,7} ][ requires_grad = TRUE ]\nFor visualization, we’d like to reduce the number of dimensions from seven to two. We can accomplish that by running PCA – Principal Components Analysis – using R’s native prcomp():\n\npca &lt;- prcomp(slope_weights, center = TRUE, scale = TRUE)\npca\n\nStandard deviations (1, .., p=3):\n[1] 2.138539e+00 1.557771e+00 2.173695e-16\n\nRotation (n x k) = (7 x 3):\n            PC1         PC2         PC3\n[1,]  0.4650931 -0.06650143  0.18974889\n[2,]  0.2915618 -0.50187753  0.42034629\n[3,] -0.4539313 -0.15412668  0.46092035\n[4,]  0.4562585 -0.14058058 -0.16106015\n[5,] -0.4203277 -0.28128658  0.29209764\n[6,] -0.2903728 -0.50317497 -0.68060609\n[7,] -0.1531762  0.60652402  0.01925451\nThis printout reflects two pieces of information: the standard deviations of the principal components (also available as pca$sdev), and the matrix of variable loadings (also available as pca$rotation).\nThe former reflect how important the resulting components are; we use them to decide if reduction to two components seems permissible. Here’s how much variance is explained by the three components each:\n\n(pca$sdev^2 / sum(pca$sdev^2)) %&gt;% round(2)\n\n[1] 0.65 0.35 0.00\nFrom that output, leaving out the third component is more than permissible.\nThe matrix of variable loadings, on the other hand, tells us how big a role each variable (here: each slot in the learned embeddings) plays in determining the “meaning” of a component. Here, a visualization is more helpful than the raw numbers (fig. 20.2):\n\nbiplot(pca)\n\n\n\n\nFigure 20.2: heart_disease$slope: PCA of embedding weights, biplot visualizing factor loadings.\n\n\nThis plot could be taken as indicating that – from a purely representational standpoint, i.e., not taking into account training performance – an embedding dimensionality of four would have been sufficient.\nFinally, how about the main thing we’re after: a representation of slope categories in two-dimensional space?\nThis information is provided by pca$x. It tells us how the original input categories relate to the principal components.\n\npca$x\n\n            PC1        PC2           PC3\n[1,] -1.9164879  1.1343079  1.783213e-17\n[2,] -0.3903307 -1.7761457  6.993398e-16\n[3,]  2.3068187  0.6418377 -4.977644e-16\nLeaving out the third component, the distribution of categories in (two-dimensional) space is easily visualized (fig. 20.3):\n\nlibrary(ggplot2)\nlibrary(ggrepel)\n\nslopes &lt;- c(\"up\", \"flat\", \"down\")\n\npca$x[, 1:2] %&gt;%\n  as.data.frame() %&gt;%\n  mutate(class = slopes) %&gt;%\n  ggplot(aes(x = PC1, y = PC2)) +\n  geom_point() +\n  geom_label_repel(aes(label = class)) +\n  coord_cartesian(\n    xlim = c(-2.5, 2.5),\n    ylim = c(-2.5, 2.5)\n  ) +\n  theme(aspect.ratio = 1) +\n  theme_classic()\n\n\n\n\nFigure 20.3: heart_disease$slope: PCA of embedding weights, locating the original input values in two-dimensional space.\n\n\nWhether this representation makes sense, I’ll leave to the experts to judge. My goal here was to demonstrate the technique, so you can employ it when it does yield some insight. Even when not, embedding modules do contribute substantially to training success for categorical-feature or mixed-feature data."
  },
  {
    "objectID": "tabular_data.html#footnotes",
    "href": "tabular_data.html#footnotes",
    "title": "20  Tabular data",
    "section": "",
    "text": "See, e.g., https://en.my-ekg.com/how-read-ekg/st-segment.html, or https://ecg.utah.edu/lesson/10, or Wikipedia.↩︎"
  },
  {
    "objectID": "time_series.html#deep-learning-for-sequences-the-idea",
    "href": "time_series.html#deep-learning-for-sequences-the-idea",
    "title": "21  Time series",
    "section": "21.1 Deep learning for sequences: the idea",
    "text": "21.1 Deep learning for sequences: the idea\nSay we have a sequence of daily average temperatures, measured in degrees Celsius: -1.1, 2.0, -0.2, -0.9, 4.5, -3.6, -9.1. Clearly, these values are not independent; we’d hardly guess that the very next measurement would result in in, say, 21. In fact, if these seven averages were all you’re given, your best guess for the next day would probably just be -9.1. But when people say “time series”, they have longer sequences in mind. With longer sequences, you can try to detect patterns, such as trends or periodicities. And that’s what the established techniques in time series analysis do.\nFor a deep learning model to do the same, it first of all has to “perceive” individual values as sequential. We make that happen by increasing tensor dimensionality by one, and using the additional dimension for sequential ordering. Now, the model has to do something useful with that. Think back of what it is a linear model is doing: It takes input \\(\\mathbf{X}\\), multiplies by its weight matrix \\(\\mathbf{W}\\), and adds bias vector \\(\\mathbf{b}\\):\n\\[\nf(\\mathbf{X}) = \\mathbf{X}\\mathbf{W} + \\mathbf{b}\n\\]\nIn sequence models, this type of operation is still present; it’s just that now, it is executed for every time step – i.e., every position in the sequence – in isolation. But this means that now, the relationship between time steps has to be taken care of. To that end, the module takes what was obtained at the previous time step, applies a different weight matrix, and adds a different bias vector. This, in itself, is again an affine transformation, – just not of the input, but of what is called the previous state. The outputs from both affine computations are added, and the result then serves as prior state to the computation due at the next time step.\nIn other words, at each time step, two types of information are combined: the (weight-transformed) input for the current time step, and the (weight-transformed) state that resulted from processing the previous one. In math:\n\\[\nstate_{(t)} = f(\\mathbf{W_{input}}\\mathbf{X_{(t)}} + \\mathbf{b_{input}} + \\mathbf{W_{state}}\\mathbf{X_{(t-1)}} + \\mathbf{b_{state}})\n\\]\nThis logic specifies a recurrence relation, and modules implementing it are called recurrent neural networks (RNNs). In the next section, we’ll implement such a module ourselves; you’ll see how, in code, that recurrence maps to straightforward iteration. Before, two remarks.\nFirst, in the above formula, the function applied to the sum of the two transformations represents an activation; typically for RNNs, the default is the hyperbolic tangent, torch_tanh().\nSecond, in the official torch documentation, you’ll see the formula written this way:\n\\[\nh_{(t)} = f(\\mathbf{W_{i\\_h}\\mathbf{X_{(t)}} }+ \\mathbf{b_{i\\_h}} + \\mathbf{W_{h\\_h}\\mathbf{X}_{(t-1)}} + \\mathbf{b}_{h\\_h})\n\\]\nHere \\(h\\) stands for “hidden”, as in “hidden state”, and subscripts \\(i\\_h\\) and \\(h\\_h\\) stand for “input-to-hidden” and “hidden-to-hidden”, respectively. The reason I’d like to de-emphasize the “hidden” in “hidden state” is because in the torch implementation, the state is not necessarily hidden from the user. You’ll see what I mean below. (Pretty soon, I’ll give up resistance against the term, though, since it is ubiquitous in descriptive prose as well as code (for example, as regards variable naming). But I wanted to state this clearly at least once, so you won’t be confused when mapping your mental model of the algorithm to the behavior of torch RNNs."
  },
  {
    "objectID": "time_series.html#a-basic-recurrent-neural-network",
    "href": "time_series.html#a-basic-recurrent-neural-network",
    "title": "21  Time series",
    "section": "21.2 A basic recurrent neural network",
    "text": "21.2 A basic recurrent neural network\nIn the above discussion, we identified two basic things a recurrent neural network has to do: (1) iterate over the input sequence; and (2) execute the “business logic” of a sequence, that is, combine information from the previous as well as the current time step.\nCommonly, these duties are divided between two different objects. One, referred to as the “cell”, implements the logic. The other takes care of the iteration. The reason for this modularization is that both “inner” and “outer” logic should be modifiable independently. For example, you might want to keep the way you iterate over time steps, but modify what happens at each point in the iteration. This will get more concrete later, when we talk about the most-used sub-types of RNNs.\nIn our basic implementation of a basic RNN, both cell and iteration handler are nn_module()s. First, we have the cell.\n\n21.2.1 Basic rnn_cell()\nThe logic combines two affine operations; and affine operations are just what linear modules are for. We therefore just have our cell delegate to two linear modules, append the respective outputs, and apply a tanh activation.\nAs already alluded to above, in naming the modules and parameters, I’m following the torch conventions, so things will sound familiar when we move on to actual torch modules. Most notably, this includes referring to the state as “hidden state”, and thus, to its dimensionality as hidden_size, even though state is hidden from the user only under certain circumstances (which we’ll come to).\n\nlibrary(torch)\nlibrary(zeallot) # for destructuring using %&lt;-%\n\nrnn_cell &lt;- nn_module(\n  initialize = function(input_size, hidden_size) {\n    self$linear_i_h &lt;- nn_linear(input_size, hidden_size)\n    self$linear_h_h &lt;- nn_linear(hidden_size, hidden_size)\n  },\n  forward = function(x, prev_state) {\n    torch_tanh(self$linear_i_h(x) +\n      self$linear_h_h(prev_state))\n  }\n)\n\nFrom the way the cell has been defined, we see that to instantiate it, we need to pass hidden_size and input_size – the latter referring to the number of features in the dataset. Let’s make those 3 and 1, respectively:\n\ncell &lt;- rnn_cell(input_size = 1, hidden_size = 3)\n\nAs a quick test, we call the module on a (tiny) batch of data, passing in the previous (or: initial) state. As in the actual torch implementation, the state is initialized to an all-zeros tensor:\n\ncell(torch_randn(2, 1), torch_zeros(2, 3))\n\ntorch_tensor\n-0.6340  0.9571 -0.9886\n-0.3007  0.9201 -0.9689\n[ CPUFloatType{2,3} ][ grad_fn = &lt;TanhBackward0&gt; ]\nNote the dimensionality of the output. For each batch item, we get the new state, of size hidden_size.\nNow, a cell is not normally supposed to be called by the user; instead, we should call the to-be-defined rnn_module(). That module will take care of the iteration, delegating to an instance of rnn_cell() at each step. Let’s implement this one next.\n\n\n21.2.2 Basic rnn_module()\nConceptually, this module is easily characterized – it iterates over points in time. But there are a few things to note in the implementation that follows.\nFirst, note that it expects a single argument to forward(), not two – there is no need to pass in an initial state. (In actual torch implementations, the user can pass an initial state to start from, if they want. But if they don’t, the state will start out as all zeros, just like in this prototype.)\nSecond, and most importantly, let’s talk about the dimensionality of x, the single input argument. Where the cell operates on tensors of size batch_size times num_features, the iteration module expects its input to have an additional dimension, inserted at position two – right “in the middle”. You can see this in the line\n\nc(batch_size, timesteps, num_features) %&lt;-% x$size()\n\nThis additional dimension is used to capture evolution over time. rnn_module() will iterate over its values, call rnn_cell() for each step in the sequence, and keep track of the outputs:\n\nfor (t in 1:timesteps) {\n  new_state &lt;- self$cell(x[ , t, ], cur_state)\n  states[[t]] &lt;- new_state\n  cur_state &lt;- new_state\n}\n\nAs you see, in every call to self$cell, the previous state is passed as well, fulfilling the contract on rnn_cell$forward().\nThe complete code for rnn_module() is just slightly longer than that for the cell:\n\nrnn_module &lt;- nn_module(\n  initialize = function(input_size, hidden_size) {\n    self$cell &lt;- rnn_cell(input_size, hidden_size)\n    self$hidden_size &lt;- hidden_size\n  },\n  forward = function(x) {\n    c(batch_size, timesteps, num_features) %&lt;-% x$size()\n    init_hidden &lt;- torch_zeros(batch_size, self$hidden_size)\n    cur_state &lt;- init_hidden\n\n    # list containing the hidden states\n    # (equivalently: outputs), of length timesteps\n    states &lt;- vector(mode = \"list\", length = timesteps)\n\n    # loop over time steps\n    for (t in 1:timesteps) {\n      new_state &lt;- self$cell(x[, t, ], cur_state)\n      states[[t]] &lt;- new_state\n      cur_state &lt;- new_state\n    }\n\n    # put sequence of states in dimension 2\n    states &lt;- torch_stack(states, dim = 2)\n\n    list(states, states[, timesteps, ])\n  }\n)\n\nNote how dimension two, the one that, in the input, held the time dimension, now is used to pack the states obtained for each time step. I’ll say more about that in a second, but first, let’s test that module. I’ll stay with a state size (hidden_size) of three, and make our sample input have four consecutive measurements:\n\nrnn &lt;- rnn_module(input_size = 1, hidden_size = 3)\n\noutput &lt;- rnn(torch_randn(2, 4, 1))\noutput\n\n[[1]]\ntorch_tensor\n(1,.,.) = \n -0.9066  0.8149 -0.3671\n -0.9772  0.2903 -0.7938\n -0.9724  0.6242 -0.7877\n -0.9811  0.4164 -0.8839\n\n(2,.,.) = \n -0.8901  0.8795  0.3131\n -0.9512  0.4883  0.4991\n -0.9297  0.4875  0.1878\n -0.9420  0.5741  0.1564\n[ CPUFloatType{2,4,3} ][ grad_fn = &lt;StackBackward0&gt; ]\n\n[[2]]\ntorch_tensor\n-0.9811  0.4164 -0.8839\n-0.9420  0.5741  0.1564\n[ CPUFloatType{2,3} ][ grad_fn = &lt;SliceBackward0&gt; ]\nSo, rnn_module() returns a list of length two. First in the list is a tensor containing the states at all time steps – for each batch item, and for each unit in the state. At the risk of being redundant, here are its dimensions:\n\n# batch_size, timesteps, hidden_size\ndim(output[[1]])\n\n[1] 2 4 3\nThe reason I’m stressing this is that you’ll see the same convention reappear in the actual torch implementation, and the conventions associated with processing time series data can take some time to get accustomed to.\nNow, what about the second tensor? It really is a slice of the first – one reflecting the final state only. Correspondingly, its number of dimensions is reduced by one:\n\n# batch_size, hidden_size\ndim(output[[2]])\n\n[1] 2 3\nNow, as users, why would we need that second tensor?\nWe don’t. We can just do the slicing ourselves. Remember how, above, I first tried to avoid the term “hidden states”, and said I’d rather just talk about “states” instead? This is why: Whether the states really are hidden is up to implementation, that is, developer choice. A framework could decide to return the very last state only, unless the caller explicitly asks for the preceding ones. In that case, it would make sense to talk about “the” output on the one hand, and the sequence of “hidden states”, on the other. We could have coded our sample implementation like that. Instead, we were following torch’s nn_rnn(), which you’ll encounter in a second.\nThus, what I’m saying is: It all is a matter of conventions. But this doesn’t explain why the torch developers chose to return an additional, sliced-to-the-last-time-step, tensor: This clearly seems redundant. Is it?\nWell, it often is. Whether it is or not depends, for one, on the type of RNN. If you use torch’s nn_rnn() (a “simple” RNN implementation not much employed in practice) or nn_gru() – creating a default Gated Recurrent Network, one of two “classical”, tried-and-true architectures – it will be. If, on the other hand, you ask torch for a setup where a single RNN module really is a composite of layers, and/or you use an LSTM (Long Short-Term Memory Network, the second “classic”), then one tensor will not be a subset of the other.\nAt this point, it definitely is time to look at those torch modules."
  },
  {
    "objectID": "time_series.html#recurrent-neural-networks-in-torch",
    "href": "time_series.html#recurrent-neural-networks-in-torch",
    "title": "21  Time series",
    "section": "21.3 Recurrent neural networks in torch",
    "text": "21.3 Recurrent neural networks in torch\nHere, first, is nn_rnn(), a more feature-rich, but similar-in-spirit to our prototype recurrent module. In practice, you’ll basically always use either nn_gru() or nn_lstm(), which is why we won’t spend much time on it. Especially, we don’t talk about optional arguments (yet), with two exceptions.\n\nrnn &lt;- nn_rnn(input_size = 1, \n              hidden_size = 3, \n              batch_first = TRUE,\n              num_layers = 1)\n\nBoth batch_first and num_layers arguments are optional. The latter, num_layers, allows for creating a stack of RNN modules instead of a single one; this is convenient because the user does not have to worry about how to correctly wire them together. The default, though, is 1: exactly what we’re passing in, above. The reason I’m specifying it explicitly is just so you know it exists, and aren’t confused by the module’s output. Namely, you’ll see that the second in the list of tensors returned by rnn$forward() has an additional dimension that indicates the layer.\nIn contrast, batch_first is not set to its default; and it’s essential to be aware of this. By default, the convention for RNNs differs from those for other modules; if we didn’t pass the argument, torch would expect the first dimension to be representing time steps, not batch items. In this book, we’ll always pass batch_first = TRUE.\nNow, calling that RNN on the same test tensor we used in our manual implementation above, and checking the dimensions of the output, we see:\n\noutput &lt;- rnn(torch_randn(2, 4, 1))\n\n# output\ndim(output[[1]]) # batch_size, timesteps, hidden_size\n\n# last hidden state (per layer)\ndim(output[[2]]) # num_layers, batch_size, hidden_size\n\n[1] 2 4 3\n[1] 1 2 3\nThe first tensor in the list has a shape that exactly matches what would be returned by our manual implementation.\nSemantically, the respective second tensors in the output lists match up as well, in that both of them zoom in on the final state. But torch, allowing for the chaining of several RNNs in a single module, returns the final state per layer. In the torch implementation, is that second tensor redundant? It is, in our example. But were we to create a multi-layer RNN, it would give us information not contained in the first tensor: namely, the last hidden state for each non-final layer."
  },
  {
    "objectID": "time_series.html#rnns-in-practice-gru-and-lstm",
    "href": "time_series.html#rnns-in-practice-gru-and-lstm",
    "title": "21  Time series",
    "section": "21.4 RNNs in practice: GRU and LSTM",
    "text": "21.4 RNNs in practice: GRU and LSTM\nBasic recurrent networks, as created by nn_rnn(), are nice for explanatory purposes, but hardly ever used in practice. The reason is that when you back-propagate through a long recurrence structure, gradients are likely to either “die” or get out of bounds. These are the so-called “vanishing gradient” and “exploding gradient” problems, respectively.\nAlready three decades before the compute- and big-data accelerated “era of deep learning”, an algorithmic solution had been found. Long Short-Term Memory Networks, described in Hochreiter and Schmidhuber (1997), enabled training on reasonably long sequences by introducing so-called gates that act as filters in various places of the state-threading calculation. Gated Recurrent Units, put forward (much more recently) in Cho et al. (2014), are similar in spirit, but a bit simpler. Together, both architectures dominate the space.\nWith these models introducing additional logic, we see how the division-of-labor strategy introduced above is useful: Iteration and state threading are taken care of by different modules. This means that, in principle, we could design our own LSTM or GRU cell, and then, iterate over it in the same fashion as before. Of course, there is no need to re-implement existing functionality. But following the same modularization approach, we can nicely experiment with variations to the processing logic if we want.\nNow, let’s see what is returned by nn_gru() and nn_lstm(), the constructors corresponding to the aforementioned architectures. At this point, I should quickly comment on optional arguments I haven’t mentioned before.\nIn general, the argument list is the same for nn_rnn(), nn_gru(), and nn_lstm().\nWe have to indicate the number of features (input_size) and the size of the state (hidden_size); we deliberately pass in batch_first = TRUE; we can have torch chain several RNNs together using num_layers. In case we do want to stack layers, we can drop out a fraction of interconnections using, well, dropout. Finally, there is bidirectional. By default, this argument is set to FALSE, meaning we’re passing through the sequence chronologically. With bidirectional = TRUE, there is an additional pass in reverse order, and weights from both passes are combined. Essentially, what we’re doing is predicting present from past as well as past from present. This may sound like “cheating”, but really, is not; it’s just making optimal use of dependencies in past data.\nTo keep our examples in sync, I’ll now instantiate the GRU and LSTM modules in the same way as nn_rnn() above, making use of single layer and a single direction.\nFirst, a GRU:\n\ngru &lt;- nn_gru(\n  input_size = 1, \n  hidden_size = 3, \n  batch_first = TRUE,\n  num_layers = 1\n)\n\noutput &lt;- gru(torch_randn(2, 4, 1))\n\n# output\ndim(output[[1]]) # batch_size, timesteps, hidden_size\n\n# last hidden state (per layer)\ndim(output[[2]]) # num_layers, batch_size, hidden_size\n\n[1] 2 4 3\n[1] 1 2 3\nAs you see, dimension-wise, the output returned from a GRU is analogous to that of a simple RNN.\nWith LSTM, however, we see a difference:\n\nlstm &lt;- nn_lstm(\n  input_size = 1,\n  hidden_size = 3,\n  batch_first = TRUE\n)\n\noutput &lt;- lstm(torch_randn(2, 4, 1))\n\n# output\ndim(output[[1]]) # batch_size, timesteps, hidden_size\n\n# last hidden state (per layer)\ndim(output[[2]][[1]]) # num_layers, batch_size, hidden_size\n\n# last cell state (per layer)\ndim(output[[2]][[2]]) # num_layers, batch_size, hidden_size\n\n[1] 2 4 3\n[1] 1 2 3\n[1] 1 2 3\nInstead of two, we now have three tensors. The first and second are no different from what we’ve seen so far; meaning, the second is as redundant as for a GRU or a simple RNN. (At least when there’s just a single layer). What about the third? Shape-wise, it looks like the second, the one we know returns the “hidden state”. In fact, it reflects an additional state, one not present in a GRU. And this one – often called cell state – really is available to the user only for the last time step, even for single-layer LSTMs.\nYou could say that with LSTMs, some hidden states are more hidden than others.\nNow that we’ve gained some familiarity with torch’s RNN-related conventions, we look at an actual time series application."
  },
  {
    "objectID": "time_series.html#forecasting-electricity-demand",
    "href": "time_series.html#forecasting-electricity-demand",
    "title": "21  Time series",
    "section": "21.5 Forecasting electricity demand",
    "text": "21.5 Forecasting electricity demand\nOur example time series, called vic_elec, is available from package tsibbledata. It reflects aggregated electricity demand for Victoria, Australia, measured in half-hour intervals. Additional features (which we won’t use here) include temperature and a holiday indicator. The dataset spans three years, ranging from January, 2012 to December, 2014.\nBefore we start, we need to define our task. In fact, there’ll be two of them. In both, we’ll attempt to predict future temperature based on past measurements. First, we’ll see how to predict the very next measurement; in terms of measurement intervals, that’s a single time step ahead. Then, we’ll modify the code to allow for forecasting several time steps in advance.\n\n21.5.1 Data inspection\nThe dataset being part of an ecosystem of packages dedicated to time series analysis, there is not much to be done in terms of pre-processing. However, as always or even more so, it is worth our while to take time for data exploration.\n\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(tibble)\nlibrary(ggplot2)\nlibrary(lubridate)\n\n# Tidy Temporal Data Frames and Tools\nlibrary(tsibble) \n# Feature Extraction and Statistics for Time Series\nlibrary(feasts) \n# Diverse Datasets for 'tsibble'\nlibrary(tsibbledata) \n\nlibrary(torch)\nlibrary(luz)\n\nvic_elec\n\n# A tsibble: 52,608 x 5 [30m] &lt;Australia/Melbourne&gt;\n   Time                Demand Temperature Date       Holiday\n   &lt;dttm&gt;               &lt;dbl&gt;       &lt;dbl&gt; &lt;date&gt;     &lt;lgl&gt;  \n 1 2012-01-01 00:00:00  4383.        21.4 2012-01-01 TRUE   \n 2 2012-01-01 00:30:00  4263.        21.0 2012-01-01 TRUE   \n 3 2012-01-01 01:00:00  4049.        20.7 2012-01-01 TRUE   \n 4 2012-01-01 01:30:00  3878.        20.6 2012-01-01 TRUE   \n 5 2012-01-01 02:00:00  4036.        20.4 2012-01-01 TRUE   \n 6 2012-01-01 02:30:00  3866.        20.2 2012-01-01 TRUE   \n 7 2012-01-01 03:00:00  3694.        20.1 2012-01-01 TRUE   \n 8 2012-01-01 03:30:00  3562.        19.6 2012-01-01 TRUE   \n 9 2012-01-01 04:00:00  3433.        19.1 2012-01-01 TRUE   \n10 2012-01-01 04:30:00  3359.        19.0 2012-01-01 TRUE   \n# … with 52,598 more rows\nConcretely, we’ll want to know what kinds of periodicities there are in the data. Conveniently, we can obtain a decomposition into trend, various seasonal components, and a remainder using feasts::STL(). Here’s what we see for a single year (fig. 21.1):\n\ndecomp &lt;- vic_elec %&gt;% \n  filter(year(Date) == 2012) %&gt;%\n  model(STL(Demand)) %&gt;% \n  components()\n\ndecomp %&gt;% autoplot()\n\n\n\n\nFigure 21.1: One year of electricity demand, decomposed into trend, seasonal components, and remainder.\n\n\nIn this plot, the scale bar on the left immediately signals a component’s importance: the smaller the bar, the more dominant the effect. Not surprisingly, day of week matters; so does time of day.\nFor greater granularity, we zoom in on a single month (fig. 21.2).\n\ndecomp &lt;- vic_elec %&gt;% \n  filter(year(Date) == 2012, month(Date) == 1) %&gt;%\n  model(STL(Demand)) %&gt;% \n  components()\n\ndecomp %&gt;% autoplot()\n\n\n\n\nFigure 21.2: A single month of electricity demand, decomposed into trend, seasonal components, and remainder.\n\n\nHere, I’ve picked January, right in the hot Australian summer. Demand for electricity, then, arises as a need for cooling, not heating. We clearly see how it’s highest around noon, and lowest during the night. Every week, it peaks on Mondays and Tuesdays, declines on Wednesdays, and is more or less stable in the second half of the week.\nNow, while those two periodicities are important, the half-hourly rhythm clearly isn’t. For training the network, I’ll thus aggregate pairs of adjacent values, reducing the number of measurements to a half. Since now, changes between consecutive values are bigger, this also makes the task harder.\n\n\n21.5.2 Forecasting the very next value\nWe start with the more modest goal: predicting the very next data point. First of all, we need a custom torch::dataset().\n\n21.5.2.1 A dataset() for single-step prediction\nIn our demonstration of how torch RNNs work, we made use of a toy data tensor that looked like this: torch_randn(2, 4, 1).\nHere, the first dimension corresponded to batch items; the second, to time steps; and the third, to features. The batch dimension will be taken care of by the dataloader(). Thus, for the dataset(), the task to return a single predictor reduces to picking just as many consecutive measurements as desired, and stacking them in the first dimension. The corresponding target, in this first setup, should be the single measurement right after the last step in the predictor sequence.\nSo in principle, .getitem(), when asked for the item at position i, should return the following:\n\nlist(\n  x = self$x[i:(i + self$n_timesteps - 1)],\n  y = self$x[self$n_timesteps + i]\n)\n\nThe code you’ll find below is a variation on this theme. Depending on the time series in question, stacking consecutive measurements like this may result in individual batches looking hardly any different from each other. To avoid redundancy, we could do the following: Instead of iterating over the series in order, we take samples. At construction time, the torch::dataset() is told what fraction of the data we’d like to be served in each epoch, and prepares a set of indices. Then at runtime, it iterates over those indices. In the present case, sampling is not really needed, since anyway we’ve decided on hourly aggregation, and the time series itself isn’t excessively long to start with. The reason I’m showing this technique is that you might find it useful in other applications.\nNow, take a look at how we initialize the dataset(). If it is supposed to stack consecutive measurements, it has to be told the desired number of time steps. For easy experimentation, we make that parameter (called n_timesteps) a constructor argument.\nHere is the complete dataset() code:\n\ndemand_dataset &lt;- dataset(\n  name = \"demand_dataset\",\n  initialize = function(x, n_timesteps, sample_frac = 1) {\n    self$n_timesteps &lt;- n_timesteps\n    self$x &lt;- torch_tensor((x - train_mean) / train_sd)\n\n    n &lt;- length(self$x) - self$n_timesteps\n\n    self$starts &lt;- sort(sample.int(\n      n = n,\n      size = n * sample_frac\n    ))\n  },\n  .getitem = function(i) {\n    start &lt;- self$starts[i]\n    end &lt;- start + self$n_timesteps - 1\n\n    list(\n      x = self$x[start:end],\n      y = self$x[end + 1]\n    )\n  },\n  .length = function() {\n    length(self$starts)\n  }\n)\n\nGiven that vic_elec holds three years of data, a by-year split into training, validation, and test sets seems to suggest itself. Here, we perform the split, right after aggregating by full hour:\n\ndemand_hourly &lt;- vic_elec %&gt;%\n  index_by(Hour = floor_date(Time, \"hour\")) %&gt;%\n  summarise(\n    Demand = sum(Demand))\n\ndemand_train &lt;- demand_hourly %&gt;% \n  filter(year(Hour) == 2012) %&gt;%\n  as_tibble() %&gt;%\n  select(Demand) %&gt;%\n  as.matrix()\n\ndemand_valid &lt;- demand_hourly %&gt;% \n  filter(year(Hour) == 2013) %&gt;%\n  as_tibble() %&gt;%\n  select(Demand) %&gt;%\n  as.matrix()\n\ndemand_test &lt;- demand_hourly %&gt;% \n  filter(year(Hour) == 2014) %&gt;%\n  as_tibble() %&gt;%\n  select(Demand) %&gt;%\n  as.matrix()\n\nIn the dataset() definition, you may have noticed the use of train_mean and train_sd to standardize the data. Time to compute them:\n\ntrain_mean &lt;- mean(demand_train)\ntrain_sd &lt;- sd(demand_train)\n\nNow, we’re all set to construct the dataset() objects – but for one decision: What is a good value for n_timesteps? As with every hyper-parameter, in the end, nothing can beat experimentation. But based on data exploration, we can establish a lower bound: We definitely want to capture variation over day and week. With the hourly-aggregated data, this gives us a minimum length of 168:\n\nn_timesteps &lt;- 7 * 24\n\nNow, we instantiate the dataset() objects, and immediately check tensor shapes:\n\ntrain_ds &lt;- demand_dataset(demand_train, n_timesteps)\nvalid_ds &lt;- demand_dataset(demand_valid, n_timesteps)\ntest_ds &lt;- demand_dataset(demand_test, n_timesteps)\n\ndim(train_ds[1]$x)\ndim(train_ds[1]$y)\n\n[1] 168   1\n[1] 1\nNext, the respective dataloader()s:\n\nbatch_size &lt;- 128\n\ntrain_dl &lt;- train_ds %&gt;%\n  dataloader(batch_size = batch_size, shuffle = TRUE)\nvalid_dl &lt;- valid_ds %&gt;%\n  dataloader(batch_size = batch_size)\ntest_dl &lt;- test_ds %&gt;%\n  dataloader(batch_size = length(test_ds))\n\nb &lt;- train_dl %&gt;%\n  dataloader_make_iter() %&gt;%\n  dataloader_next()\n\ndim(b$x)\ndim(b$y)\n\n[1] 128 168   1\n[1] 128   1\nOnce we’re dealing with batches we do see the required three-dimensional structure of the input tensor.\n\n\n21.5.2.2 Model\nIn the model, the main workhorse is the LSTM. The number of LSTM-internal layers is configurable, the default being set to one. Its final output (or: “hidden state”; more on that below) is passed on to a linear module that outputs a single prediction.\n\nmodel &lt;- nn_module(\n  initialize = function(input_size,\n                        hidden_size,\n                        dropout = 0.2,\n                        num_layers = 1,\n                        rec_dropout = 0) {\n    self$num_layers &lt;- num_layers\n\n    self$rnn &lt;- nn_lstm(\n      input_size = input_size,\n      hidden_size = hidden_size,\n      num_layers = num_layers,\n      dropout = rec_dropout,\n      batch_first = TRUE\n    )\n\n    self$dropout &lt;- nn_dropout(dropout)\n    self$output &lt;- nn_linear(hidden_size, 1)\n  },\n  forward = function(x) {\n    (x %&gt;%\n      # these two are equivalent\n      # (1)\n      # take output tensor,restrict to last time step\n      self$rnn())[[1]][, dim(x)[2], ] %&gt;%\n      # (2)\n      # from list of state tensors,take the first,\n      # and pick the final layer\n      # self$rnn())[[2]][[1]][self$num_layers, , ] %&gt;%\n      self$dropout() %&gt;%\n      self$output()\n  }\n)\n\nNote two things about the top-level module’s definition. First, its constructor accepts two dropout-related arguments. The first, dropout, specifies the fraction of elements to be zeroed out on the path between LSTM and linear module; the second, rec_dropout, gets passed on to the LSTM, to be made use of between individual layers. (If there is just a single layer, this parameter has to equal zero, its default.)\nSecondly, in forward(), I’m showing two equivalent ways to pass output from the LSTM to the linear module. We can either ask torch for “the output”, and then, take the subset of values corresponding to the last time step only; alternatively, we can extract the “hidden state” tensor and zoom in on the final layer. Of course, since whatever choice we make is fine, there is no need to complicate the decision; I merely wanted to take the occasion to, one final time, illustrate how to “talk RNN” with torch.\n\n\n21.5.2.3 Training\nLike so often, we start the training process by running the learning rate finder. The model we’re training is modest in size (12,928 parameters overall!), but powerful – not in the least due to the stacking of LSTM layers.\n\ninput_size &lt;- 1\nhidden_size &lt;- 32\nnum_layers &lt;- 2\nrec_dropout &lt;- 0.2\n\nmodel &lt;- model %&gt;%\n  setup(optimizer = optim_adam, loss = nn_mse_loss()) %&gt;%\n  set_hparams(\n    input_size = input_size,\n    hidden_size = hidden_size,\n    num_layers = num_layers,\n    rec_dropout = rec_dropout\n  )\n\nrates_and_losses &lt;- model %&gt;% \n  lr_finder(train_dl, start_lr = 1e-3, end_lr = 1)\nrates_and_losses %&gt;% plot()\n\n\n\n\nFigure 21.3: Learning rate finder output for the one-step-ahead-forecast model.\n\n\nFrom the plot (fig. 21.3), a maximal learning rate of 0.1 seems like a sensible choice, at least when combined with the one-cycle learning rate scheduler. Indeed, we see fast progress (fig. 21.4):\n\nfitted &lt;- model %&gt;%\n  fit(train_dl, epochs = 50, valid_data = valid_dl,\n      callbacks = list(\n        luz_callback_early_stopping(patience = 3),\n        luz_callback_lr_scheduler(\n          lr_one_cycle,\n          max_lr = 0.1,\n          epochs = 50,\n          steps_per_epoch = length(train_dl),\n          call_on = \"on_batch_end\")\n      ),\n      verbose = TRUE)\n\nplot(fitted)\n\nEpoch 1/50\nTrain metrics: Loss: 0.3119\nValid metrics: Loss: 0.0715\nEpoch 2/50\nTrain metrics: Loss: 0.0767\nValid metrics: Loss: 0.0562\n...\n...\nEpoch 17/50\nTrain metrics: Loss: 0.0301\nValid metrics: Loss: 0.0295\nEpoch 18/50\nTrain metrics: Loss: 0.0288 \nValid metrics: Loss: 0.0263\nEarly stopping at epoch 18 of 50\n\n\n\nFigure 21.4: Fitting a one-step forecast model on vic_elec.\n\n\n\n\n21.5.2.4 Inspecting predictions\nUsing luz::evaluate(), we can check whether performance on the test set approximately matches that on the validation set. Here is mean squared error, as obtained from test-set predictions:\n\nevaluate(fitted, test_dl)\n\nA `luz_module_evaluation`\n── Results ───────────────────────────────────────────────\nloss: 0.0364\nBut really, what does this value tell us? We need to actually look at forecasts to decide whether predictions are any good.\nFor visualization, let’s choose a subset of the test data – the final month, say. We obtain predictions, and display them together with the actual measurements (fig. 21.5):\n\ndemand_viz &lt;- demand_hourly %&gt;%\n  filter(year(Hour) == 2014, month(Hour) == 12)\n\ndemand_viz_matrix &lt;- demand_viz %&gt;%\n  as_tibble() %&gt;%\n  select(Demand) %&gt;%\n  as.matrix()\n\nviz_ds &lt;- demand_dataset(demand_viz_matrix, n_timesteps)\nviz_dl &lt;- viz_ds %&gt;% dataloader(batch_size = length(viz_ds))\n\npreds &lt;- predict(fitted, viz_dl)\npreds &lt;- preds$to(device = \"cpu\") %&gt;% as.matrix()\npreds &lt;- c(rep(NA, n_timesteps), preds)\n\npred_ts &lt;- demand_viz %&gt;%\n  add_column(forecast = preds * train_sd + train_mean) %&gt;%\n  pivot_longer(-Hour) %&gt;%\n  update_tsibble(key = name)\n\npred_ts %&gt;%\n  autoplot() +\n  scale_colour_manual(values = c(\"#08c5d1\", \"#00353f\")) +\n  theme_minimal() +\n  theme(legend.position = \"None\")\n\n\n\n\nFigure 21.5: One-step ahead forecast on the last month of test set.\n\n\nPredictions look good! However, we know this was the simpler task of the two.\n\n\n\n21.5.3 Forecasting multiple time steps ahead\nIn reality, being able to predict a single measurement may not necessarily be of great use. Fortunately, not much adaptation is required to obtain multiple predictions from this type of model. In principle, we just have to modify the output size of the final linear layer. For performance reasons, we’ll go a bit further, but it’ll still be a moderate change.\nSince the target now is a sequence, we also have to adapt the dataset().\n\n21.5.3.1 Dataset() adaptation\nJust like the number of consecutive measurements in an input tensor, that of the target tensors should be configurable. We therefore introduce a new parameter, n_forecast, that indicates the desired length:\n\ndemand_dataset &lt;- dataset(\n  name = \"demand_dataset\",\n  initialize = function(x,\n                        n_timesteps,\n                        n_forecast,\n                        sample_frac = 1) {\n    self$n_timesteps &lt;- n_timesteps\n    self$n_forecast &lt;- n_forecast\n    self$x &lt;- torch_tensor((x - train_mean) / train_sd)\n\n    n &lt;- length(self$x) -\n      self$n_timesteps - self$n_forecast + 1\n\n    self$starts &lt;- sort(sample.int(\n      n = n,\n      size = n * sample_frac\n    ))\n  },\n  .getitem = function(i) {\n    start &lt;- self$starts[i]\n    end &lt;- start + self$n_timesteps - 1\n\n    list(\n      x = self$x[start:end],\n      y = self$x[(end + 1):(end + self$n_forecast)]$\n        squeeze(2)\n    )\n  },\n  .length = function() {\n    length(self$starts)\n  }\n)\n\nWe’ll attempt to forecast demand for the subsequent seven days. Thus, n_timesteps and n_forecast are equal; but that’s a coincidence, not a requirement.\n\nn_timesteps &lt;- 7 * 24\nn_forecast &lt;- 7 * 24\n\ntrain_ds &lt;- demand_dataset(\n  demand_train,\n  n_timesteps,\n  n_forecast,\n  sample_frac = 1\n)\nvalid_ds &lt;- demand_dataset(\n  demand_valid,\n  n_timesteps,\n  n_forecast,\n  sample_frac = 1\n)\ntest_ds &lt;- demand_dataset(\n  demand_test,\n  n_timesteps,\n  n_forecast\n)\n\nbatch_size &lt;- 128\ntrain_dl &lt;- train_ds %&gt;%\n  dataloader(batch_size = batch_size, shuffle = TRUE)\nvalid_dl &lt;- valid_ds %&gt;%\n  dataloader(batch_size = batch_size)\ntest_dl &lt;- test_ds %&gt;%\n  dataloader(batch_size = length(test_ds))\n\n\n\n21.5.3.2 Model adaptation\nIn the model, we replace the final linear layer by a tiny multi-layer perceptron. It will return n_forecast predictions for each time step.\n\nmodel &lt;- nn_module(\n  initialize = function(input_size,\n                        hidden_size,\n                        linear_size,\n                        output_size,\n                        dropout = 0.2,\n                        num_layers = 1,\n                        rec_dropout = 0) {\n    self$num_layers &lt;- num_layers\n\n    self$rnn &lt;- nn_lstm(\n      input_size = input_size,\n      hidden_size = hidden_size,\n      num_layers = num_layers,\n      dropout = rec_dropout,\n      batch_first = TRUE\n    )\n\n    self$dropout &lt;- nn_dropout(dropout)\n    self$mlp &lt;- nn_sequential(\n      nn_linear(hidden_size, linear_size),\n      nn_relu(),\n      nn_dropout(dropout),\n      nn_linear(linear_size, output_size)\n    )\n  },\n  forward = function(x) {\n    x &lt;- self$rnn(x)[[2]][[1]][self$num_layers, , ] %&gt;%\n      self$mlp()\n  }\n)\n\n\n\n21.5.3.3 Training\nNot surprisingly, the training process is unaltered. It again starts with running the learning rate finder (fig. 21.6):\n\ninput_size &lt;- 1\nhidden_size &lt;- 32\nlinear_size &lt;- 512\ndropout &lt;- 0.5\nnum_layers &lt;- 2\nrec_dropout &lt;- 0.2\n\nmodel &lt;- model %&gt;%\n  setup(optimizer = optim_adam, loss = nn_mse_loss()) %&gt;%\n  set_hparams(\n    input_size = input_size,\n    hidden_size = hidden_size,\n    linear_size = linear_size,\n    output_size = n_forecast,\n    num_layers = num_layers,\n    rec_dropout = rec_dropout\n  )\n\nrates_and_losses &lt;- model %&gt;% lr_finder(\n  train_dl,\n  start_lr = 1e-4,\n  end_lr = 0.5\n)\nrates_and_losses %&gt;% plot()\n\n\n\n\nFigure 21.6: Learning rate finder output for multiple-step prediction.\n\n\nBased on that result, I’ll go with a lower maximal rate this time.\n\nfitted &lt;- model %&gt;%\n  fit(train_dl, epochs = 100, valid_data = valid_dl,\n      callbacks = list(\n        luz_callback_early_stopping(patience = 3),\n        luz_callback_lr_scheduler(\n          lr_one_cycle,\n          max_lr = 0.01,\n          epochs = 100,\n          steps_per_epoch = length(train_dl),\n          call_on = \"on_batch_end\")\n      ),\n      verbose = TRUE)\n\nplot(fitted)\n\nEpoch 1/100\nTrain metrics: Loss: 0.9639                             \nValid metrics: Loss: 0.9714\nEpoch 2/100\nTrain metrics: Loss: 0.823                              \nValid metrics: Loss: 0.7729\n...\n...\nEpoch 21/100\nTrain metrics: Loss: 0.3833                             \nValid metrics: Loss: 0.4585\nEpoch 22/100\nTrain metrics: Loss: 0.3796                             \nValid metrics: Loss: 0.4404\n...\n...\nEpoch 41/100\nTrain metrics: Loss: 0.3103                             \nValid metrics: Loss: 0.3677\nEpoch 42/100\nTrain metrics: Loss: 0.3089                             \nValid metrics: Loss: 0.3646\n...\n...\nEpoch 60/100\nTrain metrics: Loss: 0.2707                             \nValid metrics: Loss: 0.3337\nEpoch 61/100\nTrain metrics: Loss: 0.2617                             \nValid metrics: Loss: 0.3317\nEarly stopping at epoch 61 of 100\n\n\n\nFigure 21.7: Fitting a multiple-step forecast model on vic_elec.\n\n\nThe loss curves (fig. 21.7) look decent; how about actual forecasts?\n\n\n21.5.3.4 Inspecting predictions\nAs before, let’s first formally check that performance on the test set does not differ too much from that on the validation set.\n\nevaluate(fitted, test_dl)\n\nA `luz_module_evaluation`\n── Results ───────────────────────────────────────────────\nloss: 0.3782\nFor visualization, we again look at the very last month only.\n\ndemand_viz &lt;- demand_hourly %&gt;%\n  filter(year(Hour) == 2014, month(Hour) == 12)\n\ndemand_viz_matrix &lt;- demand_viz %&gt;%\n  as_tibble() %&gt;%\n  select(Demand) %&gt;%\n  as.matrix()\n\nn_obs &lt;- nrow(demand_viz_matrix)\n\nviz_ds &lt;- demand_dataset(\n  demand_viz_matrix,\n  n_timesteps,\n  n_forecast\n)\nviz_dl &lt;- viz_ds %&gt;%\n  dataloader(batch_size = length(viz_ds))\n\npreds &lt;- predict(fitted, viz_dl)\npreds &lt;- preds$to(device = \"cpu\") %&gt;%\n  as.matrix()\n\nWith each forecast now covering a week of measurements, we can’t display them all in the same plot. What we can do is pick a few sample indices, nicely spread out over the month, and plot those (fig. 21.8):\n\nexample_preds &lt;- vector(mode = \"list\", length = 3)\nexample_indices &lt;- c(1, 201, 401)\n\nfor (i in seq_along(example_indices)) {\n  cur_obs &lt;- example_indices[i]\n  example_preds[[i]] &lt;- c(\n    rep(NA, n_timesteps + cur_obs - 1),\n    preds[cur_obs, ],\n    rep(\n      NA,\n      n_obs - cur_obs + 1 - n_timesteps - n_forecast\n    )\n  )\n}\n\npred_ts &lt;- demand_viz %&gt;%\n  select(Demand) %&gt;%\n  add_column(\n    p1 = example_preds[[1]] * train_sd + train_mean,\n    p2 = example_preds[[2]] * train_sd + train_mean,\n    p3 = example_preds[[3]] * train_sd + train_mean) %&gt;%\n  pivot_longer(-Hour) %&gt;%\n  update_tsibble(key = name)\n\npred_ts %&gt;%\n  autoplot() +\n  scale_colour_manual(\n    values = c(\n      \"#08c5d1\", \"#00353f\", \"#ffbf66\", \"#d46f4d\"\n    )\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"None\")\n\n\n\n\nFigure 21.8: A sample of week-long forecasts on the last month of test set.\n\n\nIt is instructive to take a closer look. The daily and weekly rhythms are present; very much so in fact. What’s harder to devine, from the model’s point of view, are the mini-trends we see developing over the month. Without any external “cues”, how should it know that next week, demand for electricity will raise over (or fall below) the current level? To significantly improve predictions, we would have to incorporate additional data – for example, temperature forecasts. Since no book can cover everything, we won’t pursue that topic here; instead, we’ll move on to our last deep learning application: classifying speech utterances.\n\n\n\n\nCho, Kyunghyun, Bart van Merrienboer, Çaglar Gülçehre, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. 2014. “Learning Phrase Representations Using RNN Encoder-Decoder for Statistical Machine Translation.” CoRR abs/1406.1078. http://arxiv.org/abs/1406.1078.\n\n\nHochreiter, Sepp, and Jürgen Schmidhuber. 1997. “Long Short-Term Memory.” Neural Computation 9 (8): 1735–80."
  },
  {
    "objectID": "audio_classification.html#classifying-speech-data",
    "href": "audio_classification.html#classifying-speech-data",
    "title": "22  Audio classification",
    "section": "22.1 Classifying speech data",
    "text": "22.1 Classifying speech data\nThe speech command dataset (Warden (2018)) comes with torchaudio, a package that does for auditory data what torchvision does for images and video. As of this writing, there are two versions; the one provided by torchaudio is number one. In that version, the dataset holds recordings of thirty different, one- or two-syllable words, uttered by different speakers; there are about 65,000 audio files overall. The task is to predict, from the audio recording, which word was spoken.\nTo see what is involved, we download and inspect the data.\n\nlibrary(torch)\nlibrary(torchaudio)\nlibrary(luz)\n\nds &lt;- speechcommand_dataset(\n  root = \"~/.torch-datasets\", \n  url = \"speech_commands_v0.01\",\n  download = TRUE\n)\n\nds$classes\n\n[1]  \"bed\"    \"bird\"   \"cat\"    \"dog\"    \"down\"   \"eight\"\n[7]  \"five\"   \"four\"   \"go\"     \"happy\"  \"house\"  \"left\"\n[32] \" marvin\" \"nine\"   \"no\"     \"off\"    \"on\"     \"one\"\n[19] \"right\"  \"seven\" \"sheila\" \"six\"    \"stop\"   \"three\"\n[25]  \"tree\"   \"two\"    \"up\"     \"wow\"    \"yes\"    \"zero\" \nPicking a sample at random, we see that the information we’ll need is contained in four properties: waveform, sample_rate, label_index, and label.\nThe first, waveform, will be our predictor.\n\nsample &lt;- ds[2000]\ndim(sample$waveform)\n\n[1]     1 16000\nIndividual tensor values are centered at zero, and range between -1 and 1. There are 16,000 of them, reflecting the fact that the recording lasted for one second, and was registered at (or has been converted to, by the dataset creators) a rate of 16,000 samples per second. The latter information is stored in sample$sample_rate:\n\nsample$sample_rate\n\n[1] 16000\nAll recordings have been sampled at the same rate. Their length almost always equals one second; the – very – few ones that are minimally longer we can safely truncate.\nFinally, the target is stored, in integer form, in sample$label_index, with the corresponding word available from sample$label:\n\nsample$label\nsample$label_index\n\n[1] \"bird\"\ntorch_tensor\n2\n[ CPULongType{} ]\nHow does this audio signal “look” (fig. 22.1)?\n\nlibrary(ggplot2)\n\ndf &lt;- data.frame(\n  x = 1:length(sample$waveform[1]),\n  y = as.numeric(sample$waveform[1])\n  )\n\nggplot(df, aes(x = x, y = y)) +\n  geom_line(size = 0.3) +\n  ggtitle(\n    paste0(\n      \"The spoken word \\\"\", sample$label, \"\\\": Sound wave\"\n    )\n  ) +\n  xlab(\"time\") +\n  ylab(\"amplitude\") +\n  theme_minimal()\n\n\n\n\nFigure 22.1: The spoken word “bird”, in time-domain representation.\n\n\nWhat we see is a sequence of amplitudes, reflecting the sound wave produced by someone saying “bird”. Put differently, we have here a time series of “loudness values”. Even for experts, guessing which word resulted in those amplitudes is an impossible task. This is where domain knowledge is relevant. The expert may not be able to make much of the signal in this representation; but they may know a way to more meaningfully represent it.\nAt this point, you may be thinking: Right; but just because the task is impossible for human beings it need not be impossible for a machine! After all, a neural network and a person process information very differently. Maybe an RNN, trained on these waves, can learn to correctly map them to a set of words!\nThat could indeed be – you may want to try – but it turns out there is a better way, one that both appeals to our, human, desire for understanding and uses deep learning in a most beneficient way.\nThis better way is owed to a mathematical fact that – for me, at least – never ceases to inspire awe and wonder."
  },
  {
    "objectID": "audio_classification.html#two-equivalent-representations",
    "href": "audio_classification.html#two-equivalent-representations",
    "title": "22  Audio classification",
    "section": "22.2 Two equivalent representations",
    "text": "22.2 Two equivalent representations\nImagine that instead of as a sequence of amplitudes over time, the above wave were represented in a way that had no information about time at all. Next, imagine we took that representation and tried to recover the original signal. For that to be possible, the new representation would somehow have to contain “just as much” information as the wave we started from. That “just as much” is obtained by the Fourier Transform, and it consists of the magnitudes and phase shifts of the different frequencies that make up the signal. In part three, we’ll play around quite a bit with the Fourier Transform, so here I’ll keep the introduction brief. Instead, I’ll focus on the elegant symbiosis that will result, once we’ve arrived at the final pre-processing step. But we’re not quite there yet.\nTo start, how does the Fourier-transformed version of the “bird” sound wave look? We obtain it by calling torch_fft_fft() (where fft stands for Fast Fourier Transform):\n\ndft &lt;- torch_fft_fft(sample$waveform)\ndim(dft)\n\n[1]     1 16000\nThe length of this tensor is the same; however, its values are not in chronological order. Instead, they represent the Fourier coefficients, corresponding to the frequencies contained in the signal. The higher their magnitude, the more they contribute to the signal (fig. 22.2):\n\nmag &lt;- torch_abs(dft[1, ])\n\ndf &lt;- data.frame(\n  x = 1:(length(sample$waveform[1]) / 2),\n  y = as.numeric(mag[1:8000])\n)\n\nggplot(df, aes(x = x, y = y)) +\n  geom_line(size = 0.3) +\n  ggtitle(\n    paste0(\n      \"The spoken word \\\"\",\n      sample$label,\n      \"\\\": Discrete Fourier Transform\"\n    )\n  ) +\n  xlab(\"frequency\") +\n  ylab(\"magnitude\") +\n  theme_minimal()\n\n\n\n\nFigure 22.2: The spoken word “bird”, in frequency-domain representation.\n\n\nFrom this, alternate, representation, we could go back to the original sound wave by taking the frequencies present in the signal, weighting them according to their coefficients, and adding them up. (That opposite direction is called the Inverse Fourier Transform, and available in torch as torch_fft_ifft().)\nThis, in itself, is incredibly fascinating; but how does it help us with our task of classifying audio signals? Were we to work with the sound waves themselves, we’d feed them into an RNN. With frequencies, there is no recurrence relation; so RNNs are not an option. We could use a feed-forward neural network, then. But there is reason to expect this to work particularly well."
  },
  {
    "objectID": "audio_classification.html#combining-representations-the-spectrogram",
    "href": "audio_classification.html#combining-representations-the-spectrogram",
    "title": "22  Audio classification",
    "section": "22.3 Combining representations: The spectrogram",
    "text": "22.3 Combining representations: The spectrogram\nIn fact, what really would help us is a synthesis of both representations; some sort of “have your cake and eat it, too”. What if we could divide the signal into small chunks, and run the Fourier Transform on each of them? As you may have guessed from this leadup, this indeed is something we can do; and the representation it creates is called the spectrogram.\nWith a spectrogram, we still keep some time-domain information – some, since there is an unavoidable loss in granularity. On the other hand, for each of the time segments, we learn about their spectral composition. There’s an important point to be made, though. The resolutions we get in time versus in frequency, respectively, are inversely related. If we split up the signals into many chunks (called “windows”), the frequency representation per window will not be very fine-grained. Conversely, if we want to get better resolution in the frequency domain, we have to choose longer windows, thus losing information about how spectral composition varies over time. It seems, then, that all we can do is eat half a cake, and take pleasure in the sight of the other half.\nWell, all said so far is correct; but as you’ll see, this is far less of a problem than it may seem now.\nBefore we unveil the mystery, though, let’s create and inspect such a spectrogram for our example signal. In the following code snippet, the size of the – overlapping – windows is chosen so as to allow for reasonable granularity in both the time and the frequency domain. We’re left with sixty-three windows, and, for each window, obtain two hundred fifty-seven coefficients:\n\nfft_size &lt;- 512\nwindow_size &lt;- 512\npower &lt;- 0.5\n\nspectrogram &lt;- transform_spectrogram(\n  n_fft = fft_size,\n  win_length = window_size,\n  normalized = TRUE,\n  power = power\n)\n\nspec &lt;- spectrogram(sample$waveform)$squeeze()\ndim(spec)\n\n[1]   257 63\nWe can display the spectrogram visually (fig. 22.3):\n\nbins &lt;- 1:dim(spec)[1]\nfreqs &lt;- bins / (fft_size / 2 + 1) * sample$sample_rate \nlog_freqs &lt;- log10(freqs)\n\nframes &lt;- 1:(dim(spec)[2])\nseconds &lt;- (frames / dim(spec)[2]) *\n  (dim(sample$waveform$squeeze())[1] / sample$sample_rate)\n\nimage(x = as.numeric(seconds),\n      y = log_freqs,\n      z = t(as.matrix(spec)),\n      ylab = 'log frequency [Hz]',\n      xlab = 'time [s]',\n      col = hcl.colors(12, palette = \"Light grays\")\n)\nmain &lt;- paste0(\"Spectrogram, window size = \", window_size)\nsub &lt;- \"Magnitude (square root)\"\nmtext(side = 3, line = 2, at = 0, adj = 0, cex = 1.3, main)\nmtext(side = 3, line = 1, at = 0, adj = 0, cex = 1, sub)\n\n\n\n\nFigure 22.3: The spoken word “bird”: Spectrogram.\n\n\nWe know that we’ve lost some resolution, in both time and frequency. By displaying the square root of the coefficients’ magnitudes, though – and thus, enhancing sensitivity – we were still able to obtain a reasonable result. (With the Light grays color scheme, brighter shades indicate higher-valued coefficients; darker ones, the opposite.)\nFinally, let’s get back to the crucial question. If this representation is, by necessity, a compromise – why, then, would we want to employ it? This is where we take the deep learning perspective. The spectrogram is a two-dimensional representation: an image. With images, we have access to a rich reservoir of techniques and architectures – among all areas deep learning has been successful in, image recognition still stands out. Soon, you’ll see that for this task, fancy architectures are not even needed; a straightforward convnet will do a very good job."
  },
  {
    "objectID": "audio_classification.html#training-a-model-for-audio-classification",
    "href": "audio_classification.html#training-a-model-for-audio-classification",
    "title": "22  Audio classification",
    "section": "22.4 Training a model for audio classification",
    "text": "22.4 Training a model for audio classification\nThe plan is as follows. We’ll end-to-end train a baseline model, which already will be performing very well. Nevertheless, we’ll try out two ideas, to see if we can improve on that baseline. Should it turn out not to be the case, we’ll still have learned about some important techniques.\n\n22.4.1 Baseline setup: Training a convnet on spectrograms\nWe start by creating a torch::dataset() that, starting from the original speechcommand_dataset(), computes a spectrogram for every sample.\n\nspectrogram_dataset &lt;- dataset(\n  inherit = speechcommand_dataset,\n  initialize = function(...,\n                        pad_to = 16000,\n                        sampling_rate = 16000,\n                        n_fft = 512,\n                        window_size_seconds = 0.03,\n                        window_stride_seconds = 0.01,\n                        # power = 2 is default for\n                        # transform_spectrogram()\n                        # we stay with the default for now,\n                        # but will make use of this option later\n                        power = 2,\n                        # this too will be explained later\n                        n_mels = 0) {\n    self$pad_to &lt;- pad_to\n    self$window_size_samples &lt;- sampling_rate *\n      window_size_seconds\n    self$window_stride_samples &lt;- sampling_rate *\n      window_stride_seconds\n    self$power &lt;- power\n    if (n_mels == 0) {\n      self$spectrogram &lt;- transform_spectrogram(\n        n_fft = n_fft,\n        win_length = self$window_size_samples,\n        hop_length = self$window_stride_samples,\n        normalized = TRUE,\n        power = self$power\n      )\n    } else {\n      self$spectrogram &lt;- transform_mel_spectrogram(\n        n_fft = n_fft,\n        win_length = self$window_size_samples,\n        hop_length = self$window_stride_samples,\n        normalized = TRUE,\n        power = self$power,\n        n_mels = n_mels\n      )\n    }\n    super$initialize(...)\n  },\n  .getitem = function(i) {\n    item &lt;- super$.getitem(i)\n\n    x &lt;- item$waveform\n    # make sure all samples have the same length (57)\n    # shorter ones will be padded,\n    # longer ones will be truncated\n    x &lt;- nnf_pad(x, pad = c(0, self$pad_to - dim(x)[2]))\n    x &lt;- x %&gt;% self$spectrogram()\n\n    if (is.null(self$power)) {\n      # there is an additional dimension now,\n      # in position 4,\n      # that we want to appear in front\n      # (as a second channel)\n      x &lt;- x$squeeze()$permute(c(3, 1, 2))\n    }\n\n    y &lt;- item$label_index\n    list(x = x, y = y)\n  }\n)\n\nAs always, we immediately check if all is well:\n\nds &lt;- spectrogram_dataset(\n  root = \"~/.torch-datasets\",\n  url = \"speech_commands_v0.01\",\n  download = TRUE\n)\n\ndim(ds[1]$x)\nds[1]$y\n\n[1]   1 257 101\ntorch_tensor\n1\n[ CPULongType{} ]\nNext, we split up the data, and instantiate the dataset() and dataloader() objects.\n\ntrain_ids &lt;- sample(\n  1:length(ds),\n  size = 0.6 * length(ds)\n)\nvalid_ids &lt;- sample(\n  setdiff(\n    1:length(ds),\n    train_ids\n  ),\n  size = 0.2 * length(ds)\n)\ntest_ids &lt;- setdiff(\n  1:length(ds),\n  union(train_ids, valid_ids)\n)\n\nbatch_size &lt;- 128\n\ntrain_ds &lt;- dataset_subset(ds, indices = train_ids)\ntrain_dl &lt;- dataloader(\n  train_ds,\n  batch_size = batch_size, shuffle = TRUE\n)\n\nvalid_ds &lt;- dataset_subset(ds, indices = valid_ids)\nvalid_dl &lt;- dataloader(\n  valid_ds,\n  batch_size = batch_size\n)\n\ntest_ds &lt;- dataset_subset(ds, indices = test_ids)\ntest_dl &lt;- dataloader(test_ds, batch_size = 64)\n\nb &lt;- train_dl %&gt;%\n  dataloader_make_iter() %&gt;%\n  dataloader_next()\n\ndim(b$x)\n\n[1] 128   1 257 101\nLike I said, the model is a straightforward convnet, with dropout and batch normalization.\n\nmodel &lt;- nn_module(\n  initialize = function() {\n    self$features &lt;- nn_sequential(\n      nn_conv2d(1, 32, kernel_size = 3),\n      nn_batch_norm2d(32),\n      nn_relu(),\n      nn_max_pool2d(kernel_size = 2),\n      nn_dropout2d(p = 0.2),\n      nn_conv2d(32, 64, kernel_size = 3),\n      nn_batch_norm2d(64),\n      nn_relu(),\n      nn_max_pool2d(kernel_size = 2),\n      nn_dropout2d(p = 0.2),\n      nn_conv2d(64, 128, kernel_size = 3),\n      nn_batch_norm2d(128),\n      nn_relu(),\n      nn_max_pool2d(kernel_size = 2),\n      nn_dropout2d(p = 0.2),\n      nn_conv2d(128, 256, kernel_size = 3),\n      nn_batch_norm2d(256),\n      nn_relu(),\n      nn_max_pool2d(kernel_size = 2),\n      nn_dropout2d(p = 0.2),\n      nn_conv2d(256, 512, kernel_size = 3),\n      nn_batch_norm2d(512),\n      nn_relu(),\n      nn_adaptive_avg_pool2d(c(1, 1)),\n      nn_dropout2d(p = 0.2)\n    )\n\n    self$classifier &lt;- nn_sequential(\n      nn_linear(512, 512),\n      nn_batch_norm1d(512),\n      nn_relu(),\n      nn_dropout(p = 0.5),\n      nn_linear(512, 30)\n    )\n  },\n  forward = function(x) {\n    x &lt;- self$features(x)$squeeze()\n    x &lt;- self$classifier(x)\n    x\n  }\n)\n\nWhat is a good learning rate to train this model (fig. 22.4)?\n\nmodel &lt;- model %&gt;%\n  setup(\n    loss = nn_cross_entropy_loss(),\n    optimizer = optim_adam,\n    metrics = list(luz_metric_accuracy())\n  )\n\nrates_and_losses &lt;- model %&gt;%\n  lr_finder(train_dl)\nrates_and_losses %&gt;% plot()\n\n\n\n\nFigure 22.4: Learning rate finder, run on the baseline model.\n\n\nBased on the plot, I decided to use 0.01 as a maximal learning rate.\n\nfitted &lt;- model %&gt;%\n  fit(train_dl,\n    epochs = 50, valid_data = valid_dl,\n    callbacks = list(\n      luz_callback_early_stopping(patience = 3),\n      luz_callback_lr_scheduler(\n        lr_one_cycle,\n        max_lr = 1e-2,\n        epochs = 50,\n        steps_per_epoch = length(train_dl),\n        call_on = \"on_batch_end\"\n      ),\n      luz_callback_model_checkpoint(path = \"models_baseline/\"),\n      luz_callback_csv_logger(\"logs_baseline.csv\")\n    ),\n    verbose = TRUE\n  )\n\nplot(fitted)\n\n\n\n\nFigure 22.5: Fitting the baseline model.\n\n\nModel training stopped after thirty-two epochs (fig. 22.5). Here is an excerpt from the logs:\n\"epoch\",\"set\",\"loss\",\"acc\"\n\n1,\"train\",3.34194613126368,0.0687577255871446\n1,\"valid\",3.08480389211692,0.137438195302843\n2,\"train\",2.87943273042542,0.161155747836836\n2,\"valid\",2.54789054393768,0.260275030902349\n3,\"train\",2.42715575726204,0.279048207663782\n3,\"valid\",2.08232365753136,0.417567985166873\n...\n...\n30,\"train\",0.397343056799929,0.880098887515451\n30,\"valid\",0.3777267414273,0.895395550061805\n31,\"train\",0.395276123743042,0.880073135558302\n31,\"valid\",0.375300966641482,0.895781829419036\n32,\"train\",0.387298851523524,0.880691182529872\n32,\"valid\",0.379925572989034,0.89323238566131\nWith thirty classes, an accuracy of about eighty-nine percent seems decent. We double-check on the test set:\n\nevaluate(fitted, test_dl)\n\nloss: 0.3776\nacc: 0.8898\nAn interesting question is which words get confused most often. (Of course, even more interesting is how error probabilities are related to features of the spectrograms – but this we have to leave to the true domain experts.)\nA nice way of displaying the confusion matrix is to create an alluvial plot (fig. 22.6). We see the predictions, on the left, “flow into” the target slots. (Target-prediction pairs less frequent than a thousandth of test set cardinality are hidden.)\n\npreds_baseline &lt;- predict(fitted, test_dl)\npreds_baseline &lt;-\n  torch_argmax(preds_baseline, dim = 2) %&gt;%\n  as.numeric()\n\ntest_dl &lt;- dataloader(\n  test_ds,\n  batch_size = length(test_ds)\n)\nb &lt;- test_dl %&gt;%\n  dataloader_make_iter() %&gt;%\n  dataloader_next()\ntargets_baseline &lt;- b$y$to(device = \"cpu\") %&gt;%\n  as.numeric()\n\ndf_baseline &lt;- data.frame(\n  preds = preds_baseline,\n  targets = targets_baseline\n)\n\nclasses &lt;- speechcommand_ds$classes\n\ndf &lt;- df_baseline %&gt;%\n  mutate(correct = preds == targets) %&gt;%\n  mutate(\n    preds = classes[preds],\n    targets = classes[targets]\n  ) %&gt;%\n  count(preds, targets, correct)\n\nlibrary(alluvial)\nalluvial(\n  df %&gt;% select(preds, targets),\n  freq = df$n,\n  col = ifelse(df$correct, \"#d1d1d1\", \"#aaaaaa\"),\n  border = ifelse(df$correct, \"#d1d1d1\", \"#aaaaaa\"),\n  hide = df$n &lt; nrow(df_baseline) / 1000\n)\n\n\n\n\nFigure 22.6: Alluvial plot, illustrating which categories were confused most often.\n\n\nThat’s it for the baseline approach. Let’s see if we can do still better.\n\n\n22.4.2 Variation one: Use a Mel-scale spectrogram instead\nIn classical speech recognition, people did not necessarily limit pre-processing to the Fourier Transform and spectrograms. Various other steps used to – or could – follow, some of which seem obsolete once neural networks are being used. There is at least one technique, though, where it’s hard to devine a priori whether it will help or not.\nDue to the way our hearing system is built, we human beings don’t perceive differences equally accurately across the frequency range. For example, while the distances between 440 Hz and 480 Hz on the one hand, and 8000 Hz and 8040 Hz on the other, are the same mathematically, the latter will be much harder to perceive. (This is no different with other modes of perception – to tell the difference between one kilogram and two kilograms is easy, while doing the same for fourteen versus fifteen kilograms is less so.\nTo accommodate this physiological precondition, one sometimes converts the Fourier coefficients to the so-called Mel scale. Various formulae exist that do this; in one or the other way, they always include taking the logarithm. But in practice, what usually is done is to create overlapping filters that aggregate sets of Fourier coefficients into a new representation, the Mel coefficients. In the low-frequency range, the filters are narrow, and subsume only very few Fourier coefficients. Then, they successively become wider, until, in the very-high frequency range, a wide range of Fourier coefficients get to contribute to a single Mel value.\nWe can make this more concrete using a torch helper function, functional_create_fb_matrix() . What this function does is create a conversion matrix from Fourier- to Mel space:\nc\n\nfb &lt;- functional_create_fb_matrix(\n  n_freqs = 257,\n  f_min = 0,\n  f_max = 8000,\n  n_mels = 16,\n  sample_rate = 16000\n)\ndim(fb)\n\n[1] 257  16\nIn our application, we’ll use Mel spectrograms with one hundred twenty-eight coefficients; given that we’re training a convnet, we wouldn’t want to shrink spatial resolution too much. But visualization is more helpful when done with fewer filters – which is why, above, I’ve told torch to generate sixteen filters only. Here they are (fig. 22.7):\n\ndf &lt;- as_tibble(as.matrix(fb)) %&gt;%\n  rowid_to_column(\"x\")\n\ndf %&gt;%\n  pivot_longer(!x) %&gt;%\n  ggplot(\n    aes(x = x, y = value, color = name, group = name)\n  ) +\n  geom_line() +\n  xlab(\"Fourier coefficient\") +\n  ylab(\"Contribution to Mel coefficient\") +\n  theme_minimal() +\n  theme(legend.position = \"None\")\n\n\n\n\nFigure 22.7: Mel filter bank with sixteen filters, as applied to 257 Fourier coefficients.\n\n\nFrom this plot, it should make sense how Mel-scale transformation is designed to compensate for lower perceptual resolution in higher-frequency ranges.\nOf course, in order to work with Mel spectrograms, you won’t need to generate filter banks yourself. For training, we’ll just replace transform_spectrogram() by transform_mel_spectrogram(). But sometimes, playing around with helper functions can significantly aid understanding. For example, using another low-levellish utility – functional_mel_scale() – we can take a set of Fourier coefficients, convert them to Mel scale, and compare.\nHere, I’m taking the spectrogram for sample 2000 - our “bird” – and pick a time window of interest. I then use functional_mel_scale() to obtain the respective Mel-scale representation, and plot (fig. 22.8) the magnitudes of both sets of coefficients against each other (showing just the first half of the Fourier coefficients):\n\nsample_spec &lt;- ds[2000]$x\nwin_31 &lt;- sample_spec[1, , 31]\n\nmel_31 &lt;- win_31$unsqueeze(2) %&gt;%\n  functional_mel_scale(n_mel = 128)\n\ndf &lt;- data.frame(\n  coefficient = 1:128,\n  fourier = as.numeric(win_31[1:128]),\n  mel = as.numeric(mel_31)\n)\n\ndf %&gt;%\n  pivot_longer(\n    !coefficient,\n    names_to = \"type\", values_to = \"magnitude\"\n  ) %&gt;%\n  ggplot(\n    aes(x = coefficient, y = magnitude, color = type)\n  ) +\n  geom_line() +\n  theme_minimal()\n\n\n\n\nFigure 22.8: Fourier and Mel coefficients, compared on one window of the “bird” spectrogram.\n\n\nFinally, before getting back to the task, let’s quickly verify that calling functional_mel_scale() is equivalent to constructing a filter bank manually and multiplying the resulting matrix with the Fourier coefficients. To do so, we now create a 257 x 128 matrix, and apply it to the spectrogram window we extracted above. The results should be equal (apart from small numerical errors):\n\nfb &lt;- functional_create_fb_matrix(\n  n_freqs = 257,\n  f_min = 0,\n  f_max = 8000,\n  n_mels = 128,\n  sample_rate = 16000\n)\n\n(fb$t()$matmul(win_31) - mel_31[1, , 1]) %&gt;%\n  torch_max()\n\ntorch_tensor\n5.96046e-08\n[ CPUFloatType{} ]\nNow that we have an idea of how Mel-scale transformation works, and possibly a gut feeling as to whether this will help training or not: Let’s actually find out. Necessary modifications are minimal; only the argument list to spectrogram_dataset() is concerned:\n\nds &lt;- spectrogram_dataset(\n  root = \"~/.torch-datasets\",\n  url = \"speech_commands_v0.01\",\n  download = TRUE,\n  n_mels = 128\n)\n\nModel as well as training code stay the same. Learning rate finder output also looked rather similar (fig. 22.9):\n\n\n\nFigure 22.9: Learning rate finder, run on the Mel-transform-enriched model.\n\n\nUsing the same learning rate as previously, I saw training end after nineteen epochs (fig. 22.10).\n\n\n\nFigure 22.10: Fitting the Mel-transform-enriched model.\n\n\nFinal validation accuracy was at about 0.86, minimally lower than for the baseline model.\n\"epoch\",\"set\",\"loss\",\"acc\"\n1,\"train\",3.35417570164001,0.0639678615574784\n1,\"valid\",3.03987254348456,0.142228059332509\n2,\"train\",2.6629929004931,0.220874536464771\n2,\"valid\",2.20017999761245,0.367815203955501\n3,\"train\",2.03871287512623,0.400262669962917\n3,\"valid\",1.56310861835293,0.559100741656366\n...\n...\n17,\"train\",0.534631294167899,0.838715492377421\n17,\"valid\",0.524456901585355,0.854063658838072\n18,\"train\",0.526362751336659,0.840801400906469\n18,\"valid\",0.58039141460961,0.839076019777503\n19,\"train\",0.511069792840216,0.847059126493613\n19,\"valid\",0.511746386686961,0.858544499381953\nSo in this task, no improvement was seen through Mel transformation. But what matters is for us to be aware of this option, such that in other projects, we may give this technique a try.\nFor completeness, here’s double-checking against the test set.\n\nevaluate(fitted, test_dl)\n\nloss: 0.5081\nacc: 0.8555\nNo surprise here, either. Interestingly though, this time the confusion matrix looks rather different, thus hinting at a change in inner workings (fig. 22.11). (To be sure, though, we’d have to run each version several times.)\n\n\n\nFigure 22.11: Alluvial plot for the Mel-transform-enriched setup\n\n\nLeaving further exploration to the experts, we proceed to the final alternative.\n\n\n22.4.3 Variation two: Complex-valued spectograms\nWhen introducing spectrograms, I focused on their two-dimensional structure: The Fourier Transform is performed independently for a set of (overlapping) windows, leaving us with a grid where time slices and Fourier coefficients are arranged in rows and columns. This grid, we found, can be treated as an image of sorts. What I didn’t expand on were the actual values contained in that grid. Fourier coefficients are complex-valued, and thus, if left alone would need to be plotted in two-dimensional space – and then, the spectrogram, in turn, would have to be three-dimensional. In practice, for display (and other) purposes, one often works with the magnitudes of the coefficients instead, or the squares of those magnitudes.\nMaybe you’ve noticed that transform_spectrogram() takes an argument, power, which I haven’t commented on yet. This argument lets you specify whether you’d like squared magnitudes (power = 2, the default), absolute values (power = 1), any other positive value (such as 0.5, the one we used when displaying a concrete example) – or both real and imaginary parts of the coefficients (power = NULL). So far, we’ve always gone with the default. But we might well wonder whether a neural network could profit from the additional information contained in the “whole” complex number. After all, when reducing to magnitudes we lose the phase shifts for the individual coefficients, which could well contain usable information. In any case, it’s worth a try!\nOn the technical side, necessary modifications (again) are minimal. The call to spectrogram_dataset() now makes use of the power argument. Specifying power = NULL, we explicitly request both real and imaginary parts of the Fourier coefficients. The idea is to pass them to the model’s initial nn_conv2d() as two separate channels.\n\nds &lt;- spectrogram_dataset(\n  root = \"~/.torch-datasets\",\n  url = \"speech_commands_v0.01\",\n  download = TRUE,\n  power = NULL\n)\n\ndim(ds[1]$x)\n\n[1]   2 257 101\nCorrespondingly, the first convolutional module now is set up to work on two-channel inputs.\n\nmodel &lt;- nn_module(\n  initialize = function() {\n    self$features &lt;- nn_sequential(\n      nn_conv2d(2, 32, kernel_size = 3),\n      nn_batch_norm2d(32),\n      nn_relu(),\n      nn_max_pool2d(kernel_size = 2),\n      nn_dropout2d(p = 0.2),\n      nn_conv2d(32, 64, kernel_size = 3),\n      nn_batch_norm2d(64),\n      nn_relu(),\n      nn_max_pool2d(kernel_size = 2),\n      nn_dropout2d(p = 0.2),\n      nn_conv2d(64, 128, kernel_size = 3),\n      nn_batch_norm2d(128),\n      nn_relu(),\n      nn_max_pool2d(kernel_size = 2),\n      nn_dropout2d(p = 0.2),\n      nn_conv2d(128, 256, kernel_size = 3),\n      nn_batch_norm2d(256),\n      nn_relu(),\n      nn_max_pool2d(kernel_size = 2),\n      nn_dropout2d(p = 0.2),\n      nn_conv2d(256, 512, kernel_size = 3),\n      nn_batch_norm2d(512),\n      nn_relu(),\n      nn_adaptive_avg_pool2d(c(1, 1)),\n      nn_dropout2d(p = 0.2)\n    )\n\n    self$classifier &lt;- nn_sequential(\n      nn_linear(512, 512),\n      nn_batch_norm1d(512),\n      nn_relu(),\n      nn_dropout(p = 0.5),\n      nn_linear(512, 30)\n    )\n  },\n  forward = function(x) {\n    x &lt;- self$features(x)$squeeze()\n    x &lt;- self$classifier(x)\n    x\n  }\n)\n\nHere is what I saw when running the learning rate finder (fig. 22.12):\n\n\n\nFigure 22.12: Learning rate finder, run on the complex-spectrogram model.\n\n\nThis time, training went on for forty epochs.\n\n\n\nFigure 22.13: Fitting the complex-spectrogram model.\n\n\nVisually, the loss and accuracy curves for the validation set look smoother than in both other cases (fig. 22.13). Checking accuracies, we see:\n\"epoch\",\"set\",\"loss\",\"acc\"\n1,\"train\",3.09768574611813,0.12396992171405\n1,\"valid\",2.52993751740923,0.284378862793572\n2,\"train\",2.26747255972008,0.333642356819118\n2,\"valid\",1.66693911248562,0.540791100123609\n3,\"train\",1.62294889937818,0.518464153275649\n3,\"valid\",1.11740599192825,0.704882571075402\n...\n...\n38,\"train\",0.18717994078312,0.943809229501442\n38,\"valid\",0.23587799138006,0.936418417799753\n39,\"train\",0.19338578602993,0.942882159044087\n39,\"valid\",0.230597475945365,0.939431396786156\n40,\"train\",0.190593419024368,0.942727647301195\n40,\"valid\",0.243536252455384,0.936186650185414\nWith an accuracy of ~0.94, we now have a clear improvement.\nWe can confirm this on the test set:\n\nevaluate(fitted, test_dl)\n\nloss: 0.2373\nacc: 0.9324\nFinally, the confusion matrix now looks like a cleaned-up version of the baseline run (fig. 22.14).\n\n\n\nFigure 22.14: Alluvial plot for the complex-spectrogram setup.\n\n\nWe can safely say that taking into account phase information has significantly improved performance.\nWith this, we’re ending our tour of deep learning applications. But as regards the magnificent Fourier Transform, we’ll be going into a lot more detail soon!\n\n\n\n\nWarden, Pete. 2018. “Speech Commands: A Dataset for Limited-Vocabulary Speech Recognition.” CoRR abs/1804.03209. http://arxiv.org/abs/1804.03209."
  },
  {
    "objectID": "other_overview.html",
    "href": "other_overview.html",
    "title": "23  Overview",
    "section": "",
    "text": "By now, we’ve talked a lot about deep learning. But torch is fruitfully employed in other kinds of tasks, as well – scientific applications, for example, that rely on mathematical methods to discover patterns, relations, and structure.\nIn this section, we concentrate on three topics. The first is matrix computations – a subject whose importance is hard to call into question, seeing how all computations in scientific computation and machine learning are matrix computations (tensors just being higher-order matrices). Concretely, we’ll solve a least-squares problem by means of matrix factorization, making use of functions like linalg_cholesky(), linalg_qr(), and linalg_svd(). In addition, we’ll take a short look at how convolution (in its original, signal-processing sense) can be implemented efficiently.\nNext, we move on to a famous mathematical method we’ve already made (indirect, but highly beneficial) use of: the Discrete Fourier Transform (DFT). This time, though, we don’t just use it; instead, we aim to understand why and how it works. Once we have that understanding, a straightforward implementation is a matter of just a few lines of code. A second chapter is then dedicated to implementing the DFT efficiently, by means of the Fast Fourier Transform (FFT). Again, we start by analyzing its workings, and go on to code it from scratch. You’ll see one of the hand-coded methods coming surprisingly close, in performance, to torch’s own torch_fft_fft().\nFinally, we explore an idea that is far more recent than Fourier methods; namely, the Wavelet Transform. This transform is widely used in data analysis, and we’ll understand clearly why that’s the case. In torch, there is no dedicated method to compute the Wavelet Transform; but we’ll see how repeated use of torch_fft_fft() results in an efficient implementation."
  },
  {
    "objectID": "matrix_computations_leastsquares.html#five-ways-to-do-least-squares",
    "href": "matrix_computations_leastsquares.html#five-ways-to-do-least-squares",
    "title": "24  Matrix computations: Least-squares problems",
    "section": "24.1 Five ways to do least squares",
    "text": "24.1 Five ways to do least squares\nHow do you compute linear least-squares regression? In R, using lm(); in torch, there is linalg_lstsq(). Where R, sometimes, hides complexity from the user, high-performance computation frameworks like torch tend to ask a bit more up-front effort, be it careful reading of documentation, or playing around some, or both. For example, here is the central piece of documentation for linalg_lstsq(), elaborating on the driver parameter to the function:\n\ndriver chooses the LAPACK/MAGMA function that will be used.\nFor CPU inputs the valid values are ‘gels’, ‘gelsy’, ‘gelsd, ’gelss’.\nFor CUDA input, the only valid driver is ‘gels’, which assumes that A is full-rank.\nTo choose the best driver on CPU consider:\n\nIf A is well-conditioned (its condition number is not too large), or you do not mind some precision loss:\n\nFor a general matrix: ‘gelsy’ (QR with pivoting) (default)\nIf A is full-rank: ‘gels’ (QR)\n\nIf A is not well-conditioned:\n\n‘gelsd’ (tridiagonal reduction and SVD)\nBut if you run into memory issues: ‘gelss’ (full SVD).\n\n\n\nWhether you’ll need to know this will depend on the problem you’re solving. But if you do, it certainly will help to have an idea what is being talked about there, if only in a high-level way.\nIn our example problem below, we’re going to be lucky. All drivers will return the same result – but only once we’ll have applied a “trick”, of sorts. Still, we’ll go on and dig deeper into the various methods used by linalg_lstsq(), as well as a few others of common use. Concretely, we’ll solve least squares:\n\nBy means of the so-called normal equations, the most direct way, in the sense that it immediately results from a mathematical statement of the problem.\nAgain, starting from the normal equations, but making use of Cholesky factorization in solving them.\nYet again, taking the normal equations for a point of departure, but proceeding by means of LU decomposition.\nFourth, employing another type of factorization – QR – that, together with the final one, accounts for the vast majority of decompositions applied “in the real world”. With QR decomposition, the solution algorithm does not start from the normal equations.\nAnd fifth and finally, making use of Singular Value Decomposition (SVD). Here, too, the normal equations are not needed.\n\nAll methods will first be applied to a real-world dataset, and then, be tested on a benchmark problem well known for its lack of stability."
  },
  {
    "objectID": "matrix_computations_leastsquares.html#regression-for-weather-prediction",
    "href": "matrix_computations_leastsquares.html#regression-for-weather-prediction",
    "title": "24  Matrix computations: Least-squares problems",
    "section": "24.2 Regression for weather prediction",
    "text": "24.2 Regression for weather prediction\nThe dataset we’ll use is available from the UCI Machine Learning Repository. The way we’ll use it does not quite match the original purpose of collection; instead of forecasting temperature with machine learning, the original study (Cho et al. (2020)) really was about bias correction of forecasts obtained from a numerical weather prediction model. But never mind – our focus here is on matrix methods, and the dataset lends itself very well to the kinds of explorations we’re going to do.\n\nset.seed(777)\n\nlibrary(torch)\ntorch_manual_seed(777)\n\nlibrary(dplyr)\nlibrary(readr)\n\nlibrary(zeallot)\n\nuci &lt;- \"https://archive.ics.uci.edu\"\nds_path &lt;- \"ml/machine-learning-databases/00514\"\nds_file &lt;- \"Bias_correction_ucl.csv\"\n\n# download.file(\n#   file.path(uci, ds_path, ds_file),\n#   destfile = \"resources/matrix-weather.csv\"\n# )\n\nweather_df &lt;- read_csv(\"resources/matrix-weather.csv\") %&gt;%\n  na.omit()\nweather_df %&gt;% glimpse()\n\nRows: 7,588\nColumns: 25\n$ station           &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11,…\n$ Date              &lt;date&gt; 2013-06-30, 2013-06-30,…\n$ Present_Tmax      &lt;dbl&gt; 28.7, 31.9, 31.6, 32.0, 31.4, 31.9,…\n$ Present_Tmin      &lt;dbl&gt; 21.4, 21.6, 23.3, 23.4, 21.9, 23.5,…\n$ LDAPS_RHmin       &lt;dbl&gt; 58.25569, 52.26340, 48.69048,…\n$ LDAPS_RHmax       &lt;dbl&gt; 91.11636, 90.60472, 83.97359,…\n$ LDAPS_Tmax_lapse  &lt;dbl&gt; 28.07410, 29.85069, 30.09129,…\n$ LDAPS_Tmin_lapse  &lt;dbl&gt; 23.00694, 24.03501, 24.56563,…\n$ LDAPS_WS          &lt;dbl&gt; 6.818887, 5.691890, 6.138224,…\n$ LDAPS_LH          &lt;dbl&gt; 69.45181, 51.93745, 20.57305,…\n$ LDAPS_CC1         &lt;dbl&gt; 0.2339475, 0.2255082, 0.2093437,…\n$ LDAPS_CC2         &lt;dbl&gt; 0.2038957, 0.2517714, 0.2574694,…\n$ LDAPS_CC3         &lt;dbl&gt; 0.1616969, 0.1594441, 0.2040915,…\n$ LDAPS_CC4         &lt;dbl&gt; 0.1309282, 0.1277273, 0.1421253,…\n$ LDAPS_PPT1        &lt;dbl&gt; 0.0000000, 0.0000000, 0.0000000,…\n$ LDAPS_PPT2        &lt;dbl&gt; 0.000000, 0.000000, 0.000000,…\n$ LDAPS_PPT3        &lt;dbl&gt; 0.0000000, 0.0000000, 0.0000000,…\n$ LDAPS_PPT4        &lt;dbl&gt; 0.0000000, 0.0000000, 0.0000000,…\n$ lat               &lt;dbl&gt; 37.6046, 37.6046, 37.5776, 37.6450,…\n$ lon               &lt;dbl&gt; 126.991, 127.032, 127.058, 127.022,…\n$ DEM               &lt;dbl&gt; 212.3350, 44.7624, 33.3068, 45.7160,…\n$ Slope             &lt;dbl&gt; 2.7850, 0.5141, 0.2661, 2.5348,…\n$ `Solar radiation` &lt;dbl&gt; 5992.896, 5869.312, 5863.556,…\n$ Next_Tmax         &lt;dbl&gt; 29.1, 30.5, 31.1, 31.7, 31.2, 31.5,…\n$ Next_Tmin         &lt;dbl&gt; 21.2, 22.5, 23.9, 24.3, 22.5, 24.0,…\nThe way we’re framing the task, basically everything in the dataset serves (or would serve, if we kept it – more on that below) as a predictor. As target, we’ll use Next_Tmax, the maximal temperature reached on the subsequent day. This means we need to remove Next_Tmin from the set of predictors, as it would make for too powerful of a clue. We’ll do the same for station, the weather station id, and Date. This leaves us with twenty-one predictors, including measurements of actual temperature (Present_Tmax, Present_Tmin), model forecasts of various variables (LDAPS_*), and auxiliary information (lat, lon, and `Solar radiation`, among others).\n\nweather_df &lt;- weather_df %&gt;%\n  select(-c(station, Next_Tmin, Date)) %&gt;%\n  mutate(across(.fns = scale))\n\nNote how, above, I’ve added a line to standardize the predictors. This is the “trick” I was alluding to above. We’ll talk about why we’re doing this soon.\nFor torch, we split up the data into two tensors: a matrix A, containing all predictors, and a vector b that holds the target.\n\nweather &lt;- torch_tensor(weather_df %&gt;% as.matrix())\nA &lt;- weather[ , 1:-2]\nb &lt;- weather[ , -1]\n\ndim(A)\n\n[1] 7588   21\nNow, first let’s determine the expected output.\n\n24.2.1 Least squares (I): Setting expectations with lm()\nIf there’s a least squares implementation we “believe in”, it surely must be lm().\n\nfit &lt;- lm(Next_Tmax ~ . , data = weather_df)\nfit %&gt;% summary()\n\nCall:\nlm(formula = Next_Tmax ~ ., data = weather_df)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.94439 -0.27097  0.01407  0.28931  2.04015 \n\nCoefficients:\n                    Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)        2.605e-15  5.390e-03   0.000 1.000000    \nPresent_Tmax       1.456e-01  9.049e-03  16.089  &lt; 2e-16 ***\nPresent_Tmin       4.029e-03  9.587e-03   0.420 0.674312    \nLDAPS_RHmin        1.166e-01  1.364e-02   8.547  &lt; 2e-16 ***\nLDAPS_RHmax       -8.872e-03  8.045e-03  -1.103 0.270154    \nLDAPS_Tmax_lapse   5.908e-01  1.480e-02  39.905  &lt; 2e-16 ***\nLDAPS_Tmin_lapse   8.376e-02  1.463e-02   5.726 1.07e-08 ***\nLDAPS_WS          -1.018e-01  6.046e-03 -16.836  &lt; 2e-16 ***\nLDAPS_LH           8.010e-02  6.651e-03  12.043  &lt; 2e-16 ***\nLDAPS_CC1         -9.478e-02  1.009e-02  -9.397  &lt; 2e-16 ***\nLDAPS_CC2         -5.988e-02  1.230e-02  -4.868 1.15e-06 ***\nLDAPS_CC3         -6.079e-02  1.237e-02  -4.913 9.15e-07 ***\nLDAPS_CC4         -9.948e-02  9.329e-03 -10.663  &lt; 2e-16 ***\nLDAPS_PPT1        -3.970e-03  6.412e-03  -0.619 0.535766    \nLDAPS_PPT2         7.534e-02  6.513e-03  11.568  &lt; 2e-16 ***\nLDAPS_PPT3        -1.131e-02  6.058e-03  -1.866 0.062056 .  \nLDAPS_PPT4        -1.361e-03  6.073e-03  -0.224 0.822706    \nlat               -2.181e-02  5.875e-03  -3.713 0.000207 ***\nlon               -4.688e-02  5.825e-03  -8.048 9.74e-16 ***\nDEM               -9.480e-02  9.153e-03 -10.357  &lt; 2e-16 ***\nSlope              9.402e-02  9.100e-03  10.331  &lt; 2e-16 ***\n`Solar radiation`  1.145e-02  5.986e-03   1.913 0.055746 .  \n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 0.4695 on 7566 degrees of freedom\nMultiple R-squared:  0.7802,    Adjusted R-squared:  0.7796 \nF-statistic:  1279 on 21 and 7566 DF,  p-value: &lt; 2.2e-16\nWith an explained variance of 78%, the forecast is working pretty well. This is the baseline we want to check all other methods against. To that purpose, we’ll store respective predictions and prediction errors (the latter being operationalized as root mean squared error, RMSE). For now, we just have entries for lm():\n\nrmse &lt;- function(y_true, y_pred) {\n  (y_true - y_pred)^2 %&gt;%\n    sum() %&gt;%\n    sqrt()\n}\n\nall_preds &lt;- data.frame(\n  b = weather_df$Next_Tmax,\n  lm = fit$fitted.values\n)\nall_errs &lt;- data.frame(lm = rmse(all_preds$b, all_preds$lm))\nall_errs\n\n       lm\n1 40.8369\n\n\n24.2.2 Least squares (II): Using linalg_lstsq()\nNow, for a moment let’s assume this was not about exploring different approaches, but getting a quick result. In torch, we have linalg_lstsq(), a function dedicated specifically to solving least-squares problems. (This is the function whose documentation I was citing, above.) Just like we did with lm(), we’d probably just go ahead and call it, making use of the default settings:\n\nx_lstsq &lt;- linalg_lstsq(A, b)$solution\n\nall_preds$lstsq &lt;- as.matrix(A$matmul(x_lstsq))\nall_errs$lstsq &lt;- rmse(all_preds$b, all_preds$lstsq)\n\ntail(all_preds)\n\n              b         lm      lstsq\n7583 -1.1380931 -1.3544620 -1.3544616\n7584 -0.8488721 -0.9040997 -0.9040993\n7585 -0.7203294 -0.9675286 -0.9675281\n7586 -0.6239224 -0.9044044 -0.9044040\n7587 -0.5275154 -0.8738639 -0.8738635\n7588 -0.7846007 -0.8725795 -0.8725792\nPredictions resemble those of lm() very closely – so closely, in fact, that we may guess those tiny differences are just due to numerical errors surfacing from deep down the respective call stacks. RMSE, thus, should be equal as well:\n\nall_errs\n\n       lm    lstsq\n1 40.8369 40.8369\nIt is; and this is a satisfying outcome. However, it only really came about due to that “trick”: normalization. Of course, when I say “trick”, I don’t really mean it. Standardizing the data is a common operation, and especially with neural networks, it tends to get used routinely, to speed up training. The point I’d like to make is this: Frameworks for high-performance computation, like torch, will often presuppose more domain knowledge, or more up-front analysis, on the part of the user.\nI’ll explain.\n\n\n24.2.3 Interlude: What if we hadn’t standardized the data?\nFor quick comparison, let’s create an alternate matrix of predictors: not normalizing the data, this time.\n\nweather_df_alt &lt;- \n  read_csv(\"resources/matrix-weather.csv\") %&gt;% \n  na.omit() %&gt;%\n  select(-c(station, Next_Tmin, Date)) \n\nweather_alt &lt;- torch_tensor(weather_df_alt %&gt;% as.matrix())\nA_alt &lt;- weather_alt[ , 1:-2]\nb_alt &lt;- weather_alt[ , -1]\n\nTo set our expectations, we again call lm():\n\nfit_alt &lt;- lm(Next_Tmax ~ ., data = weather_df_alt)\nall_preds_alt &lt;- data.frame(\n  b = weather_df_alt$Next_Tmax,\n  lm = fit_alt$fitted.values\n)\n\nall_errs_alt &lt;- data.frame(\n  lm = rmse(\n    all_preds_alt$b,\n    all_preds_alt$lm\n  )\n)\n\nall_errs_alt\n\n        lm\n1 127.0765\nNow, we call linalg_lstsq(), using the default arguments just like we did before.\n\nx_lstsq_alt &lt;- linalg_lstsq(A_alt, b_alt)$solution\n\nall_preds_alt$lstsq &lt;- as.matrix(A_alt$matmul(x_lstsq_alt))\nall_errs_alt$lstsq &lt;- rmse(\n  all_preds_alt$b, all_preds_alt$lstsq\n)\n\nall_errs_alt\n\n        lm    lstsq\n1 127.0765 177.9128\nWow – what happened here? Thinking back of that piece of documentation I’ve cited, maybe the default arguments aren’t working out that well, this time. Let’s find out why.\n\n24.2.3.1 Investigating “the issue”\nTo efficiently solve a linear least-squares problem, torch calls into LAPACK, a set of Fortran routines designed to efficiently and scaleably address the tasks most frequently found in linear algebra: solving linear systems of equations, computing eigenvectors and eigenvalues, and determining singular values.\nThe allowed drivers in linalg_lstsq() correspond to different LAPACK procedures1, and these procedures all apply different algorithms in order to solve the problem – analogously to what’ll we do ourselves, below.\nThus, in investigating what is going on, step one is to determine which method gets used and why; analyse (if possible) why the result is unsatisfying; determine the LAPACK routine we’d like to be using instead, and check what happens if indeed we do. (Of course, given the little effort involved, we’d probably give all methods a try.)\nThe main concept involved here is the rank of a matrix.\n\n\n24.2.3.2 Concepts (I): Rank of a matrix\n“But wait!” you may be thinking – from the above-cited piece of documentation, it seems like the first thing we should check is not rank, but condition number: whether the matrix is “well-conditioned”. Yes, the condition number certainly is important, and we’ll get back to it very soon. However, there is something even more fundamental at work here, something that does not really “jump to the eye”.\nThe central piece of information is found in that LAPACK piece of documentation we’re being referred to by linalg_lstsq(). Between the four routines GELS, GELSY, GELSD, and GELSS, differences are not restricted to implementation. The goal of optimization differs, as well. The rationale is the following. Throughout, let’s assume we’re working with a matrix that has more rows than columns (more observations than features, in the most-frequent case):\n\nIf the matrix is full-rank – meaning, its columns are linearly independent – there is no “perfect” solution. The problem is over-determined. All we can do is find the best possible approximation. This is done by minimizing the prediction error – we’ll come back to that when discussing the normal equations. Minimize prediction error is what the GELS routine does, and it is GELS we should use when we have a full-rank matrix of predictors.\nIf the matrix is not full-rank, the problem is under-determined; there is an infinite number of solutions. All the remaining routines – GELSY, GELSD, and GELSS – are suited to this situation. While they do proceed differently, they all pursue the same strategy, different from the one followed by GELS: In addition to the prediction error, they also minimize the vector of coefficients. This is called finding a minimum-norm least-squares solution.\n\nIn sum, GELS (for full-rank matrices) and the three of GELSY, GELSD, and GELSS (for when the matrix is rank-deficient) intentionally follow different optimization criteria.\nNow, as per the documentation for linalg_lstsq(), when no driver is passed explicitly, it is GELSY that gets called. That should be fine if our matrix is rank-deficient – but is it?\n\nlinalg_matrix_rank(A_alt)\n\ntorch_tensor\n21\n[ CPULongType{} ]\nThe matrix has twenty-one columns; so if its rank is twenty-one, then it is full-rank for sure. We definitely want to be calling the GELS routine.\n\n\n24.2.3.3 Calling linalg_lstsq() the right way\nNow that we know what to pass for driver, here is the modified call:\n\nx_lstsq_alt &lt;- linalg_lstsq(\n  A_alt, b_alt,\n  driver = \"gels\"\n)$solution\n\nall_preds_alt$lstsq &lt;- as.matrix(A_alt$matmul(x_lstsq_alt))\nall_errs_alt$lstsq &lt;- rmse(\n  all_preds_alt$b,\n  all_preds_alt$lstsq\n)\n\nall_errs_alt\n\n        lm    lstsq\n1 127.0765 127.9489\nNow, the respective RMSE values are very close. You’ll be wondering, though: Why didn’t we have to specify the Fortran routine when working with the standardized matrix?\n\n\n24.2.3.4 Why did standardization help?\nFor our matrix, what standardization did was reduce significantly the range spanned by the singular values. With A, the standardized matrix, the largest singular value is about ten times as large as the smallest one:\n\nsvals_normalized_A &lt;- linalg_svdvals(A)/linalg_svdvals(A)[1]\nsvals_normalized_A %&gt;% as.numeric()\n\n[1] 1.0000000 0.7473214 0.5929527 0.5233989 0.5188764 0.4706140\n[7] 0.4391665 0.4249273 0.4034659 0.3815900 0.3621315 0.3557949\n[13] 0.3297923 0.2707912 0.2489560 0.2229859 0.2175170 0.1852890\n[19] 0.1627083 0.1553169 0.1075778\nWhile with A_alt, it is a million times as large:\n\nsvals_normalized_A_alt &lt;- linalg_svdvals(A_alt) /\n  linalg_svdvals(A_alt)[1]\nsvals_normalized_A_alt %&gt;% as.numeric()\n\n[1] 1.000000e+00 1.014369e-02 6.407313e-03 2.881966e-03\n[5] 2.236537e-03 9.633782e-04 6.678377e-04 3.988165e-04\n[9] 3.584047e-04 3.137257e-04 2.699152e-04 2.383501e-04\n[13] 2.234150e-04 1.803384e-04 1.625245e-04 1.300101e-04\n[17] 4.312536e-05 3.463851e-05 1.964120e-05 1.689913e-05\n[18] 8.419599e-06\nWhy is this important? It’s here that we finally get back to the condition number.\n\n\n24.2.3.5 Concepts (II): Condition number\nThe higher the so-called condition number of a matrix, the more likely we are to run into problems of numerical stability when computing with it. In torch, linalg_cond() is used to obtain the condition number. Let’s compare the condition numbers for A and A_alt, respectively.\n\nlinalg_cond(A)\nlinalg_cond(A_alt)\n\ntorch_tensor\n9.2956\n[ CPUFloatType{} ]\n\ntorch_tensor\n118770\n[ CPUFloatType{} ]\nThat is quite a difference! How does it arise?\nThe condition number is defined as the matrix norm of A, divided by the norm of its inverse. Different kinds of norm may be used; the default is the 2-norm. In this case, condition number can be computed from the matrix’s singular values: Namely, the 2-norm of A equals the largest singular value, while that of its inverse is given by the smallest one.\nWe can verify this ourselves, using linalg_svdvals() as before:\n\nlinalg_svdvals(A)[1]/linalg_svdvals(A)[21]\nlinalg_svdvals(A_alt)[1]/linalg_svdvals(A_alt)[21]\n\ntorch_tensor\n9.29559\n[ CPUFloatType{} ]\n\ntorch_tensor\n118770\n[ CPUFloatType{} ]\nTo reiterate, this is a substantial difference. Incidentally, do you remember that in the case of A_alt, RMSE was a tiny bit worse for linalg_lstsq() than for lm(), even when using the appropriate routine, GELS? Given that both essentially use the same algorithm (QR factorization, to be introduced very soon) this may very well have been due to numerical errors, arising from the high condition number of A_alt.\nBy now, I may have convinced you that with torch’s linalg component, it helps to know a bit about how the most-in-use least-squares algorithms work. Let’s get acquainted.\n\n\n\n24.2.4 Least squares (III): The normal equations\nWe start by stating the goal. Given a matrix, \\(\\mathbf{A}\\), that holds features in its columns and observations in its rows, and a vector of observed outcomes, \\(\\mathbf{b}\\), we want to find regression coefficients, one for each feature, that allow to approximate \\(\\mathbf{b}\\) as well as possible. Call the vector of regression coefficients \\(\\mathbf{x}\\). To obtain it, we need to solve a simultaneous system of equations, that in matrix notation appears as\n\\[\n\\mathbf{Ax} = \\mathbf{b}\n\\]\nIf \\(\\mathbf{b}\\) were a square, invertible matrix, the solution could directly be computed as \\(\\mathbf{x} = \\mathbf{A}^{-1}\\mathbf{b}\\). This will hardly ever be possible, though; we’ll (hopefully) always have more observations than predictors. Another approach is needed. It directly starts from the problem statement.\nWhen we use the columns of \\(\\mathbf{A}\\) to approximate \\(\\mathbf{b}\\), that approximation necessarily is in the column space of \\(\\mathbf{A}\\). \\(\\mathbf{b}\\), on the other hand, normally won’t be. We want those two to be as close as possible; in other words, we want to minimize the distance between them. Choosing the 2-norm for the distance, this yields the objective\n\\[\nminimize \\ ||\\mathbf{Ax}-\\mathbf{b}||^2\n\\]\nThis distance is the (squared) length of the vector of prediction errors. That vector necessarily is orthogonal to \\(\\mathbf{A}\\) itself. That is, when we multiply it with \\(\\mathbf{A}\\), we get the zero vector:\n\\[\n\\mathbf{A}^T(\\mathbf{Ax} - \\mathbf{b}) = \\mathbf{0}\n\\]\nA rearrangement of this equation yields the so-called normal equations:\n\\[\n\\mathbf{A}^T \\mathbf{A} \\mathbf{x} = \\mathbf{A}^T \\mathbf{b}\n\\]\nThese may be solved for \\(\\mathbf{x}\\), computing the inverse of \\(\\mathbf{A}^T\\mathbf{A}\\):\n\\[\n\\mathbf{x} = (\\mathbf{A}^T \\mathbf{A})^{-1} \\mathbf{A}^T \\mathbf{b}\n\\]\n\\(\\mathbf{A}^T\\mathbf{A}\\) is a square matrix. It still might not be invertible, in which case the so-called pseudoinverse would be computed instead. In our case, this will not be needed; we already know \\(\\mathbf{A}\\) has full rank, and so does \\(\\mathbf{A}^T\\mathbf{A}\\).\nThus, from the normal equations we have derived a recipe for computing \\(\\mathbf{b}\\). Let’s put it to use, and compare with what we got from lm() and linalg_lstsq().\n\nAtA &lt;- A$t()$matmul(A)\nAtb &lt;- A$t()$matmul(b)\ninv &lt;- linalg_inv(AtA)\nx &lt;- inv$matmul(Atb)\n\nall_preds$neq &lt;- as.matrix(A$matmul(x))\nall_errs$neq &lt;- rmse(all_preds$b, all_preds$neq)\n\nall_errs\n\n       lm   lstsq     neq\n1 40.8369 40.8369 40.8369\nHaving confirmed that the direct way works, we may allow ourselves some sophistication. Four different matrix factorizations will make their appearance: Cholesky, LU, QR, and Singular Value Decomposition. The goal, in every case, is to avoid the expensive computation of the (pseudo-) inverse. That’s what all methods have in common. However, they do not differ “just” in the way the matrix is factorized, but also, in which matrix is. This has to do with the constraints the various methods impose. Roughly speaking, the order they’re listed in above reflects a falling slope of preconditions, or put differently, a rising slope of generality. Due to the constraints involved, the first two (Cholesky, as well as LU decomposition) will be performed on \\(\\mathbf{A}^T\\mathbf{A}\\), while the latter two (QR and SVD) operate on \\(\\mathbf{A}\\) directly. With them, there never is a need to compute \\(\\mathbf{A}^T\\mathbf{A}\\).\n\n\n24.2.5 Least squares (IV): Cholesky decomposition\nIn Cholesky decomposition, a matrix is factored into two triangular matrices of the same size, with one being the transpose of the other. This commonly is written either\n\\[\n\\mathbf{A} = \\mathbf{L} \\mathbf{L}^T\n\\] or\n\\[\n\\mathbf{A} = \\mathbf{R}^T\\mathbf{R}\n\\]\nHere symbols \\(\\mathbf{L}\\) and \\(\\mathbf{R}\\) denote lower-triangular and upper-triangular matrices, respectively.\nFor Cholesky decomposition to be possible, a matrix has to be both symmetric and positive definite. These are pretty strong conditions, ones that will not often be fulfilled in practice. In our case, \\(\\mathbf{A}\\) is not symmetric; this immediately implies we have to operate on \\(\\mathbf{A}^T\\mathbf{A}\\) instead. And since \\(\\mathbf{A}\\) already is positive definite, we know that \\(\\mathbf{A}^T\\mathbf{A}\\) is, as well.\nIn torch, we obtain the Cholesky decomposition of a matrix using linalg_cholesky(). By default, this call will return \\(\\mathbf{L}\\), a lower-triangular matrix.\n\n# AtA = L L_t\nAtA &lt;- A$t()$matmul(A)\nL &lt;- linalg_cholesky(AtA)\n\nLet’s check that we can reconstruct \\(\\mathbf{A}\\) from \\(\\mathbf{L}\\):\n\nLLt &lt;- L$matmul(L$t())\ndiff &lt;- LLt - AtA\nlinalg_norm(diff, ord = \"fro\")\n\ntorch_tensor\n0.00258896\n[ CPUFloatType{} ]\nHere, I’ve computed the Frobenius norm of the difference between the original matrix and its reconstruction. The Frobenius norm individually sums up all matrix entries, and returns the square root. In theory, we’d like to see zero here; but in the presence of numerical errors, the result is sufficient to indicate that the factorization worked fine.\nNow that we have \\(\\mathbf{L}\\mathbf{L}^T\\) instead of \\(\\mathbf{A}^T\\mathbf{A}\\), how does that help us? It’s here that the magic happens, and you’ll find the same type of magic at work in the remaining three methods. The idea is that due to some decomposition, a more performant way arises of solving the system of equations that constitute a given task.\nWith \\(\\mathbf{L}\\mathbf{L}^T\\), the point is that \\(\\mathbf{L}\\) is triangular, and when that’s the case the linear system can be solved by simple substitution. That is best visible with a tiny example:\n\\[\n\\begin{bmatrix}\n  1 & 0 & 0\\\\\n  2 & 3 & 0\\\\\n  3 & 4 & 1\n\\end{bmatrix}\n\\begin{bmatrix}\n  x1\\\\\n  x2\\\\\n  x3\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n  1\\\\\n  11\\\\\n  15\n\\end{bmatrix}\n\\]\nStarting in the top row, we immediately see that \\(x1\\) equals \\(1\\); and once we know that it is straightforward to calculate, from row two, that \\(x2\\) must be \\(3\\). The last row then tells us that \\(x3\\) must be \\(0\\).\nIn code, torch_triangular_solve() is used to efficiently compute the solution to a linear system of equations where the matrix of predictors is lower- or upper-triangular. An additional requirement is for the matrix to be symmetric – but that condition we already had to satisfy in order to be able to use Cholesky factorization.\nBy default, torch_triangular_solve() expects the matrix to be upper- (not lower-)triangular; but there is a function parameter, upper, that lets us correct that expectation. The return value is a list, and its first item contains the desired solution. To illustrate, here is torch_triangular_solve(), applied to the toy example we manually solved above:\n\nsome_L &lt;- torch_tensor(\n  matrix(c(1, 0, 0, 2, 3, 0, 3, 4, 1), nrow = 3, byrow = TRUE)\n)\nsome_b &lt;- torch_tensor(matrix(c(1, 11, 15), ncol = 1))\n\nx &lt;- torch_triangular_solve(\n  some_b,\n  some_L,\n  upper = FALSE\n)[[1]]\nx\n\ntorch_tensor\n 1\n 3\n 0\n[ CPUFloatType{3,1} ]\nReturning to our running example, the normal equations now look like this:\n\\[\n\\mathbf{L}\\mathbf{L}^T \\mathbf{x} = \\mathbf{A}^T \\mathbf{b}\n\\]\nWe introduce a new variable, \\(\\mathbf{y}\\), to stand for \\(\\mathbf{L}^T \\mathbf{x}\\),\n\\[\n\\mathbf{L}\\mathbf{y} = \\mathbf{A}^T \\mathbf{b}\n\\]\nand compute the solution to this system:\n\nAtb &lt;- A$t()$matmul(b)\n\ny &lt;- torch_triangular_solve(\n  Atb$unsqueeze(2),\n  L,\n  upper = FALSE\n)[[1]]\n\nNow that we have \\(y\\), we look back at how it was defined:\n\\[\n\\mathbf{y} = \\mathbf{L}^T \\mathbf{x}\n\\]\nTo determine \\(\\mathbf{x}\\), we can thus again use torch_triangular_solve():\n\nx &lt;- torch_triangular_solve(y, L$t())[[1]]\n\nAnd there we are.\nAs usual, we compute the prediction error:\n\nall_preds$chol &lt;- as.matrix(A$matmul(x))\nall_errs$chol &lt;- rmse(all_preds$b, all_preds$chol)\n\nall_errs\n\n       lm   lstsq     neq    chol\n1 40.8369 40.8369 40.8369 40.8369\nNow that you’ve seen the rationale behind Cholesky factorization – and, as already suggested, the idea carries over to all other decompositions – you might like to save yourself some work making use of a dedicated convenience function, torch_cholesky_solve(). This will render obsolete the two calls to torch_triangular_solve().\nThe following lines yield the same output as the code above – but, of course, they do hide the underlying magic.\n\nL &lt;- linalg_cholesky(AtA)\n\nx &lt;- torch_cholesky_solve(Atb$unsqueeze(2), L)\n\nall_preds$chol2 &lt;- as.matrix(A$matmul(x))\nall_errs$chol2 &lt;- rmse(all_preds$b, all_preds$chol2)\nall_errs\n\n       lm   lstsq     neq    chol   chol2\n1 40.8369 40.8369 40.8369 40.8369 40.8369\nLet’s move on to the next method – equivalently, to the next factorization.\n\n\n24.2.6 Least squares (V): LU factorization\nLU factorization is named after the two factors it introduces: a lower-triangular matrix, \\(\\mathbf{L}\\), as well as an upper-triangular one, \\(\\mathbf{U}\\). In theory, there are no restrictions on LU decomposition: Provided we allow for row exchanges, effectively turning \\(\\mathbf{A} = \\mathbf{L}\\mathbf{U}\\) into \\(\\mathbf{A} = \\mathbf{P}\\mathbf{L}\\mathbf{U}\\) (where \\(\\mathbf{P}\\) is a permutation matrix), we can factorize any matrix.\nIn practice, though, if we want to make use of torch_triangular_solve() , the input matrix has to be symmetric. Therefore, here too we have to work with \\(\\mathbf{A}^T\\mathbf{A}\\), not \\(\\mathbf{A}\\) directly. (And that’s why I’m showing LU decomposition right after Cholesky – they’re similar in what they make us do, though not at all similar in spirit.)\nWorking with \\(\\mathbf{A}^T\\mathbf{A}\\) means we’re again starting from the normal equations. We factorize \\(\\mathbf{A}^T\\mathbf{A}\\), then solve two triangular systems to arrive at the final solution. Here are the steps, including the not-always-needed permutation matrix \\(\\mathbf{P}\\):\n\\[\n\\begin{aligned}\n\\mathbf{A}^T \\mathbf{A} \\mathbf{x} &= \\mathbf{A}^T \\mathbf{b} \\\\\n\\mathbf{P} \\mathbf{L}\\mathbf{U} \\mathbf{x} &= \\mathbf{A}^T \\mathbf{b} \\\\\n\\mathbf{L} \\mathbf{y} &= \\mathbf{P}^T \\mathbf{A}^T \\mathbf{b} \\\\\n\\mathbf{y} &= \\mathbf{U} \\mathbf{x}\n\\end{aligned}\n\\]\nWe see that when \\(\\mathbf{P}\\) is needed, there is an additional computation: Following the same strategy as we did with Cholesky, we want to move \\(\\mathbf{P}\\) from the left to the right. Luckily, what may look expensive – computing the inverse – is not: For a permutation matrix, its transpose reverses the operation.\nCode-wise, we’re already familiar with most of what we need to do. The only missing piece is torch_lu(). torch_lu() returns a list of two tensors, the first a compressed representation of the three matrices \\(\\mathbf{P}\\), \\(\\mathbf{L}\\), and \\(\\mathbf{U}\\). We can uncompress it using torch_lu_unpack() :\n\nlu &lt;- torch_lu(AtA)\n\nc(P, L, U) %&lt;-% torch_lu_unpack(lu[[1]], lu[[2]]) \n\nWe move \\(\\mathbf{P}\\) to the other side:\n\nAtb &lt;- P$t()$matmul(Atb)\n\nAll that remains to be done is solve two triangular systems, and we are done:\n\ny &lt;- torch_triangular_solve(\n  Atb$unsqueeze(2),\n  L,\n  upper = FALSE\n)[[1]]\nx &lt;- torch_triangular_solve(y, U)[[1]]\n\nall_preds$lu &lt;- as.matrix(A$matmul(x))\nall_errs$lu &lt;- rmse(all_preds$b, all_preds$lu)\nall_errs[1, -5]\n\n       lm   lstsq     neq    chol      lu\n1 40.8369 40.8369 40.8369 40.8369 40.8369\nAs with Cholesky decomposition, we can save ourselves the trouble of calling torch_triangular_solve() twice. torch_lu_solve() takes the decomposition, and directly returns the final solution:\n\nlu &lt;- torch_lu(AtA)\nx &lt;- torch_lu_solve(Atb$unsqueeze(2), lu[[1]], lu[[2]])\n\nall_preds$lu2 &lt;- as.matrix(A$matmul(x))\nall_errs$lu2 &lt;- rmse(all_preds$b, all_preds$lu2)\nall_errs[1, -5]\n\n       lm   lstsq     neq    chol      lu      lu\n1 40.8369 40.8369 40.8369 40.8369 40.8369 40.8369\nNow, we look at the two methods that don’t require computation of \\(\\mathbf{A}^T\\mathbf{A}\\).\n\n\n24.2.7 Least squares (VI): QR factorization\nAny matrix can be decomposed into an orthogonal matrix, \\(\\mathbf{Q}\\), and an upper-triangular matrix, \\(\\mathbf{R}\\). QR factorization is probably the most popular approach to solving least-squares problems; it is, in fact, the method used by R’s lm(). In what ways, then, does it simplify the task?\nAs to \\(\\mathbf{R}\\), we already know how it is useful: By virtue of being triangular, it defines a system of equations that can be solved step-by-step, by means of mere substitution. \\(\\mathbf{Q}\\) is even better. An orthogonal matrix is one whose columns are orthogonal – meaning, mutual dot products are all zero – and have unit norm; and the nice thing about such a matrix is that its inverse equals its transpose. In general, the inverse is hard to compute; the transpose, however, is easy. Seeing how computation of an inverse – solving \\(\\mathbf{x}=\\mathbf{A}^{-1}\\mathbf{b}\\) – is just the central task in least squares, it’s immediately clear how significant this is.\nCompared to our usual scheme, this leads to a slightly shortened recipe. There is no “dummy” variable \\(\\mathbf{y}\\) anymore. Instead, we directly move \\(\\mathbf{Q}\\) to the other side, computing the transpose (which is the inverse). All that remains, then, is back-substitution. Also, since every matrix has a QR decomposition, we now directly start from \\(\\mathbf{A}\\) instead of \\(\\mathbf{A}^T\\mathbf{A}\\):\n\\[\n\\begin{aligned}\n\\mathbf{A}\\mathbf{x} &= \\mathbf{b}\\\\\n\\mathbf{Q}\\mathbf{R}\\mathbf{x} &= \\mathbf{b}\\\\\n\\mathbf{R}\\mathbf{x} &= \\mathbf{Q}^T\\mathbf{b}\\\\\n\\end{aligned}\n\\]\nIn torch, linalg_qr() gives us the matrices \\(\\mathbf{Q}\\) and \\(\\mathbf{R}\\).\n\nc(Q, R) %&lt;-% linalg_qr(A)\n\nOn the right side, we used to have a “convenience variable” holding \\(\\mathbf{A}^T\\mathbf{b}\\) ; here, we skip that step, and instead, do something “immediately useful”: move \\(\\mathbf{Q}\\) to the other side.\n\nQtb &lt;- Q$t()$matmul(b)\n\nThe only remaining step now is to solve the remaining triangular system.\n\nx &lt;- torch_triangular_solve(Qtb$unsqueeze(2), R)[[1]]\n\nall_preds$qr &lt;- as.matrix(A$matmul(x))\nall_errs$qr &lt;- rmse(all_preds$b, all_preds$qr)\nall_errs[1, -c(5,7)]\n\n       lm   lstsq     neq    chol      lu      qr\n1 40.8369 40.8369 40.8369 40.8369 40.8369 40.8369 \nBy now, you’ll be expecting for me to end this section saying “there is also a dedicated solver in torch/torch_linalg, namely …”). Well, not literally, no; but effectively, yes. If you call linalg_lstsq() passing driver = \"gels\", it is QR factorization that will be used.\n\n\n24.2.8 Least squares (VII): Singular Value Decomposition (SVD)\nIn true climactic order, the last factorization method we discuss is the most versatile, most diversely applicable, most semantically meaningful one: Singular Value Decomposition (SVD). The third aspect, fascinating though it is, does not relate to our current task, so I won’t go into it here. Here, it is universal applicability that matters: Every matrix can be composed into components SVD-style.\nSingular Value Decomposition factors an input \\(\\mathbf{A}\\) into two orthogonal matrices, called \\(\\mathbf{U}\\) and \\(\\mathbf{V}^T\\), and a diagonal one, named \\(\\symbf{\\Sigma}\\), such that \\(\\mathbf{A} = \\mathbf{U} \\symbf{\\Sigma} \\mathbf{V}^T\\). Here \\(\\mathbf{U}\\) and \\(\\mathbf{V}^T\\) are the left and right singular vectors, and \\(\\symbf{\\Sigma}\\) holds the singular values.\n\\[\n\\begin{aligned}\n\\mathbf{A}\\mathbf{x} &= \\mathbf{b}\\\\\n\\mathbf{U}\\symbf{\\Sigma}\\mathbf{V}^T\\mathbf{x} &= \\mathbf{b}\\\\\n\\symbf{\\Sigma}\\mathbf{V}^T\\mathbf{x} &= \\mathbf{U}^T\\mathbf{b}\\\\\n\\mathbf{V}^T\\mathbf{x} &= \\mathbf{y}\\\\\n\\end{aligned}\n\\]\nWe start by obtaining the factorization, using linalg_svd() . The argument full_matrices = FALSE tells torch that we want a \\(\\mathbf{U}\\) of dimensionality same as \\(\\mathbf{A}\\), not expanded to 7588 x 7588.\n\nc(U, S, Vt) %&lt;-% linalg_svd(A, full_matrices = FALSE)\n\ndim(U)\ndim(S)\ndim(Vt)\n\n[1] 7588   21\n[1] 21\n[1] 21 21\nWe move \\(\\mathbf{U}\\) to the other side – a cheap operation, thanks to \\(\\mathbf{U}\\) being orthogonal.\n\nUtb &lt;- U$t()$matmul(b)\n\nWith both \\(\\mathbf{U}^T\\mathbf{b}\\) and \\(\\symbf{\\Sigma}\\) being same-length vectors, we can use element-wise multiplication to do the same for \\(\\symbf{\\Sigma}\\). We introduce a temporary variable, y, to hold the result.\n\ny &lt;- Utb / S\n\nNow left with the final system to solve, \\(\\mathbf{\\mathbf{V}^T\\mathbf{x} = \\mathbf{y}}\\), we again profit from orthogonality – this time, of the matrix \\(\\mathbf{V}^T\\).\n\nx &lt;- Vt$t()$matmul(y)\n\nWrapping up, let’s calculate predictions and prediction error:\n\nall_preds$svd &lt;- as.matrix(A$matmul(x))\nall_errs$svd &lt;- rmse(all_preds$b, all_preds$svd)\n\nall_errs[1, -c(5, 7)]\n\n       lm   lstsq     neq    chol      lu     qr      svd\n1 40.8369 40.8369 40.8369 40.8369 40.8369 40.8369 40.8369\nThat concludes our tour of important least-squares algorithms. Wrapping up the example, we take a quick look at performance.\n\n\n24.2.9 Checking execution times\nLike I said, the focus in this chapter is on concepts, not performance. But once you work with bigger datasets, you inevitably will care about speed. Also, it’s just interesting to see how fast those methods are! So, let’s do a quick performance benchmark. Just, please, don’t extrapolate from these results – instead, run analogous code on the data you care about.\nTo time them, we need all algorithms encapsulated in their respective functions. Here they are:\n\n# normal equations\nls_normal_eq &lt;- function(A, b) {\n  AtA &lt;- A$t()$matmul(A)\n  x &lt;- linalg_inv(AtA)$matmul(A$t())$matmul(b)\n  x\n}\n\n# normal equations and Cholesky decomposition (done manually)\n# A_t A x = A_t b\n# L L_t x = A_t b\n# L y = A_t b  \n# L_t x = y\nls_cholesky_diy &lt;- function(A, b) {\n  AtA &lt;- A$t()$matmul(A)\n  Atb &lt;- A$t()$matmul(b)\n  L &lt;- linalg_cholesky(AtA)\n  y &lt;- torch_triangular_solve(\n    Atb$unsqueeze(2),\n    L,\n    upper = FALSE\n  )[[1]]\n  x &lt;- torch_triangular_solve(y, L$t())[[1]]\n  x\n}\n\n# torch's Cholesky solver\nls_cholesky_solve &lt;- function(A, b) {\n  AtA &lt;- A$t()$matmul(A)\n  Atb &lt;- A$t()$matmul(b)\n  L &lt;- linalg_cholesky(AtA)\n  x &lt;- torch_cholesky_solve(Atb$unsqueeze(2), L)\n  x\n}\n\n# normal equations and LU factorization (done manually)\n# A_t A x = A_t b\n# P L U x = A_t b\n# L y = P_t A_t b          # where y = U x\n# U x = y\nls_lu_diy &lt;- function(A, b) {\n  AtA &lt;- A$t()$matmul(A)\n  Atb &lt;- A$t()$matmul(b)\n  lu &lt;- torch_lu(AtA)\n  c(P, L, U) %&lt;-% torch_lu_unpack(lu[[1]], lu[[2]]) \n  Atb &lt;- P$t()$matmul(Atb)\n  y &lt;- torch_triangular_solve(\n    Atb$unsqueeze(2),\n    L,\n    upper = FALSE\n  )[[1]]\n  x &lt;- torch_triangular_solve(y, U)[[1]]\n  x\n}\n\n# torch's LU solver\nls_lu_solve &lt;- function(A, b) {\n  AtA &lt;- A$t()$matmul(A) \n  Atb &lt;- A$t()$matmul(b)\n  lu &lt;- torch_lu(AtA)\n  m = lu[[1]]\n  pivots = lu[[2]]\n  x &lt;- torch_lu_solve(Atb$unsqueeze(2), m, pivots)\n  x\n}\n\n# QR factorization\n# A x = b\n# Q R x = b\n# R x = Q_t b \nls_qr &lt;- function(A, b) {\n  c(Q, R) %&lt;-% linalg_qr(A)\n  Qtb &lt;- Q$t()$matmul(b)\n  x &lt;- torch_triangular_solve(Qtb$unsqueeze(2), R)[[1]]\n  x\n}\n\n# SVD\n# A x = b\n# U S V_ x = b\n# S V_t x = U_t b\n# S y = U_t b \n# V_t x = y\nls_svd &lt;- function(A, b) {\n  c(U, S, Vt) %&lt;-% linalg_svd(A, full_matrices = FALSE)\n  Utb &lt;- U$t()$matmul(b)\n  y &lt;- Utb / S\n  x &lt;- Vt$t()$matmul(y)\n  x\n}\n\n# torch's general least squares solver\nls_lstsq &lt;- function(A, b) {\n  x &lt;- linalg_lstsq(A, b)\n  x\n}\n\nWe use the bench package to profile those methods. The mark() function does a lot more than just track time; however, here we just take a glance at the distributions of execution times (fig. 24.1):\n\nset.seed(777)\ntorch_manual_seed(777)\nlibrary(bench)\nlibrary(ggplot2)\n\nres &lt;- mark(ls_normal_eq(A, b),\n            ls_cholesky_diy(A, b),\n            ls_cholesky_solve(A, b),\n            ls_lu_diy(A, b),\n            ls_lu_solve(A, b),\n            ls_qr(A, b),\n            ls_svd(A, b),\n            ls_lstsq(A, b)$solution,\n            min_iterations = 1000)\n\nautoplot(res, type = \"ridge\") + theme_minimal()\n\n\n\n\nFigure 24.1: Timing least-squares algorithms, by example.\n\n\nIn conclusion, we saw how different ways of factorizing a matrix can help in solving least squares problems. We also quickly showed a way to time those strategies; however, speed is not all that counts. We want the solution to be reliable, as well. The technical term here is stability."
  },
  {
    "objectID": "matrix_computations_leastsquares.html#a-quick-look-at-stability",
    "href": "matrix_computations_leastsquares.html#a-quick-look-at-stability",
    "title": "24  Matrix computations: Least-squares problems",
    "section": "24.3 A quick look at stability",
    "text": "24.3 A quick look at stability\nWe’ve already talked about condition numbers. The concept of stability is similar in spirit, but refers to an algorithm instead of a matrix. In both cases, the idea is that small changes in the input to a calculation should lead to small changes in the output. Whole books have been dedicated to this topic, so I’ll refrain from going into details2.\nInstead, I’ll use an example of an ill-conditioned least-squares problem – meaning, the matrix is ill-conditioned – for us to form an idea about the stability of the algorithms we’ve discussed3.\nThe matrix of predictors is a 100 x 15 Vandermonde matrix, created like so:\n\nset.seed(777)\ntorch_manual_seed(777)\n\nm &lt;- 100\nn &lt;- 15\nt &lt;- torch_linspace(0, 1, m)$to(dtype = torch_double())\n\nA &lt;- torch_vander(t, N = n, increasing = TRUE)$to(\n  dtype = torch_double()\n)\n\nCondition number is very high:\n\nlinalg_cond(A)\n\ntorch_tensor\n2.27178e+10\n[ CPUDoubleType{} ]\nEven higher is the condition number obtained when we multiply it with its transpose – remember that some algorithms actually need to work with this matrix:\n\nlinalg_cond(A$t()$matmul(A))\n\ntorch_tensor\n7.27706e+17\n[ CPUDoubleType{} ]\nNext, we have the prediction target:\n\nb &lt;- torch_exp(torch_sin(4*t))\nb &lt;- b/2006.787453080206\n\nIn our example above, we ended up with the same RMSE for all methods. It will be interesting to see what happens here. I’ll restrict myself to the “DIY” ones among the methods shown before. Here they are, listed again for convenience:\n\n# normal equations\nls_normal_eq &lt;- function(A, b) {\n  AtA &lt;- A$t()$matmul(A)\n  x &lt;- linalg_inv(AtA)$matmul(A$t())$matmul(b)\n  x\n}\n\n# normal equations and Cholesky decomposition (done manually)\n# A_t A x = A_t b\n# L L_t x = A_t b\n# L y = A_t b  \n# L_t x = y\nls_cholesky_diy &lt;- function(A, b) {\n  # add a small multiple of the identity matrix \n  # to counteract numerical instability\n  # if Cholesky decomposition fails in your \n  # setup, increase eps\n  eps &lt;- 1e-10\n  id &lt;- eps * torch_diag(torch_ones(dim(A)[2]))\n  AtA &lt;- A$t()$matmul(A) + id\n  Atb &lt;- A$t()$matmul(b)\n  L &lt;- linalg_cholesky(AtA)\n  y &lt;- torch_triangular_solve(\n    Atb$unsqueeze(2),\n    L,\n    upper = FALSE\n  )[[1]]\n  x &lt;- torch_triangular_solve(y, L$t())[[1]]\n  x\n}\n\n# normal equations and LU factorization (done manually)\n# A_t A x = A_t b\n# P L U x = A_t b\n# L y = P_t A_t b          # where y = U x\n# U x = y\nls_lu_diy &lt;- function(A, b) {\n  AtA &lt;- A$t()$matmul(A)\n  Atb &lt;- A$t()$matmul(b)\n  lu &lt;- torch_lu(AtA)\n  c(P, L, U) %&lt;-% torch_lu_unpack(lu[[1]], lu[[2]]) \n  Atb &lt;- P$t()$matmul(Atb)\n  y &lt;- torch_triangular_solve(\n    Atb$unsqueeze(2),\n    L,\n    upper = FALSE\n  )[[1]]\n  x &lt;- torch_triangular_solve(y, U)[[1]]\n  x\n}\n\n# QR factorization\n# A x = b\n# Q R x = b\n# R x = Q_t b \nls_qr &lt;- function(A, b) {\n  c(Q, R) %&lt;-% linalg_qr(A)\n  Qtb &lt;- Q$t()$matmul(b)\n  x &lt;- torch_triangular_solve(Qtb$unsqueeze(2), R)[[1]]\n  x\n}\n\n# SVD\n# A x = b\n# U S V_ x = b\n# S V_t x = U_t b\n# S y = U_t b \n# V_t x = y\nls_svd &lt;- function(A, b) {\n  c(U, S, Vt) %&lt;-% linalg_svd(A, full_matrices = FALSE)\n  Utb &lt;- U$t()$matmul(b)\n  y &lt;- Utb / S\n  x &lt;- Vt$t()$matmul(y)\n  x\n}\n\nLet’s see, then!\n\nalgorithms &lt;- c(\n  \"ls_normal_eq\",\n  \"ls_cholesky_diy\",\n  \"ls_lu_diy\",\n  \"ls_qr\",\n  \"ls_svd\"\n)\n\nrmses &lt;- purrr::map(\n  algorithms,\n  function(m) {\n    rmse(\n      as.numeric(b),\n      as.numeric(A$matmul(get(m)(A, b)))\n    )\n  }\n)\n\nrmse_df &lt;- data.frame(\n  method = algorithms,\n  rmse = unlist(rmses)\n)\n\nrmse_df\n\n           method         rmse\n1    ls_normal_eq 2.882399e-03\n2 ls_cholesky_diy 1.373906e-06\n3       ls_lu_diy 1.274305e-07\n4           ls_qr 3.436749e-08\n5          ls_svd 3.436749e-08\nThis is pretty impressive! We clearly see how the normal equations, straightforward though they are, may not be the best option once problems cease to be well-conditioned. Cholesky as well as LU decomposition fare better; however, the clear “winners” are QR factorization and the SVD. No wonder those two (with two variants each) are the ones made use of by linalg_lstsq().\n\n\n\n\nCho, Dongjin, Cheolhee Yoo, Jungho Im, and Dong-Hyun Cha. 2020. “Comparative Assessment of Various Machine Learning-Based Bias Correction Methods for Numerical Weather Prediction Model Forecasts of Extreme Air Temperatures in Urban Areas.” Earth and Space Science 7 (4): e2019EA000740. https://doi.org/https://doi.org/10.1029/2019EA000740.\n\n\nTrefethen, Lloyd N., and David Bau. 1997. Numerical Linear Algebra. SIAM."
  },
  {
    "objectID": "matrix_computations_leastsquares.html#footnotes",
    "href": "matrix_computations_leastsquares.html#footnotes",
    "title": "24  Matrix computations: Least-squares problems",
    "section": "",
    "text": "The documentation for driver cited above is basically an excerpt from the corresponding documentation in LAPACK, as we can easily verify, since the page in question has conveniently been linked in the documentation for linalg_lstsq().↩︎\nTo learn more, consider consulting one of those books, for example, the widely-used (and concise) treatment by Trefethen and Bau (1997).↩︎\nThe example is taken from the book by Trefethen and Bau referred to in the footnote above. Credits to Rachel Thomas, who brought this to my attention by virtue of using it in her numerical linear algebra course.↩︎"
  },
  {
    "objectID": "matrix_computations_convolution.html#why-convolution",
    "href": "matrix_computations_convolution.html#why-convolution",
    "title": "25  Matrix computations: Convolution",
    "section": "25.1 Why convolution?",
    "text": "25.1 Why convolution?\nIn signal processing, filters are used to modify a signal in some desired way – for example, to cut off the high frequencies. Imagine you have the Fourier-transformed representation of a time series; meaning, a set of frequencies with associated magnitudes and phases. You’d like to set all frequencies higher than some threshold to zero. The easiest way is to multiply the set of frequencies by a sequence of ones and zeroes. If you do that, filtering is happening in the frequency domain, and often, that’s by far the most convenient way.\nWhat, though, if the same result should be achieved in the time domain – that is, working with the raw time series? In that case, you’d have to find the time-domain representation of the filter (achieved by the Inverse Fourier Transform). This representation would then have to be convolved with the time series. Put differently, convolution in the time domain corresponds to multiplication in the frequency domain. This basic fact gets made use of all the time.\nNow, let’s try to understand better what convolution does, and how it is implemented. We begin with a single dimension, and then, explore a bit of what happens in the two-dimensional case."
  },
  {
    "objectID": "matrix_computations_convolution.html#convolution-in-one-dimension",
    "href": "matrix_computations_convolution.html#convolution-in-one-dimension",
    "title": "25  Matrix computations: Convolution",
    "section": "25.2 Convolution in one dimension",
    "text": "25.2 Convolution in one dimension\nWe start by creating a simple signal, x, and a simple filter, h. That choice of variable names is not a whim; in signal processing, \\(h\\) is the usual symbol denoting the impulse response, a term we’ll get to very soon.\n\nlibrary(torch)\n\nx &lt;- torch_arange(start = 1, end = 4) \nh &lt;- torch_tensor(c(-1, 0, 1))\n\nNow – given that we do have torch_conv1d() available – why don’t we call it and see what happens? The way convolution is defined, output length equals input length plus filter length, minus one. Using torch_conv1d(), to obtain length-six output, given a filter of length three, we need to pad it by two on both sides.\nIn the following code, don’t let the calls to view() distract you – they’re present only due to torch expecting three-dimensional input, with dimensions one and two relating to batch item and channel, as usual.\n\ntorch_conv1d(\n  x$view(c(1, 1, 4)),\n  h$view(c(1, 1, 3)),\n  padding = 2\n)\n\ntorch_tensor\n(1,.,.) = \n  1  2  2  2 -3 -4\n[ CPUFloatType{1,1,6} ]\nBut wait, you’ll be thinking – didn’t we say that what torch_conv1d() computes is cross-correlation, not convolution? Well, R has convolve() – let’s double-check:1\n\nx_ &lt;- as.numeric(x)\nh_ &lt;- as.numeric(h)\n\nconvolve(x_, h_, type = \"open\")\n\n[1]  1  2  2  2 -3 -4\nThe result is the same. However, looking into the documentation for convolve(), we see:\n\nNote that the usual definition of convolution of two sequences x and y is given by convolve(x, rev(y), type = \"o\").\n\nEvidently, we need to reverse the order of items in the filter:\n\nconvolve(x_, rev(h_), type = \"open\")\n\n[1] -1 -2 -2 -2  3  4\nIndeed, the result is different now. Let’s do the same with torch_conv1d():\n\ntorch_conv1d(\n  x$view(c(1, 1, 4)),\n  h$flip(1)$view(c(1, 1, 3)),\n  padding = 2\n)\n\ntorch_tensor\n(1,.,.) = \n -1 -2 -2 -2  3  4\n[ CPUFloatType{1,1,6} ]\nAgain, the outcome is the same between torch and R. So: That laconic phrase, found in the “Details” section of convolve()’s documentation, captures the complete difference between cross-correlation and convolution: In convolution, the second argument is reversed. Or flipped, in signal-processing speak. (“Flipped”, indeed, happens to be a far better term, since it generalizes to higher dimensions.)\nTechnically, the difference is tiny – just a change in sign. But mathematically, it is essential – in the sense that it directly derives from what a filter is, and what it does. We’ll be able to get some insight into this soon.\nThe operation underlying convolution can be pictured in two ways.\n\n25.2.1 Two ways to think about convolution\nFor one, we can look at a single output value and determine how it comes about. That is, we ask which input elements contribute to its value, and how those are being combined. This may be called the “output view”, and it’s one we’re already familiar with from cross-correlation.\nAs to cross-correlation, we described it like this. A filter “slides” over an image, and at each image location (pixel), we sum up the products of surrounding input pixels with the corresponding “overlayed” filter values. Put differently, each output pixel results from computing the dot product between matched input and filter values.\nThe second way of looking at things is from the point of view of the input (named, accordingly, the “input view”). It asks: In what way does each input value contribute to the output? This view takes some more getting-used-to than the first; but maybe that’s just a matter of socialization – the manner in which the topic is usually presented in a neural-networks context. In any case, the input view is highly instructive, in that it allows us to learn about the mathematical meaning of convolution.\nWe’re going to look at both, starting with more familiar one, the output view.\n\n25.2.1.1 Output view\nIn the output view, we start by padding the input signal on both sides, just like we did when calling torch_conv2d() with padding = 2. As required, we flip the impulse response, turning it into 1, 0, -1. Then, we picture the “sliding”.\nBelow, you find this visualized in tabular form (tbl. 25.1). The bottom row holds the result, obtained from summing up the individual products at each position.\n\n\nTable 25.1: Convolution: Output view.\n\n\nSignal\nFlipped IR\n\n\n\n\n\n\n\n\n\n0\n1\n\n\n\n\n\n\n\n0\n0\n1\n\n\n\n\n\n\n1\n-1\n0\n1\n\n\n\n\n\n2\n\n-1\n0\n1\n\n\n\n\n3\n\n\n-1\n0\n1\n\n\n\n4\n\n\n\n-1\n0\n1\n\n\n0\n\n\n\n\n-1\n0\n\n\n0\n\n\n\n\n\n-1\n\n\nResult\n-1\n-2\n-2\n-2\n3\n4\n\n\n\n\nAfter all we’ve said on the topic, this depiction should offer few surprises. On to the input view.\n\n\n25.2.1.2 Input view\nThe essential thing about the input view is the way we conceptualize the input signal: Each individual element is seen as a – scaled and shifted – impulse.\nThe impulse is given by the unit sample (or: impulse) function, delta (\\(\\delta\\)). This function is zero everywhere, except at zero, where its value is one:\n\\[\n\\delta [n]={\\begin{cases}1\\ \\ \\ if \\ n=0\\\\0\\ \\ \\ if \\ n \\ne 0\\end{cases}}\n\\]\nThis is like a Kronecker delta, \\(\\delta_{ij}\\)2, with one of the indices being fixed at 0:\n\\[\n\\delta [n]= \\delta _{n0}= \\delta _{0n}\n\\]\nThus, equipped with only that function, \\(\\delta[n]\\) – with \\(n\\) representing discrete time, say – we can represent exactly one signal value, the one at time \\(n = 0\\)3, and its only possible value is 1. Now we add to this the operations scale and shift.\n\nBy scaling, we can produce any value at \\(n = 0\\); for example: \\(x_0 = 0 * \\delta [n]\\).\nBy shifting, we can affect values at other points in time. For example, time \\(n = 3\\) can be addressed as \\(\\delta [n - 3]= \\delta _{n3}\\), since \\(n - 3 = 0\\).\nCombining both, we can represent any value at any point in time. For example: \\(x_5 = 1.11 * \\delta [n - 5]\\).\n\nSo far, we’ve talked just about the signal. What about the filter? Just like the impulse is essential in characterizing a signal, a filter is completely described by its impulse response4. The impulse response, by definition, is what comes out when the input is an impulse (that is, happens at time \\(n = 0\\)). In notation analogous to that used for the signal, with \\(h\\) denoting the impulse response, we have:\n\\[\nh[n] = h[n- 0] \\equiv h(\\delta[n- 0])\n\\]\nIn our example, that would be the sequence -1, 0, 1. But just like the signal needs to be represented at additional times, not just at \\(0\\), the filter has to be applicable to other positions, as well. To that purpose, again, a shift operation is employed, and it is formalized in an analogous way: For instance, \\(h[n - 1]\\) means the filter is applied to time \\(1\\), the time when \\(n - 1\\) equals zero. These shifts correspond to what we informally refer to as “sliding”.\nNow, all that remains to be done is combine the pieces. At time \\(n = 0\\), we take the un-shifted impulse response, and scale it by the amplitude of the signal. In our example, that value was \\(1\\). Thus: \\(1 * h[n - 0] = 1 * [-1, 0, 1] = [-1, 0, 1]\\). For the other times, we shift the impulse response to the input position in question, and multiply. Finally, once we’ve obtained all contributions from all input positions, we add them up, thus obtaining the convolved output.\nThe following table aims to illustrate that (tbl. 25.2):\n\n\nTable 25.2: Convolution: Input view.\n\n\nSignal\nImpulse response\nProduct\n\n\n\n\n1\nh[n - 0]\n-1  0  1  0  0  0\n\n\n2\nh[n - 1]\n0 -2  0  2  0  0\n\n\n3\nh[n - 2]\n0  0 -3  0  3  0\n\n\n4\nh[n - 3]\n0  0  0 -4  0  4\n\n\nSum\n\n-1 -2 -2 -2  3  4\n\n\n\n\nPersonally, while I do find the output view easier to grasp, I feel I can derive more insight from the input view. In particular, it answers the – unavoidable – question: So why do we flip the impulse response?\nIt turns out that, far from being due to whatever mysterious forces, the minus sign is merely a mechanical outcome of the way signals are represented: The signal measured at time \\(n = 2\\) is denoted by \\(\\delta [n - 2]\\) (two minus two yielding zero); and the filter applied to that signal, accordingly, as \\(h[n -2]\\).\n\n\n\n25.2.2 Implementation\nFrom the way I’ve described the output view, you may well think there’s not much to say about how to code this. It looks straightforward: Loop over the input vector, and compute the dot product at every prospective output position. But that would mean calculating many vector products, the more, the longer the input sequence.\nFortunately, there is a better way. Single-dimension (linear) convolution is computed by means of Toeplitz matrices, matrices that have some number of constant diagonals, and values of zero everywhere else. Once the filter has been formulated as a Toeplitz matrix, there is just a single multiplication to be carried out: that of the Toeplitz matrix and the input. And even though the matrix will need to have as many columns as the input has values (otherwise we couldn’t do the multiplication), computational cost is small due to the matrix’s being “nearly empty”.\nHere is such a Toeplitz matrix, constructed for our running example:\n\nh &lt;-torch_tensor(\n  rbind(c(-1, 0, 0, 0),\n        c(0, -1, 0, 0),\n        c(1, 0, -1, 0),\n        c(0, 1, 0, -1),\n        c(0, 0, 1, 0),\n        c(0, 0, 0, 1)\n        ))\nh\n\ntorch_tensor\n-1  0  0  0\n 0 -1  0  0\n 1  0 -1  0\n 0  1  0 -1\n 0  0  1  0\n 0  0  0  1\n[ CPUFloatType{6,4} ]\nLet’s check that multiplication with our example input yields the expected result:\n\nh$matmul(x)\n\ntorch_tensor\n-1\n-2\n-2\n-2\n 3\n 4\n[ CPUFloatType{6} ]\nIt does. Now, let’s move on to two dimensions. Conceptually, there is no difference, but actual computation (both “by hand” and using matrices) gets a lot more involved. Thus, we’ll content ourselves with presenting a (generalizeable) part of the manual calculation, and, in the computational part, don’t aim at elucidating every single detail."
  },
  {
    "objectID": "matrix_computations_convolution.html#convolution-in-two-dimensions",
    "href": "matrix_computations_convolution.html#convolution-in-two-dimensions",
    "title": "25  Matrix computations: Convolution",
    "section": "25.3 Convolution in two dimensions",
    "text": "25.3 Convolution in two dimensions\nTo show how, conceptually, one-dimensional and two-dimensional convolution are analogous, we assume the output view.\n\n25.3.1 How it works (output view)\nThis time, the example input is two-dimensional. It could look like this:\n\\[\n\\begin{bmatrix}\n  1 & 4 & 1\\\\\n  2 & 5 & 3\\\\\n\\end{bmatrix}\n\\]\nThe same goes for the filter. Here is a possible one:\n\\[\n\\begin{bmatrix}\n  1 & 1\\\\\n  1 & -1\\\\\n\\end{bmatrix}\n\\]\nWe take the output view, the one where the filter “slides” over the input. But, to keep things readable, let me just pick a single output value (“pixel”) for demonstration. If the input is of size m1 x n1, and the filter, m2 x n2, the output will have size (m1 + m2 - 1) x (n1 + n2 - 1); thus, it will be 3 x 4 in our case. I’ll pick the value at position (0, 1) – counting rows from the bottom, as is usual in image processing:\n\\[\n\\begin{bmatrix}\n  . & . & . & .\\\\\n  . & . & . & .\\\\\n  . & y_{01} & . & .\\\\\n\\end{bmatrix}\n\\]\nHere is the input, displayed in a table that will allow us to picture elements at non-existing (negative) positions.\n\n\n\nPosition (x/y)\n-1\n0\n1\n2\n\n\n\n\n1\n\n1\n4\n1\n\n\n0\n\n2\n5\n3\n\n\n-1\n\n\n\n\n\n\n\nAnd here, the filter, with values arranged correspondingly:\n\n\n\nPosition (x/y)\n-1\n0\n1\n2\n\n\n\n\n1\n\n1\n1\n\n\n\n0\n\n1\n-1\n\n\n\n-1\n\n\n\n\n\n\n\nAs in the one-dimensional case, the first thing to be done is flip the filter. Flipping here means rotation by hundred-eighty degrees.\n\n\n\nPosition (x/y)\n-1\n0\n1\n2\n\n\n\n\n1\n\n\n\n\n\n\n0\n-1\n1\n\n\n\n\n-1\n1\n1\n\n\n\n\n\nNext, the filter is shifted to the desired output position. What we want to do is shift to the right by one, leaving unaffected vertical position.\n\n\n\nPosition (x/y)\n-1\n0\n1\n2\n\n\n\n\n1\n\n\n\n\n\n\n0\n\n-1\n1\n\n\n\n-1\n\n1\n1\n\n\n\n\nNow we are all set to compute the output value at position (0, 1). It’s the dot product of all overlapping image and filter values:\n\n\n\nPosition (x/y)\n-1\n0\n1\n2\n\n\n\n\n1\n\n\n\n\n\n\n0\n\n-1*2=-2\n1*5=5\n\n\n\n-1\n\n\n\n\n\n\n\nThe final result, then, is -2 + 5 = 3.\n\\[\n\\begin{bmatrix}\n  . & . & . & .\\\\\n  . & . & . & .\\\\\n  . & 3 & . & .\\\\\n\\end{bmatrix}\n\\]\nAll values still missing can be computed in an analogous way. But we’ll skip that exercise, and take a look at how an actual computation would proceed.\n\n\n25.3.2 Implementation\nThe way two-dimensional convolution is actually implemented in code again involves Toeplitz matrices. Like I already said, we won’t go into why exactly every step takes the exact form it takes – the intent here is to show a working example, an example you could build on, if you wanted, for your own explorations.\n\n25.3.2.1 Step one: Prepare filter matrix\nWe start by padding the filter to the output size, 3 x 4.\n0  0 0 0\n1  1 0 0\n1 -1 0 0\nWe then create a Toeplitz matrix for every row in the filter, starting at the bottom.\n# H0\n 1  0  0  \n-1  1  0  \n 0 -1  1  \n 0  0 -1  \n \n# H1\n 1  0  0  \n 1  1  0  \n 0  1  1  \n 0  0  1  \n \n# H2\n 0  0  0  \n 0  0  0  \n 0  0  0  \nIn code, we have:\n\nH0 &lt;- torch_tensor(\n  cbind(\n    c(1, -1, 0, 0),\n    c(0, 1, -1, 0),\n    c(0, 0, 1, -1)\n  )\n)\n\nH1 &lt;- torch_tensor(\n  cbind(\n    c(1, 1, 0, 0),\n    c(0, 1, 1, 0),\n    c(0, 0, 1, 1)\n  )\n)\n\nH2 &lt;- torch_tensor(0)$unsqueeze(1)\n\nNext, these three matrices are assembled so as to form a doubly-blocked Toeplitz matrix. Like so:\nH0   0\nH1  H0\nH2  H1\nOne way of coding this is to (twice) use torch_block_diag() to build up the two non-zero blocks, and concatenate them:\n\nH &lt;- torch_cat(\n  list(\n    torch_block_diag(list(H0, H0)), torch_zeros(4, 6)\n  )\n) +\n  torch_cat(\n    list(\n      torch_zeros(4, 6),\n      torch_block_diag(list(H1, H1))\n    )\n  )\n\nH\n\ntorch_tensor\n 1  0  0  0  0  0\n-1  1  0  0  0  0\n 0 -1  1  0  0  0\n 0  0 -1  0  0  0\n 1  0  0  1  0  0\n 1  1  0 -1  1  0\n 0  1  1  0 -1  1\n 0  0  1  0  0 -1\n 0  0  0  1  0  0\n 0  0  0  1  1  0\n 0  0  0  0  1  1\n 0  0  0  0  0  1\n[ CPUFloatType{12,6} ]\nThe final matrix has two non-zero “bands”, separated by two all-zero diagonals. This is the final form of the filter needed for matrix multiplication.\n\n\n25.3.2.2 Step two: Prepare input\nTo be multiplicable with this 12 x 6 matrix, the input needs to be flattened into a vector. Again, we proceed row-by-row, starting from the bottom here as well.\n\nx0 &lt;- torch_tensor(c(2, 5, 3)) \nx1 &lt;- torch_tensor(c(1, 4, 1))\n\nx &lt;- torch_cat(list(x0, x1))\nx\n\ntorch_tensor\n 2\n 5\n 3\n 1\n 4\n 1\n[ CPUFloatType{6} ]\n\n\n25.3.2.3 Step three: Multiply\nBy now, convolution has morphed into straightforward matrix multiplication:\n\ny &lt;- H$matmul(x)\ny\n\ntorch_tensor\n  2\n  3\n -2\n -3\n  3\n 10\n  5\n  2\n  1\n  5\n  5\n  1\n[ CPUFloatType{12} ]\nAll that remains to be done is reshape the output into the correct two-dimensional structure. Building up the rows in order (again, bottom-first) we obtain:\n\\[\n\\begin{bmatrix}\n  1 & 5 & 5 & 1\\\\\n  3 & 10 & 5 & 2\\\\\n  2 & 3 & -2 & -3\\\\\n\\end{bmatrix}\n\\]\nLooking at element (0, 1), we see that the computation confirms our manual calculation.\nHerewith, we conclude the topic of matrix computations with torch. But, as we move on to our next topic, the Fourier transform, we won’t actually stray that far away. Remember how, above, we said that time-domain convolution corresponds to frequency-domain multiplication?\nThis correspondence is often used to speed up computation: The input data are Fourier-transformed, the result is multiplied by the filter, and the filtered frequency-domain representation is transformed back again. Just have a look at the documentation for R’s convolve(). It directly starts out stating:\n\nUse the Fast Fourier Transform to compute the several kinds of convolutions of two sequences.\n\nOn to the Fourier Transform, then!"
  },
  {
    "objectID": "matrix_computations_convolution.html#footnotes",
    "href": "matrix_computations_convolution.html#footnotes",
    "title": "25  Matrix computations: Convolution",
    "section": "",
    "text": "The argument type = \"open\" is passed to request linear, not circular, convolution.↩︎\nThe Kronecker delta, \\(\\delta_{ij}\\), evaluates to one if \\(i\\) equals \\(j\\), and to zero, otherwise.↩︎\nI’m using \\(n\\), instead of \\(t\\), to index into different positions, because the signal – like any digitized one – only “exists” at discrete points in time (or space). In some contexts, this reads a bit awkward, but it at least is consistent.↩︎\nLike everywhere in the chapter, when I talk of filters, I think of linear time-invariant systems only. The restriction to time-invariant systems is immanent in the convolution operation.↩︎"
  },
  {
    "objectID": "fourier_transform_dft.html#understanding-the-output-of-torch_fft_fft",
    "href": "fourier_transform_dft.html#understanding-the-output-of-torch_fft_fft",
    "title": "26  Exploring the Discrete Fourier Transform (DFT)",
    "section": "26.1 Understanding the output of torch_fft_fft()",
    "text": "26.1 Understanding the output of torch_fft_fft()\nOur explorations take off hands-on: We call the main function associated with the (forward) Fourier Transform, torch_fft_fft() 1, and see if we can make sense of its output.\nAs we care about actual understanding, we start from the simplest possible example signal, a pure cosine that performs one revolution over the complete sampling period.\n\n26.1.1 Starting point: A cosine of frequency 1\nThe way we set things up, there will be sixty-four samples; the sampling period thus equals N = 64. The content of frequency(), the below helper function used to construct the signal, reflects how we represent the cosine. Namely:\n\\[\nf(x) = cos(\\frac{2 \\pi}{N} \\ k \\ x)\n\\]\nHere \\(x\\) values progress over time (or space), and \\(k\\) is the frequency index. A cosine is periodic with period \\(2 \\pi\\); so if we want it to first return to its starting state after sixty-four samples, and \\(x\\) runs between zero and sixty-three, we’ll want \\(k\\) to be equal to \\(1\\). Like that, we’ll reach the initial state again at position \\(x = \\frac{2 \\pi}{64} * 1 * 64\\).\n\nlibrary(torch)\nlibrary(ggplot2)\nlibrary(patchwork)\n\nN &lt;- 64\nsample_positions &lt;- 0:(N - 1)\n\nfrequency &lt;- function(k, N) {\n  (2 * pi / N) * k\n}\n\nx &lt;- torch_cos(frequency(1, N) * sample_positions)\n\nLet’s quickly confirm this did what it was supposed to (fig. 26.1):\n\ndf &lt;- data.frame(x = sample_positions, y = as.numeric(x))\n\nggplot(df, aes(x = x, y = y)) +\n  geom_line() +\n  xlab(\"time\") +\n  ylab(\"amplitude\") +\n  theme_minimal()\n\n\n\n\nFigure 26.1: Pure cosine that accomplishes one revolution over the complete sample period (64 samples).\n\n\nNow that we have the input signal, torch_fft_fft() computes for us the Fourier coefficients, that is, the importance of the various frequencies present in the signal. The number of frequencies considered will equal the number of sampling points: So \\(X\\) will be of length sixty-four as well.\n(In our example, you’ll notice that the second half of coefficients will equal the first in magnitude.2 This is the case for every real-valued signal. In such cases, you could call torch_fft_rfft() instead, which yields “nicer” (in the sense of shorter) vectors to work with. Here though, I want to explain the general case, since that’s what you’ll find done in most expositions on the topic.)\n\nFt &lt;- torch_fft_fft(x)\n\nEven with the signal being real, the Fourier coefficients are complex numbers. There are four ways to inspect them. The first is to extract the real part:\n\nreal_part &lt;- Ft$real\nas.numeric(real_part) %&gt;% round(5)\n\n[1]  0 32 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n[29] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n[57] 0 0 0 0 0 0 0 32\nOnly a single coefficient is non-zero, the one at position 1. (We start counting from zero, and may discard the second half, as explained above.)\nNow looking at the imaginary part, we find it is zero throughout:\n\nimag_part &lt;- Ft$imag\nas.numeric(imag_part) %&gt;% round(5)\n\n[1]  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n[29] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n[57] 0 0 0 0 0 0 0 0\nAt this point we know that there is just a single frequency present in the signal, namely, that at \\(k = 1\\). This matches (and it better had to) the way we constructed the signal: namely, as accomplishing a single revolution over the complete sampling period.\nSince, in theory, every coefficient could have non-zero real and imaginary parts, often what you’d report is the magnitude (the square root of the sum of squared real and imaginary parts):\n\nmagnitude &lt;- torch_abs(Ft)\nas.numeric(magnitude) %&gt;% round(5)\n\n[1]  0 32 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n[29] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n[57] 0 0 0 0 0 0 0 32\nUnsurprisingly, these values exactly reflect the respective real parts.\nFinally, there’s the phase, indicating a possible shift of the signal (a pure cosine is unshifted). In torch, we have torch_angle() complementing torch_abs(), but we need to take into account roundoff error here. We know that in each but a single case, the real and imaginary parts are both exactly zero; but due to finite precision in how numbers are presented in a computer, the actual values will often not be zero. Instead, they’ll be very small. If we take one of these “fake non-zeroes” and divide it by another, as happens in the angle calculation, big values can result. To prevent this from happening, our custom implementation rounds both inputs before triggering the division.\n\nphase &lt;- function(Ft, threshold = 1e5) {\n  torch_atan2(\n    torch_abs(torch_round(Ft$imag * threshold)),\n    torch_abs(torch_round(Ft$real * threshold))\n  )\n}\n\nas.numeric(phase(Ft)) %&gt;% round(5)\n\n[1]  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n[29] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n[57] 0 0 0 0 0 0 0 0\nAs expected, there is no phase shift in the signal.\nLet’s visualize what we found (fig. 26.2).\n\ncreate_plot &lt;- function(x, y, quantity) {\n  df &lt;- data.frame(\n    x_ = x,\n    y_ = as.numeric(y) %&gt;% round(5)\n  )\n  ggplot(df, aes(x = x_, y = y_)) +\n    geom_col() +\n    xlab(\"frequency\") +\n    ylab(quantity) +\n    theme_minimal()\n}\n\np_real &lt;- create_plot(\n  sample_positions,\n  real_part,\n  \"real part\"\n)\np_imag &lt;- create_plot(\n  sample_positions,\n  imag_part,\n  \"imaginary part\"\n)\np_magnitude &lt;- create_plot(\n  sample_positions,\n  magnitude,\n  \"magnitude\"\n)\np_phase &lt;- create_plot(\n  sample_positions,\n  phase(Ft),\n  \"phase\"\n)\n\np_real + p_imag + p_magnitude + p_phase\n\n\n\n\nFigure 26.2: Real parts, imaginary parts, magnitudes and phases of the Fourier coefficients, obtained on a pure cosine that performs a single revolution over the sampling period. Imaginary parts as well as phases are all zero.\n\n\nIt’s fair to say that we have no reason to doubt what torch_fft_fft() has done. But with a pure sinusoid like this, we can understand exactly what’s going on by computing the DFT ourselves, by hand. Doing this now will significantly help us later, when we’re writing the code.\n\n\n26.1.2 Reconstructing the magic\nOne caveat about this section. With a topic as rich as the Fourier Transform, and an audience who I imagine to vary widely on a dimension of math and sciences education, my chances to meet your expectations, dear reader, must be very close to zero. Still, I want to take the risk. If you’re an expert on these things, you’ll anyway be just scanning the text, looking out for pieces of torch code. If you’re moderately familiar with the DFT, you may still like being reminded of its inner workings. And – most importantly – if you’re rather new, or even completely new, to this topic, you’ll hopefully take away (at least) one thing: that what seems like one of the greatest wonders of the universe (assuming there is a reality somehow corresponding to what goes on in our minds) may well be a wonder, but neither “magic” nor a thing reserved to the initiated.\nIn a nutshell, the Fourier Transform is a basis transformation. In the case of the DFT – the Discrete Fourier Transform, where time and frequency representations both are finite vectors, not functions – the new basis looks like this:\n\\[\n\\begin{aligned}\n&\\mathbf{w}^{0n}_N = e^{i\\frac{2 \\pi}{N}* 0 * n} = 1\\\\\n&\\mathbf{w}^{1n}_N = e^{i\\frac{2 \\pi}{N}* 1 * n} = e^{i\\frac{2 \\pi}{N} n}\\\\\n&\\mathbf{w}^{2n}_N = e^{i\\frac{2 \\pi}{N}* 2 * n} = e^{i\\frac{2 \\pi}{N}2n}\\\\& ... \\\\\n&\\mathbf{w}^{(N-1)n}_N = e^{i\\frac{2 \\pi}{N}* (N-1) * n} = e^{i\\frac{2 \\pi}{N}(N-1)n}\\\\\n\\end{aligned}\n\\]\nHere \\(N\\), as before, is the number of samples (64, in our case); thus, there are \\(N\\) basis vectors. With \\(k\\) running through the basis vectors, they can be written:\n\\[\n\\mathbf{w}^{kn}_N = e^{i\\frac{2 \\pi}{N}k n}\n\\tag{26.1}\\]\nLike \\(k\\), \\(n\\) runs from \\(0\\) to \\(N-1\\). To understand what these basis vectors are doing, it is helpful to temporarily switch to a shorter sampling period, \\(N = 4\\), say. If we do so, we have four basis vectors: \\(\\mathbf{w}^{0n}_N\\), \\(\\mathbf{w}^{1n}_N\\), \\(\\mathbf{w}^{2n}_N\\), and \\(\\mathbf{w}^{3n}_N\\). The first one looks like this:\n\\[\n\\mathbf{w}^{0n}_N\n=\n\\begin{bmatrix}\n   e^{i\\frac{2 \\pi}{4}* 0 * 0}\\\\\n   e^{i\\frac{2 \\pi}{4}* 0 * 1}\\\\\n   e^{i\\frac{2 \\pi}{4}* 0 * 2}\\\\\n   e^{i\\frac{2 \\pi}{4}* 0 * 3}\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n   1\\\\\n   1\\\\\n   1\\\\\n   1\\\\\n\\end{bmatrix}\n\\]\nThe second, like so:\n\\[\n\\mathbf{w}^{1n}_N\n=\n\\begin{bmatrix}\n   e^{i\\frac{2 \\pi}{4}* 1 * 0}\\\\\n   e^{i\\frac{2 \\pi}{4}* 1 * 1}\\\\\n   e^{i\\frac{2 \\pi}{4}* 1 * 2}\\\\\n   e^{i\\frac{2 \\pi}{4}* 1 * 3}\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n   1\\\\\n   e^{i\\frac{\\pi}{2}}\\\\\n   e^{i \\pi}\\\\\n   e^{i\\frac{3 \\pi}{4}}\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n   1\\\\\n   i\\\\\n   -1\\\\\n   -i\\\\\n\\end{bmatrix}\n\\]\nThis is the third:\n\\[\n\\mathbf{w}^{2n}_N\n=\n\\begin{bmatrix}\n   e^{i\\frac{2 \\pi}{4}* 2 * 0}\\\\\n   e^{i\\frac{2 \\pi}{4}* 2 * 1}\\\\\n   e^{i\\frac{2 \\pi}{4}* 2 * 2}\\\\\n   e^{i\\frac{2 \\pi}{4}* 2 * 3}\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n   1\\\\\n   e^{i\\pi}\\\\\n   e^{i 2 \\pi}\\\\\n   e^{i\\frac{3 \\pi}{2}}\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n   1\\\\\n   -1\\\\\n   1\\\\\n   -1\\\\\n\\end{bmatrix}\n\\]\nAnd finally, the fourth:\n\\[\n\\mathbf{w}^{3n}_N\n=\n\\begin{bmatrix}\n   e^{i\\frac{2 \\pi}{4}* 3 * 0}\\\\\n   e^{i\\frac{2 \\pi}{4}* 3 * 1}\\\\\n   e^{i\\frac{2 \\pi}{4}* 3 * 2}\\\\\n   e^{i\\frac{2 \\pi}{4}* 3 * 3}\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n   1\\\\\n   e^{i\\frac{3 \\pi}{2}}\\\\\n   e^{i 3 \\pi}\\\\\n   e^{i\\frac{9 \\pi}{2}}\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n   1\\\\\n   -i\\\\\n   -1\\\\\n   i\\\\\n\\end{bmatrix}\n\\]\nWe can characterize these four basis vectors in terms of their “speed”: how fast they move around the unit circle. To do this, we simply look at the rightmost column vectors, where the final calculation results appear. The values in that column correspond to positions pointed to by the revolving basis vector at different points in time. This means that looking at a single “update of position”, we can see how fast the vector is moving in a single time step.\nLooking first at \\(\\mathbf{w}^{0n}_N\\), we see that it does not move at all. \\(\\mathbf{w}^{1n}_N\\) goes from \\(1\\) to \\(i\\) to \\(-1\\) to \\(-i\\); one more step, and it would be back where it started. That’s one revolution in four steps, or a step size of \\(\\frac{\\pi}{2}\\). Then \\(\\mathbf{w}^{2n}_N\\) goes at double that pace, moving a distance of \\(\\pi\\) along the circle. That way, it ends up completing two revolutions overall. Finally, \\(\\mathbf{w}^{3n}_N\\) achieves three complete loops, for a step size of \\(\\frac{3 \\pi}{2}\\).\nThe thing that makes these basis vectors so useful is that they are mutually orthogonal. That is, their dot product is zero:\n\\[\n\\langle \\mathbf{w}^{kn}_N, \\mathbf{w}^{ln}_N \\rangle \\ = \\ \\sum_{n=0}^{N-1} ({e^{i\\frac{2 \\pi}{N}k n}})^* e^{i\\frac{2 \\pi}{N}l n} = \\ \\sum_{n=0}^{N-1} ({e^{-i\\frac{2 \\pi}{N}k n}})e^{i\\frac{2 \\pi}{N}l n} = 0\n\\tag{26.2}\\]\nLet’s take, for example, \\(\\mathbf{w}^{2n}_N\\) and \\(\\mathbf{w}^{3n}_N\\). Indeed, their dot product evaluates to zero.\n\\[\n\\begin{bmatrix}\n   1 & -1 & 1 & -1\\\\\n\\end{bmatrix}\n\\begin{bmatrix}\n   1\\\\\n   -i\\\\\n   -1\\\\\n   i\\\\\n\\end{bmatrix}\n=\n1 + i + (-1) + (-i)  = 0\n\\]\nNow, we’re about to see how the orthogonality of the Fourier basis substantially simplifies the calculation of the DFT. Did you notice the similarity between these basis vectors and the way we wrote the example signal? Here it is again:\n\\[\nf(x) = cos(\\frac{2 \\pi}{N} k x)\n\\]\nIf we manage to represent this function in terms of the basis vectors \\(\\mathbf{w}^{kn}_N = e^{i\\frac{2 \\pi}{N}k n}\\), the inner product between the function and each basis vector will be either zero (the “default”) or a multiple of one (in case the function has a component matching the basis vector in question). Luckily, sines and cosines can easily be converted into complex exponentials. In our example, this is how that goes:3\n\\[\n\\begin{aligned}\n\\mathbf{x}_n &= cos(\\frac{2 \\pi}{64} n) \\\\\n&= \\frac{1}{2} (e^{i\\frac{2 \\pi}{64} n} + e^{-i\\frac{2 \\pi}{64} n}) \\\\\n&= \\frac{1}{2} (e^{i\\frac{2 \\pi}{64} n} + e^{i\\frac{2 \\pi}{64} 63n}) \\\\\n&= \\frac{1}{2} (\\mathbf{w}^{1n}_N + \\mathbf{w}^{63n}_N)\n\\end{aligned}\n\\]\nHere the first step directly results from Euler’s formula4, and the second reflects the fact that the Fourier coefficients are periodic, with frequency -1 being the same as 63, -2 equaling 62, and so on.\nNow, the \\(k\\)th Fourier coefficient is obtained by projecting the signal onto basis vector \\(k\\).\nDue to the orthogonality of the basis vectors, only two coefficients will not be zero: those for \\(\\mathbf{w}^{1n}_N\\) and \\(\\mathbf{w}^{63n}_N\\). They are obtained by computing the inner product between the function and the basis vector in question, that is, by summing over \\(n\\). For each \\(n\\) ranging between \\(0\\) and \\(N-1\\), we have a contribution of \\(\\frac{1}{2}\\), leaving us with a final sum of \\(32\\) for both coefficients. For example, for \\(\\mathbf{w}^{1n}_N\\):\n\\[\n\\begin{aligned}\nX_1 &= \\langle \\mathbf{w}^{1n}_N, \\mathbf{x}_n \\rangle \\\\\n&= \\langle \\mathbf{w}^{1n}_N, \\frac{1}{2} (\\mathbf{w}^{1n}_N + \\mathbf{w}^{63n}_N) \\rangle \\\\\n&= \\frac{1}{2} * 64 \\\\\n&= 32\n\\end{aligned}\n\\]\nAnd analogously for \\(X_{63}\\).\nNow, looking back at what torch_fft_fft() gave us, we see we were able to arrive at the same result. And we’ve learned something along the way.\nAs long as we stay with signals composed of one or more basis vectors, we can compute the DFT in this way. At the end of the chapter, we’ll develop code that will work for all signals, but first, let’s see if we can dive even deeper into the workings of the DFT. Three things we’ll want to explore:\n\nWhat would happen if frequencies changed – say, a melody were sung at a higher pitch?\nWhat about amplitude changes – say, the music were played twice as loud?\nWhat about phase – e.g., there were an offset before the piece started?\n\nIn all cases, we’ll call torch_fft_fft() only once we’ve determined the result ourselves.\nAnd finally, we’ll see how complex sinusoids, made up of different components, can still be analyzed in this way, provided they can be expressed in terms of the frequencies that make up the basis.\n\n\n26.1.3 Varying frequency\nAssume we quadrupled the frequency, giving us a signal that looked like this:\n\\[\n\\mathbf{x}_n = cos(\\frac{2 \\pi}{N}*4*n)\n\\]\nFollowing the same logic as above, we can express it like so:\n\\[\n\\mathbf{x}_n = \\frac{1}{2} (\\mathbf{w}^{4n}_N + \\mathbf{w}^{60n}_N)\n\\]\nWe already see that non-zero coefficients will be obtained only for frequency indices \\(4\\) and \\(60\\). Picking the former, we obtain\n\\[\n\\begin{aligned}\nX_4 &= \\langle \\mathbf{w}^{4n}_N, \\mathbf{x}_n \\rangle \\\\\n&= \\langle \\mathbf{w}^{4n}_N, \\frac{1}{2} (\\mathbf{w}^{4n}_N + \\mathbf{w}^{60n}_N) \\rangle \\\\\n&= 32\n\\end{aligned}\n\\]\nFor the latter, we’d arrive at the same result.\nNow, let’s make sure our analysis is correct. The following code snippet contains nothing new; it generates the signal, calculates the DFT, and plots them both (fig. 26.3).\n\nx &lt;- torch_cos(frequency(4, N) * sample_positions)\n\nplot_ft &lt;- function(x) {\n\n  df &lt;- data.frame(x = sample_positions, y = as.numeric(x))\n  p_signal &lt;- ggplot(df, aes(x = x, y = y)) +\n    geom_line() +\n    xlab(\"time\") +\n    ylab(\"amplitude\") +\n    theme_minimal()\n\n  # in the code, I'm using Ft instead of X because not\n  # all operating systems treat variables as case-sensitive\n  Ft &lt;- torch_fft_fft(x)\n\n  p_real &lt;- create_plot(\n    sample_positions,\n    Ft$real,\n    \"real part\"\n  )\n  p_imag &lt;- create_plot(\n    sample_positions,\n    Ft$imag,\n    \"imaginary part\"\n  )\n  p_magnitude &lt;- create_plot(\n    sample_positions,\n    torch_abs(Ft),\n    \"magnitude\"\n  )\n  p_phase &lt;- create_plot(\n    sample_positions,\n    phase(Ft),\n    \"phase\"\n  )\n\n  (p_signal | plot_spacer()) /\n    (p_real | p_imag) /\n    (p_magnitude | p_phase)\n}\n\nplot_ft(x)\n\n\n\n\nFigure 26.3: A pure cosine that performs four revolutions over the sampling period, and its DFT. Imaginary parts and phases are still are zero.\n\n\nThis does indeed confirm our calculations.\nA special case arises when signal frequency rises to the highest one “allowed”, in the sense of being detectable without aliasing. That will be the case at one half of the number of sampling points. Then, the signal will look like so:\n\\[\n\\mathbf{x}_n = \\frac{1}{2} (\\mathbf{w}^{32n}_N + \\mathbf{w}^{32n}_N)\n\\]\nConsequently, we end up with a single coefficient, corresponding to a frequency of 32 revolutions per sample period, of double the magnitude (64, thus). Here are the signal and its DFT (fig. 26.4):\n\nx &lt;- torch_cos(frequency(32, N) * sample_positions)\nplot_ft(x)\n\n\n\n\nFigure 26.4: A pure cosine that performs thirty-two revolutions over the sampling period, and its DFT. This is the highest frequency where, given sixty-four sample points, no aliasing will occur. Imaginary parts and phases still zero.\n\n\n\n\n26.1.4 Varying amplitude\nNow, let’s think about what happens when we vary amplitude. For example, say the signal gets twice as loud. Now, there will be a multiplier of 2 that can be taken outside the inner product. In consequence, the only thing that changes is the magnitude of the coefficients.\nLet’s verify this. The modification is based on the example we had before the very last one, with four revolutions over the sampling period (fig. 26.5):\n\nx &lt;- 2 * torch_cos(frequency(4, N) * sample_positions)\nplot_ft(x)\n\n\n\n\nFigure 26.5: Pure cosine with four revolutions over the sampling period, and doubled amplitude. Imaginary parts and phases still zero.\n\n\nSo far, we have not once seen a coefficient with non-zero imaginary part. To change this, we add in phase.\n\n\n26.1.5 Adding phase\nChanging the phase of a signal means shifting it in time. Our example signal is a cosine, a function whose value is 1 at \\(t=0\\). (That also was the – arbitrarily chosen – starting point of the signal.)\nNow assume we shift the signal forward by \\(\\frac{\\pi}{2}\\). Then the peak we were seeing at zero moves over to \\(\\frac{\\pi}{2}\\); and if we still start “recording” at zero, we must find a value of zero there. An equation describing this is the following. For convenience, we assume a sampling period of \\(2 \\pi\\) and \\(k=1\\), so that the example is a simple cosine:\n\\[\nf(x) = cos(x - \\phi)\n\\]\nThe minus sign may look unintuitive at first. But it does make sense: We now want to obtain a value of 1 at \\(x=\\frac{\\pi}{2}\\), so \\(x - \\phi\\) should evaluate to zero. (Or to any multiple of \\(\\pi\\).) Summing up, a delay in time will appear as a negative phase shift.\nNow, we’re going to calculate the DFT for a shifted version of our example signal. But if you like, take a peek at the phase-shifted version of the time-domain picture now already. You’ll see that a cosine, delayed by \\(\\frac{\\pi}{2}\\), is nothing else than a sine starting at 0.\nTo compute the DFT, we follow our familiar-by-now strategy. The signal now looks like this:\n\\[\n\\mathbf{x}_n = cos(\\frac{2 \\pi}{N}*4*x - \\frac{\\pi}{2})\n\\]\nFirst, we express it in terms of basis vectors:\n\\[\n\\begin{aligned}\n\\mathbf{x}_n &= cos(\\frac{2 \\pi}{64} 4 n - \\frac{\\pi}{2}) \\\\\n&= \\frac{1}{2} (e^{i\\frac{2 \\pi}{64} 4n - \\frac{pi}{2}} + e^{i\\frac{2 \\pi}{64} 60n - \\frac{pi}{2}}) \\\\\n&= \\frac{1}{2} (e^{i\\frac{2 \\pi}{64} 4n}  e^{-i \\frac{\\pi}{2}} + e^{i\\frac{2 \\pi}{64} 60n}  e^{i\\frac{pi}{2}}) \\\\\n&= \\frac{1}{2} (e^{-i \\frac{\\pi}{2}} \\mathbf{w}^{4n}_N + e^{i \\frac{\\pi}{2}} \\mathbf{w}^{60n}_N)\n\\end{aligned}\n\\]\nAgain, we have non-zero coefficients only for frequencies \\(4\\) and \\(60\\). But they are complex now, and both coefficients are no longer identical. Instead, one is the complex conjugate of the other. First, \\(X_4\\):\n\\[\n\\begin{aligned}\nX_4 &= \\langle \\mathbf{w}^{4n}_N, \\mathbf{x}_n \\rangle \\\\\n&=\\langle \\mathbf{w}^{4n}_N, \\frac{1}{2} (e^{-i \\frac{\\pi}{2}} \\mathbf{w}^{4n}_N + e^{i \\frac{\\pi}{2}} \\mathbf{w}^{60n}_N) \\rangle\\\\\n&= 32 *e^{-i \\frac{\\pi}{2}} \\\\\n&= -32i\n\\end{aligned}\n\\]\nAnd here, \\(X_{60}\\):\n\\[\n\\begin{aligned}\nX_{60} &= \\langle \\mathbf{w}^{60n}_N, \\mathbf{x}_N \\rangle \\\\\n&= 32 *e^{i \\frac{\\pi}{2}} \\\\\n&= 32i\n\\end{aligned}\n\\]\nAs usual, we check our calculation using torch_fft_fft() (fig. 26.6).\n\nx &lt;- torch_cos(frequency(4, N) * sample_positions - pi / 2)\n\nplot_ft(x)\n\n\n\n\nFigure 26.6: Delaying a pure cosine wave by \\(\\pi/2\\) yields a pure sine wave. Now the real parts of all coefficients are zero; instead, non-zero imaginary values are appearing. The phase shift at those positions is \\(\\pi/2\\).\n\n\nFor a pure sine wave, the non-zero Fourier coefficients are imaginary. The phase shift in the coefficients, reported as \\(\\frac{\\pi}{2}\\), reflects the time delay we applied to the signal.\nFinally – before we write some code – let’s put it all together, and look at a wave that has more than a single sinusoidal component.\n\n\n26.1.6 Superposition of sinusoids\nThe signal we construct may still be expressed in terms of the basis vectors, but it is no longer a pure sinusoid. Instead, it is a linear combination of such:\n\\[\n\\begin{aligned}\n\\mathbf{x}_n &= 3 sin(\\frac{2 \\pi}{64} 4n) + 6 cos(\\frac{2 \\pi}{64} 2n) +2cos(\\frac{2 \\pi}{64} 8n)\n\\end{aligned}\n\\]\nI won’t go through the calculation in detail, but it is no different from the previous ones. You compute the DFT for each of the three components, and assemble the results. Without any calculation, however, there’s quite a few things we can say:\n\nSince the signal consists of two pure cosines and one pure sine, there will be four coefficients with non-zero real parts, and two with non-zero imaginary parts. The latter will be complex conjugates of each other.\nFrom the way the signal is written, it is easy to locate the respective frequencies, as well: The all-real coefficients will correspond to frequency indices 2, 8, 56, and 62; the all-imaginary ones to indices 4 and 60.\nFinally, amplitudes will result from multiplying with \\(\\frac{64}{2}\\) the scaling factors obtained for the individual sinusoids.\n\nLet’s check (fig. 26.7):\n\nx &lt;- 3 * torch_sin(frequency(4, N) * sample_positions) +\n  6 * torch_cos(frequency(2, N) * sample_positions) +\n  2 * torch_cos(frequency(8, N) * sample_positions)\n\nplot_ft(x)\n\n\n\n\nFigure 26.7: Superposition of pure sinusoids, and its DFT.\n\n\nNow, how do we calculate the DFT for less convenient signals?"
  },
  {
    "objectID": "fourier_transform_dft.html#coding-the-dft",
    "href": "fourier_transform_dft.html#coding-the-dft",
    "title": "26  Exploring the Discrete Fourier Transform (DFT)",
    "section": "26.2 Coding the DFT",
    "text": "26.2 Coding the DFT\nFortunately, we already know what has to be done. We want to project the signal onto each of the basis vectors. In other words, we’ll be computing a bunch of inner products. Logic-wise, nothing changes: The only difference is that in general, it will not be possible to represent the signal in terms of just a few basis vectors, like we did before. Thus, all projections will actually have to be calculated. But isn’t automation of tedious tasks one thing we have computers for?\nLet’s start by stating input, output, and central logic of the algorithm to be implemented. As throughout this chapter, we stay in a single dimension. The input, thus, is a one-dimensional tensor, encoding a signal. The output is a one-dimensional vector of Fourier coefficients, of the same length as the input, each holding information about a frequency. The central idea is: To obtain a coefficient, project the signal onto the corresponding basis vector.\nTo implement that idea, we need to create the basis vectors, and for each one, compute its inner product with the signal. This can be done in a loop. Surprisingly little code is required to accomplish the goal:\n\ndft &lt;- function(x) {\n  n_samples &lt;- length(x)\n\n  n &lt;- torch_arange(0, n_samples - 1)$unsqueeze(1)\n\n  Ft &lt;- torch_complex(\n    torch_zeros(n_samples), torch_zeros(n_samples)\n  )\n\n  for (k in 0:(n_samples - 1)) {\n    w_k &lt;- torch_exp(-1i * 2 * pi / n_samples * k * n)\n    dot &lt;- torch_matmul(w_k, x$to(dtype = torch_cfloat()))\n    Ft[k + 1] &lt;- dot\n  }\n  Ft\n}\n\nTo test the implementation, we can take the last signal we analysed, and compare with the output of torch_fft_fft().\n\nFt &lt;- dft(x)\ntorch_round(Ft$real) %&gt;% as.numeric()\ntorch_round(Ft$imag) %&gt;% as.numeric()\n\n[1]  0 0 192 0 0 0 0 0 64 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n[29] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n[57] 64 0 0 0 0 0 192 0\n\n[1]  0 0 0 0 -96 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n[29] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n[57] 0 0 0 0 96 0 0 0\nReassuringly – if you look back – the results are the same.\nAbove, did I say “little code”? In fact, a loop is not even needed. Instead of working with the basis vectors one-by-one, we can stack them in a matrix. Then each row will hold the conjugate of a basis vector, and there will be \\(N\\) of them. The columns correspond to positions \\(0\\) to \\(N-1\\); there will be \\(N\\) of them as well. For example, this is how the matrix would look for \\(N=4\\):\n\\[\n\\mathbf{W}_4\n=\n\\begin{bmatrix}\n   e^{-i\\frac{2 \\pi}{4}* 0 * 0} &   e^{-i\\frac{2 \\pi}{4}* 0 * 1}  & e^{-i\\frac{2 \\pi}{4}* 0 * 2} &  e^{-i\\frac{2 \\pi}{4}* 0 * 3}\\\\\ne^{-i\\frac{2 \\pi}{4}* 1 * 0} &   e^{-i\\frac{2 \\pi}{4}* 1 * 1}  & e^{-i\\frac{2 \\pi}{4}* 1 * 2} &  e^{-i\\frac{2 \\pi}{4}* 1 * 3}\\\\\ne^{-i\\frac{2 \\pi}{4}* 2 * 0} &   e^{-i\\frac{2 \\pi}{4}* 2 * 1}  & e^{-i\\frac{2 \\pi}{4}* 2 * 2} &  e^{-i\\frac{2 \\pi}{4}* 2 * 3}\\\\\ne^{-i\\frac{2 \\pi}{4}* 3 * 0} &   e^{-i\\frac{2 \\pi}{4}* 3 * 1}  & e^{-i\\frac{2 \\pi}{4}* 3 * 2} &  e^{-i\\frac{2 \\pi}{4}* 3 * 3}\\\\\n\\end{bmatrix}\n\\tag{26.3}\\]\nOr, evaluating the expressions:\n\\[\n\\mathbf{W}_4\n=\n\\begin{bmatrix}\n   1 &   1  & 1 &  1\\\\\n1 &   -i  & -1 &  i\\\\\n1 &   -1  & 1 &  -1\\\\\n1 &   i  & -1 &  -i\\\\\n\\end{bmatrix}\n\\]\nWith that modification, the code looks a lot more elegant:\n\ndft_vec &lt;- function(x) {\n  n_samples &lt;- length(x)\n\n  n &lt;- torch_arange(0, n_samples - 1)$unsqueeze(1)\n  k &lt;- torch_arange(0, n_samples - 1)$unsqueeze(2)\n\n  mat_k_m &lt;- torch_exp(-1i * 2 * pi / n_samples * k * n)\n\n  torch_matmul(mat_k_m, x$to(dtype = torch_cfloat()))\n}\n\nAs you can easily verify, the result is the same.\nBefore we move on to the next chapter – and the Fast Fourier Transform – we should test our implementation on something more complex."
  },
  {
    "objectID": "fourier_transform_dft.html#fun-with-sox",
    "href": "fourier_transform_dft.html#fun-with-sox",
    "title": "26  Exploring the Discrete Fourier Transform (DFT)",
    "section": "26.3 Fun with sox",
    "text": "26.3 Fun with sox\nsox is a command line tool for processing audio, most often (I guess) used to quickly play some sound file, for resampling, or to convert between different file formats. However, it can generate sound, as well! (Admittedly, the syntax is not the most intuitive.) Here is the command I used in generating the test sound analysed below:\n\nsox --combine concatenate \\\n  \"|sox -r 4000 -n -p synth 0.2 \\\n  sine 941 sine 1336 pinknoise remix -\" \\\n  \"|sox -r 4000 -n -p synth 0.2 \\\n  sine 852 sine 1336 pinknoise remix - pad 0.15 0.15\" \\\n  \"|sox -r 4000 -n -p synth 0.2 \\\n  sine 852 sine 1477 pinknoise remix -\" \\\n  resources/dft-dial.wav\n\nThe result is a concatenation of three distinct sound events, of duration 0.2 seconds, separated by slightly shorter periods of silence (pad 0.15 0.15). Each sound event is a superposition of three components: two sine waves of different frequency, and pink noise. The frequency combinations used are not accidental: They represent the encoding of the numbers 0, 8, and 9 in the Dual-tone multi-frequency signaling (DTMF) system, respectively.\nFor this signal, what would we expect to see in a spectrogram? We expect to see three distinct phases, clearly separated by “nothing”, that each show two dominant frequencies. The first and the second should have one frequency in common; the same holds for phases two and three. In addition, for all three phases, we should see contributions from all other frequencies, with lower frequencies having stronger impact than higher ones. (That’s what defines pink noise.)\nAt this point, allow me to call sox one last time. If you’ve been using that program before, did you know that it can create spectrograms? This one-liner uses all the default settings; yet the result has all the information we want (fig. 26.8):\n\nsox resources/dial.wav -n spectrogram \\ \n  -m -l -w kaiser -o dial-spectrogram.png\n\n\n\n\nFigure 26.8: Spectrogram, created by sox resources/dial.wav -n spectrogram -m -l -w kaiser -o dial-spectrogram.png.\n\n\nNow, let’s load this file into R, making use of torchaudio, a package we already know from the audio classification chapter.\n\nlibrary(torchaudio)\n\nwav &lt;- tuneR_loader(\"resources/dft-dial.wav\")\nwav\n\nWaveMC Object\n  Number of Samples:      3600\n  Duration (seconds):     0.9\n  Samplingrate (Hertz):   4000\n  Number of channels:     1\n  PCM (integer format):   TRUE\n  Bit (8/16/24/32/64):    32\nFrom all the information stored in tuneR’s WaveMC object, we need just the sampling rate and the data itself.\n\nwaveform_and_sample_rate &lt;- transform_to_tensor(wav)\nwaveform &lt;- waveform_and_sample_rate[[1]]\nsample_rate &lt;- waveform_and_sample_rate[[2]]\n\ndim(waveform)\n\n[1]    1 3600\nAs expected, a plot of amplitude over time does not reveal too much (fig. 26.9):\n\ndf &lt;- data.frame(\n  x = 1:dim(waveform)[2],\n  y = as.numeric(waveform$squeeze(1))\n)\nggplot(df, aes(x = x, y = y)) +\n  geom_line() +\n  xlab(\"time\") +\n  ylab(\"amplitude\") +\n  theme_minimal()\n\n\n\n\nFigure 26.9: Three consecutive ringtones, time domain representation.\n\n\nNow, we will compute the Fourier Transform – twice. First, with torch_fft_fft(), so we know the “truth”. Then, using our own dft(). For both, we will display the magnitudes of the lower half of the spectrum, that is, all frequencies up to the Nyquist rate. Finally, with help of torch_fft_ifft(), we will make sure we can regenerate the time domain representation from what dft() gave us.\nIn order to create a meaningful plot, there’s one additional step to be taken. So far, the magnitudes displayed in the frequency-domain plots have simply been ordered by \\(k\\), the index of the basis vector in question. In fact, there was no concept of a real-world frequency. But now, we want to see frequencies treated the way we conceptualize them: as number of cycles per second, measured in Hertz.\nTo be able to do the conversion, we need the sampling rate, which we already saw is 4000 Hertz. We then map the lower-half sample indices (bins_below_nyquist) to real-world frequencies, multiplying by the ratio of sampling rate to overall number of samples:\n\nnum_samples &lt;- dim(waveform)[2]\nnyquist_cutoff &lt;- num_samples / 2 + 1\nbins_below_nyquist &lt;- 1:nyquist_cutoff\n\nfrequencies_per_bin &lt;- sample_rate / num_samples\nreal_world_frequencies &lt;- frequencies_per_bin *\n  bins_below_nyquist\n\nHere, then, is the magnitude plot for torch_fft_fft() (fig. 26.10):\n\ndial_fft &lt;- torch_fft_fft(waveform)$squeeze()\n\np_magnitude &lt;- create_plot(\n  real_world_frequencies,\n  torch_abs(dial_fft)[1:nyquist_cutoff], \"magnitude\"\n)\n\np_magnitude\n\n\n\n\nFigure 26.10: DFT of the ringtone signal, computed by means of torch_fft_fft() . Displayed are the magnitudes of frequencies below the Nyquist rate.\n\n\nThe spectrum reflects the noise component in the signal, but the four peaks (which we know to be located at 852, 941, 1336, and 1477 Hertz) are clearly visible.\nNow, does our hand-written code yield the same result (fig. 26.11)?\n\ndial_dft &lt;- dft(waveform$squeeze())\n\np_magnitude &lt;- create_plot(\n  real_world_frequencies,\n  torch_abs(dial_fft)[1:nyquist_cutoff], \"magnitude\"\n)\n\np_magnitude\n\n\n\n\nFigure 26.11: DFT of the ringtone signal, using our hand-written code. Displayed are the magnitudes of frequencies below the Nyquist rate.\n\n\nIt does. Finally, let’s use the Inverse DFT to recreate the signal (fig. 26.12).\n\nreconstructed &lt;- torch_fft_ifft(dial_dft)\ndf &lt;- data.frame(\n  x = 1:num_samples,\n  y = as.numeric(reconstructed$real)\n)\n\nggplot(df, aes(x = x, y = y)) +\n  geom_line() +\n  xlab(\"time\") +\n  ylab(\"amplitude\") +\n  theme_minimal()\n\n\n\n\nFigure 26.12: Reconstruction of the time domain signal from the output of dft().\n\n\nAnd there we are. We’ve implemented the DFT ourselves, and learned quite a bit on our way. For the Fast Fourier Transform, the topic of the next chapter, the plan is exactly the same."
  },
  {
    "objectID": "fourier_transform_dft.html#footnotes",
    "href": "fourier_transform_dft.html#footnotes",
    "title": "26  Exploring the Discrete Fourier Transform (DFT)",
    "section": "",
    "text": "Its counterpart – that takes the Fourier representation and yields a time-domain signal – is called torch_fft_ifft() (with ifft standing for Inverse Fourier Transform).↩︎\nExpanding on this a bit: For real-valued signals, the magnitudes as well as the real parts of corresponding coefficients are equal, while the phases and the imaginary parts are conjugated. In other words, the coefficients are complex conjugates of each other. We’ll see this in later examples.↩︎\nI’ll be writing \\(\\mathbf{x}_n\\) instead of \\(f(x)\\) from now on to indicate that we’re working with discrete samples, not the continuous function itself.↩︎\nEuler’s formula relates complex exponentials to sines and cosines, stating that \\(e^{i \\theta} = cos \\theta + i sin \\theta\\).↩︎"
  },
  {
    "objectID": "fourier_transform_fft.html#some-terminology",
    "href": "fourier_transform_fft.html#some-terminology",
    "title": "27  The Fast Fourier Transform (FFT)",
    "section": "27.1 Some terminology",
    "text": "27.1 Some terminology\nLet me make clear right at the outset that, as compared to the DFT, the FFT is not another transform. Its output is just the same as that of the DFT. With the FFT, it’s all about the “fast”. Also, there is no “one” FFT. Instead, there are different families; and in each family, there are various sub-types.\nHere, I’m focusing on the “classic among the classics”: the one that goes by radix-2 decimation-in-time (DIT). Here “radix-2” refers to an implementation detail; it indicates that the algorithm will require input size to equal a power of two. “Decimation in time”, on the other hand, relates to the overall strategy being employed: divide-and-conquer. The input is recursively split into halves, and partial results are combined in a clever way. (Just as an aside, there is a very similar algorithm called “decimation in frequency”. There, it is the frequency domain where recursive splitting occurs.)\nNow, we discuss how this works. You’ll see that once we’ve worked out clearly what we want to do, a (naive) implementation does not require more code than the straightforward DFT from the last chapter. And please be assured that although this section has many equations, each and every manipulation will be explained in words."
  },
  {
    "objectID": "fourier_transform_fft.html#radix-2-decimation-in-timedit-walkthrough",
    "href": "fourier_transform_fft.html#radix-2-decimation-in-timedit-walkthrough",
    "title": "27  The Fast Fourier Transform (FFT)",
    "section": "27.2 Radix-2 decimation-in-time(DIT) walkthrough",
    "text": "27.2 Radix-2 decimation-in-time(DIT) walkthrough\nThe simplifications resulting from decimation in time can be presented as a two-step logic. Reflecting that view, and re-using (with different semantics) terminology employed in the convolution chapter, I could call them “input-view phase” and “output-view phase”. However, these are not phases in a temporal sense, and more importantly – in a software-focused book – we’ll see that as ported to code, their respective impacts differ a lot. Therefore, I’ll name the upcoming two sections in ways reflecting importance instead. (I’ll still make clear what I mean by input and output views here, though.)\n\n27.2.1 The main idea: Recursive split\nAs we would expect for a divide-and-conquer algorithm, the main and most impactful observation is that if the input is split up recursively, the problem can be divided into sub-problems that get increasingly easier to solve – provided we know how to combine the partial results into a final solution.\nBefore we start, let me recall some notation, and make a slight modification that will turn out convenient. In the previous chapter, with \\(N\\) the size of the input (equivalently, the number of resulting frequency coefficients), and \\(k\\) (ranging from zero to \\(N-1\\)) referencing the vector in question, the DFT basis vectors were defined like so:\n\\[\n\\mathbf{w}^{kn}_N = e^{i\\frac{2 \\pi}{N}k n}\n\\]\nThen, the \\(k\\)th frequency coefficient was obtained by computing the inner product between \\(\\mathbf{w}^{kn}_N\\) and the input, \\(\\mathbf{x}_n\\):\n\\[\n\\begin{aligned}\nX_k &= \\langle \\mathbf{w}^{kn}_N, \\mathbf{x}_n \\rangle \\\\ &= \\sum_{n=0}^{N-1} x[n] \\ w^{-nk}_N\\\\\n\\end{aligned}\n\\]\nIn this chapter, we will be working with the complex conjugates of the basis vectors throughout, since it’s those that actually get (element-wise) multiplied with the input vector. For convenience, we thus slightly change notation, and let \\(w^{kn}_N\\) refer to the conjugated complex exponential1:\n\\[\nw^{kn}_N = e^{-i\\frac{2 \\pi}{N}k n}\n\\tag{27.1}\\]\nThen, abstracting over \\(n\\), we also have \\[\nw^k_N = e^{-i\\frac{2 \\pi}{N}k}\n\\tag{27.2}\\]\nThat said, we state again what we want to compute: the frequency coefficients \\(X_k\\).\n\\[\n\\begin{aligned}\nX_k &=  \\sum_{n=0}^{N-1} x[n] \\ w^{nk}_N\\\\\n\\end{aligned}\n\\]\nNow comes what I was thinking to refer to as the “input view”. We take the input sequence, and divide up the computation into two parts. One will deal with the even, the other, with the odd indices of the signal. So expressed, the sums only go up to \\(N/2 - 1\\).\n\\[\n\\begin{aligned}\nX_k &=  \\sum_{n=0}^{(N/2-1)} x[2n] \\ w^{2nk}_N + \\sum_{n=0}^{N/2-1} x[2n+1] \\ w^{(2n+1)k}_N \\\\\n\\end{aligned}\n\\tag{27.3}\\]\nNow, that second sum can be rewritten, splitting up the \\(w^{2nk+k}_N\\) into two factors:\n\\[\n\\sum_{n=0}^{N/2-1} x[2n+1] \\ w^{2nk+k}_N = \\sum_{n=0}^{N/2-1} x[2n+1] \\ w^{2nk}_N w^{k}_N\n\\]\nThe second factor is exactly the \\(w^{k}_N\\) we were introducing above. Since it does not depend on \\(n\\), we can move it out of the sum. This yields\n\\[\n\\begin{aligned}\nX_k &=  \\sum_{n=0}^{(N/2-1)} x[2n] \\ w^{2nk}_N + w^k_N \\sum_{n=0}^{N/2-1} x[2n+1] \\ w^{2nk}_N \\\\\n\\end{aligned}\n\\]\nNow the exponential factor is the same in both sums. Let’s inspect it a bit more closely. It is the multiplication factor of a DFT of size \\(N\\), at (frequency-by-time) position \\(2nk\\). If we write this out, we see that we can move the factor \\(2\\) from the numerator to the denominator of the fraction:\n\\[\nw^{2nk}_N = e^{-i\\frac{2 \\pi}{N}2nk} = e^{-i\\frac{2 \\pi}{N/2}nk} = w^{nk}_{N/2}\n\\]\nWhy does this matter? The result is actually the corresponding basis vector of a DFT of size \\(N/2\\), at position \\(nk\\). Which means that now, we are actually computing a DFT of half the size – or rather, two such DFTs:\n\\[\nX_k =  \\sum_{n=0}^{(N/2-1)} x[2n] \\ w^{nk}_{N/2} + w^k_N \\ \\sum_{n=0}^{N/2-1} x[2n+1] \\ w^{nk}_{N/2} \\\\\n\\tag{27.4}\\]\nLet’s write this in more readable form:\n\\[\nX_k =  X^{even}_k +  w^k_N \\ X^{odd}_k\n\\tag{27.5}\\]\nNow, you probably see where this is going. What we’ve done once – halve the size of the computation – we can do again … and again. It is this recursive halving that allows the FFT to obtain it famous reduction in computational cost.\nThis is the main ingredient of the magic, but it is not quite everything yet.\n\n\n27.2.2 One further simplification\nThere is one additional simplification we can make. Compared to the first, it is of lesser significance, at least as far as computational performance is concerned. However, it definitely matters from an aesthetics point of view.\nDid you notice something strange in our final formula? We are computing DFTs of size \\(N/2\\), but still, the factor \\(w^k_N\\) appears! This isn’t a problem, but it is not “nice”, either. Fortunately, for those who mind, the “inconsequence” can be eliminated.\nWhat follows is what I was tempted to name the “output-side view”. That’s because now, we roll up things from the end, starting from the computation’s output. We take the set of Fourier coefficients \\(X_k\\), and independently consider the first and the second half. Note how here, like in the input-centric view, we apply a split-in-two strategy; just this time, the halving is done in a different way.\nLooking at both halves, we notice that both have their dedicated subsets of multiplication factors \\(w^k_N\\), one with \\(k\\) ranging from \\(0\\) to \\(N/2-1\\), the other, from \\(N/2\\) to \\(N-1\\). For the first, this means we can change the upper limit in the sum, yielding\n\\[\nX^{upper}_k =  X^{even}_k +  w^k_N \\ X^{odd}_k \\ , \\ \\ k = 0 ... N/2-1\n\\]\nFor the second, we can achieve the desired result by summing over the same range, but adding \\(N/2\\) to \\(k\\) everywhere.\n\\[\nX^{lower}_{k+N/2} =  \\sum_{n=0}^{N/2-1} x[2n] \\ w^{n (k+N/2)}_{N/2} + w^{k+N/2}_N \\ \\sum_{n=0}^{N/2-1} x[2n + 1] \\ w^{n (k+ N/2)}_{N/2} \\\\\n\\]\nNow, in the first of the sums that make up \\(X^{lower}\\), the exponential can be factored, and we see that the factor containing the \\(N/2\\) evaluates to \\(1\\) (and thus, disappears):\n\\[\n\\begin{aligned}\nw^{n(k+N/2)}_{N/2}&= e^{-i\\frac{2 \\pi}{N/2} n (k + N/2)} \\\\ &= e^{-i\\frac{2 \\pi}{N/2}n k} *  e^{-i\\frac{2 \\pi}{N/2}n (N/2)} \\\\ &= e^{-i\\frac{2 \\pi}{N/2}n k} *  e^{-i\\frac{2 \\pi}{N/2}(N/2)} \\\\ &= e^{-i\\frac{2 \\pi}{N/2}n k} *  1\n\\end{aligned}\n\\]\nAs a result, the first of the sums now looks like this:\n\\[\nX^{lower}_{k+N/2} =  \\sum_{n=0}^{N/2-1} x[2n] \\ w^{n k}_{N/2} + [...]\n\\]\nThe same thing can be done in the second sum:\n\\[\nX^{lower}_{k+N/2} =  [...] + w^{k+N/2}_N \\ \\sum_{n=0}^{N/2-1} x[2n + 1] \\ w^{n k}_{N/2} \\\\\n\\]\nNow there’s just one last inconvenient index of \\(k + N/2\\) remaining. A calculation similar to that above shows we can replace it by a minus sign:\n\\[\n\\begin{aligned}\nw^{k+N/2}_N&= e^{-i\\frac{2 \\pi}{N} (k + N/2)} \\\\\n&= e^{-i\\frac{2 \\pi}{N} k} * e^{-i\\frac{2 \\pi}{N} N/2}\\\\\n&= e^{-i\\frac{2 \\pi}{N} k} * e^{-i \\pi}\\\\\n&= e^{-i\\frac{2 \\pi}{N} k} * (-1)\\\\\n\\end{aligned}\n\\]\nIn consequence, the complete second part can be written like this:\n\\[\nX^{lower}_{k+N/2} =  \\sum_{n=0}^{N/2-1} x[2n] \\ w^{n k}_{N/2} - w^k_N \\ \\sum_{n=0}^{N/2-1} x[2n + 1] \\ w^{n k}_{N/2}\n\\]\nAnd now, the second half looks nearly like the first one, just with a change in sign. Here, then, is the final algorithm:\n\\[\n\\begin{aligned}\n& X^{upper}_k =  X^{even}_k +  w^k_N \\ X^{odd}_k \\ , \\ \\ k = 0 ... N/2-1 \\\\\n& X^{lower}_{k+N/2} =  X^{even}_k -  w^k_N \\ X^{odd}_k \\ , \\ \\ k = 0 ... N/2-1\n\\end{aligned}\n\\tag{27.6}\\]\nOwed to a popular form of visualization, this representation is often referred to as the “butterfly”. If you’re curious, you won’t have problems finding related diagrams on the net. Personally, I don’t find them very helpful, which is why I’m not reproducing them here.\nIn conclusion, we’ve now arrived at a rule that tells us how to simplify an FFT of size \\(N\\) by replacing it with an FFT of size \\(N/2\\). The complete algorithm then consists in recursive application of that rule.\nWe’re ready to start thinking about how to implement this."
  },
  {
    "objectID": "fourier_transform_fft.html#fft-as-matrix-factorization",
    "href": "fourier_transform_fft.html#fft-as-matrix-factorization",
    "title": "27  The Fast Fourier Transform (FFT)",
    "section": "27.3 FFT as matrix factorization",
    "text": "27.3 FFT as matrix factorization\nBelow, we’ll explore different ways of coding the FFT. In two of them, you’ll directly recognize the rule Equation 27.6. The third is different, though. It makes direct use of the fact that the DFT matrix \\(\\mathbf{W}_N\\) can be factored into three sparse matrices, each materializing one of the three stages inherent in the rule: split up the input into even and odd indices; compute the two half-sized FFTs; recombine the results.\nFor example, take \\(\\mathbf{W}_4\\), the matrix we analyzed in the previous chapter:\n\\[\n\\mathbf{W}_4\n=\n\\begin{bmatrix}\n1 &   1  & 1 &  1\\\\\n1 &   -i  & -1 &  i\\\\\n1 &   -1  & 1 &  -1\\\\\n1 &   i  & -1 &  -i\\\\\n\\end{bmatrix}\n\\]\nThis can be factorized into three matrices like so:\n\\[\n\\begin{aligned}\n\\mathbf{W}_4\n&=\n\\begin{bmatrix}\n1 &   1  & 1 &  1\\\\\n1 &   -i  & -1 &  i\\\\\n1 &   -1  & 1 &  -1\\\\\n1 &   i  & -1 &  -i\\\\\n\\end{bmatrix} \\\\\n&=\n\\begin{bmatrix}\n1 &   0  & 1 &  0\\\\\n0 &   1  & 0 &  -i\\\\\n1 &   0  & -1 &  0\\\\\n0 &   1  & 0 &  i\\\\\n\\end{bmatrix}\n\\begin{bmatrix}\n1 &   1  & 0 &  0\\\\\n1 &   -1  &  0 &  0\\\\\n0 &   0  & 1 &  1\\\\\n0 &   0  & 1 &  -1\\\\\n\\end{bmatrix}\n\\begin{bmatrix}\n1 &   0  & 0 &  0\\\\\n0 &   0  & 1 &  0\\\\\n0 &   1  & 0 &  0\\\\\n0 &   0  & 0 &  1\\\\\n\\end{bmatrix}\n\\end{aligned}\n\\]\nThe rightmost matrix (call it \\(P\\), for permutation) reorders the input. Here’s how it acts on a suitably-sized input vector.\n\\[\n\\mathbf{P}_4 \\mathbf{x}\n=\n\\begin{bmatrix}\n1 &   0  & 0 &  0\\\\\n0 &   0  & 1 &  0\\\\\n0 &   1  & 0 &  0\\\\\n0 &   0  & 0 &  1\\\\\n\\end{bmatrix}\n\\begin{bmatrix}\nx1 \\\\\nx2 \\\\\nx3 \\\\\nx4 \\\\\n\\end{bmatrix}\n=\n\\begin{bmatrix}\nx1 \\\\\nx3 \\\\\nx2 \\\\\nx4 \\\\\n\\end{bmatrix}\n\\]\nNow that even- and odd-indexed values are nicely separated, we can construct a block matrix that applies a DFT to each of those groups in isolation. In this case, the DFT in question is of size two. If you look at the central matrix above, you’ll see that it contains two instances of \\(\\mathbf{W}_2\\), with \\(\\mathbf{W}_2\\) being\n\\[\n\\mathbf{W}_2 =\n\\begin{bmatrix}\n1 &   1  \\\\\n1 &   -1 \\\\\n\\end{bmatrix}\n\\]\nTaking as input the permuted signal, \\(\\mathbf{P}_4 \\mathbf{x}\\), the block matrix produces the following output:\n\\[\n\\mathbf{W}_{2*2}\\mathbf{P}_{4}\\mathbf{x}\n=\n\\begin{bmatrix}\n1 &   1  & 0 &  0\\\\\n1 &   -1  &  0 &  0\\\\\n0 &   0  & 1 &  1\\\\\n0 &   0  & 1 &  -1\\\\\n\\end{bmatrix}\n\\begin{bmatrix}\nx1 \\\\\nx3 \\\\\nx2 \\\\\nx4 \\\\\n\\end{bmatrix}\n=\n\\begin{bmatrix}\nx1+x3 \\\\\nx1-x3 \\\\\nx2+x4 \\\\\nx2-x4 \\\\\n\\end{bmatrix}\n\\]\nNext, the two sets of coefficients need to be recombined in the correct way. Personally, I find it hard to mentally picture how the leftmost matrix in the factorization does it; so let’s try if we can build up the matrix ourselves.\nThe FFT rule Equation 27.6 tells us what has to happen. Here it is again:\n\\[\nX^{upper}_k =  X^{even}_k +  w^k_N \\ X^{odd}_k \\ , \\ \\ k = 0 ... N/2-1\n\\]\n\\[\nX^{lower}_{k+N/2} =  X^{even}_k -  w^k_N \\ X^{odd}_k \\ , \\ \\ k = 0 ... N/2-1\n\\]\nIn this example, \\(N/2\\) is 2; we thus need \\(w^0_4\\) and \\(w^1_4\\). Their values are\n\\[\n\\begin{aligned}\n&w^0_4 = e^{-i\\frac{2 \\pi}{4}0} = 1\\\\\n&w^1_4 = e^{-i\\frac{2 \\pi}{4}1} = -i\\\\\n\\end{aligned}\n\\]\nAs an aside, we could also have read them off the transformation matrix, \\(W_4\\): These are the first two basis vectors, computed at \\(n=1\\), and thus are found right on top of the second column.\nNow, we just mechanically apply the rule.\n\\[\n\\begin{aligned}\n&X_0 = X^{upper}_0 = X^{even}_0 +  w^0_4 \\ X^{odd}_0 = (x_1 + x_3) + 1 * (x_2 + x_4)  \\\\\n&X_1 = X^{upper}_1 = X^{even}_1 +  w^1_4 \\ X^{odd}_1 = (x_1 - x_3) - i * (x_2 - x_4)  \\\\\n&X_2 = X^{lower}_2 = X^{even}_0 -  w^0_4 \\ X^{odd}_0 = (x_1 + x_3) - 1 * (x_2 + x_4)  \\\\\n&X_3 = X^{lower}_3 = X^{even}_1 -  w^1_4 \\ X^{odd}_1 = (x_1 - x_3) + i * (x_2 - x_4)  \\\\\n\\end{aligned}\n\\]\nThis gives us the multiplication factors to be applied to the vector input. All that remains to be done is put them into a matrix. Directly reading off the above equations, and filling in zeroes whenever an input is not used, this is what we obtain for the “butterfly” matrix \\(\\mathbf{B}\\):\n\\[\n\\mathbf{B}_4\n=\n\\begin{bmatrix}\n1 &   0  & 1 &  0\\\\\n0 &   1  & 0 &  -i\\\\\n1 &   0  & -1 &  0\\\\\n0 &   1  & 0 &  i\\\\\n\\end{bmatrix}\n\\]\nComparing with the leftmost matrix in the factorization, we see we’ve arrived at the correct result.\nOne thing that’s not immediately clear, however, is how to implement this recursively. It certainly seems like a lot of overhead to compute the complete matrix factorization at every recursive step. Fortunately, this is not needed. For one, the sorting can be done just once, right in the beginning. And secondly, it turns out that the matrices I’ve been referring to as \\(\\mathbf{W}_{2*2}\\) and \\(\\mathbf{B}_{4}\\) are intimately related: \\(\\mathbf{B}_{4}\\) is what would go into a block matrix \\(\\mathbf{W}_{4*4}\\).\nThe recursive procedure can then be laid out very clearly. Here, for example, is how the complete procedure would look for an input size of 8:\n\\[\n\\mathbf{W}_8\n=\n\\mathbf{B}_8\n\\begin{bmatrix}\n\\mathbf{B}_4 &  \\mathbf{0}\\\\\n\\mathbf{0} &  \\mathbf{B}_4\\\\\n\\end{bmatrix}\n\\begin{bmatrix}\n\\mathbf{B}_2 &   \\mathbf{0}  & \\mathbf{0} &  \\mathbf{0}\\\\\n\\mathbf{0} &   \\mathbf{B}_2  &  \\mathbf{0} &  \\mathbf{0}\\\\\n\\mathbf{0} &   \\mathbf{0}  & \\mathbf{B}_2 &  \\mathbf{0}\\\\\n\\mathbf{0} &   \\mathbf{0}  & \\mathbf{0} &  \\mathbf{B}_2\\\\\n\\end{bmatrix}\n\\mathbf{R}\n\\]\n\\(\\mathbf{R}\\) is the matrix that, once and for all, sorts the input in the required way. I’ve written \\(\\mathbf{R}\\) for “bit reversal”, since that is the actual algorithm used. We won’t go into its workings here, but explanations are readily found on the web.\nHaving discussed DFT matrix factorization, we’re ready to look at some code."
  },
  {
    "objectID": "fourier_transform_fft.html#implementing-the-fft",
    "href": "fourier_transform_fft.html#implementing-the-fft",
    "title": "27  The Fast Fourier Transform (FFT)",
    "section": "27.4 Implementing the FFT",
    "text": "27.4 Implementing the FFT\nWe discuss and compare for performance three different implementations of the FFT, plus our two DFT versions from the last chapter. Let me start by listing, again, what we did there.\n\n27.4.1 DFT, the “loopy” way\nThis was the way we first coded the DFT, computing the dot products between input and each basis vector in a loop.\n\nlibrary(torch)\n\ndft &lt;- function(x) {\n  n_samples &lt;- length(x)\n  n &lt;- torch_arange(0, n_samples - 1)$unsqueeze(1)\n  F &lt;- torch_complex(\n    torch_zeros(n_samples),\n    torch_zeros(n_samples)\n  )\n\n  for (k in 0:(n_samples - 1)) {\n    w_k &lt;- torch_exp(-1i * 2 * pi / n_samples * k * n)\n    dot &lt;- torch_matmul(w_k, x$to(dtype = torch_cfloat()))\n    F[k + 1] &lt;- dot\n  }\n  F\n}\n\n\n\n27.4.2 DFT, vectorized\nNext, we went on to replace the loop by arranging the basis vectors in a matrix.\n\ndft_vec &lt;- function(x) {\n  n_samples &lt;- length(x)\n\n  n &lt;- torch_arange(0, n_samples - 1)$unsqueeze(1)\n  k &lt;- torch_arange(0, n_samples - 1)$unsqueeze(2)\n  mat_k_m &lt;- torch_exp(-1i * 2 * pi/n_samples * k * n)\n\n  torch_matmul(mat_k_m, x$to(dtype = torch_cfloat()))\n}\n\n\n\n27.4.3 Radix-2 decimation in time FFT, recursive\nJust like we did for the DFT algorithm per se, we can straightforwardly, by-specification implement the FFT. This time, the logically-imposed design is recursive, not iterative. In each call to fft(), the input is split into even and odd indices, respective half-size FFTs are computed, and the two sets of outputs are combined as required.\n\n# straightforward, recursive implementation of the FFT.\n# Expects input size to be a power of 2.\nfft &lt;- function(x) {\n  n_samples &lt;- length(x)\n  if (n_samples == 1) {\n    return(x)\n  }\n\n  X_upper &lt;- fft(x[1:n_samples:2])\n  X_lower &lt;- fft(x[2:n_samples:2])\n\n  w_k &lt;- torch_exp(\n    -2 * pi * torch_complex(0, 1) *\n      torch_arange(0, n_samples / 2 - 1) / n_samples\n  )\n  torch_cat(list(\n    X_upper + w_k * X_lower,\n    X_upper - w_k * X_lower\n  ))\n}\n\nThis function expects input size to equal a power of two.\n\n\n27.4.4 Radix-2 decimation in time FFT by matrix factorization\nNext, we implement the matrix-factorization strategy described above. fft_vec() is a generalization of the logic spelt out in Brad Osgood’s wonderful book on the Fourier Transform (Osgood (2019)).\nWe sort the input tensor (a single time), apply successive block matrices of doubled-in-size “butterflies”, and multiply the result with a single butterfly matrix of size matching the number of inputs.\nThe sorting may conveniently be done using bitrevorder(), a function provided by the R package gsignal.\nIn the loop, you can see how the butterfly matrices are built: They consist of a combination of identity matrices with diagonal matrices holding the \\(w^k_N\\).\n\nlibrary(torch)\nlibrary(gsignal)\n\n# requirements: input length is at least 4, and a power of 2\nfft_matrix &lt;- function(x) {\n\n  # perform sorting just once, a the beginning\n  x &lt;- torch_tensor(\n    bitrevorder(as.numeric(x)),\n    dtype = torch_cfloat()\n  )\n\n  n_samples &lt;- length(x)\n\n  # smallest butterfly matrix, needed for all valid inputs\n  B2 &lt;- torch_tensor(\n    c(1, 1, 1, -1),\n    dtype = torch_cfloat()\n  )$view(c(2, 2))\n  B2_block &lt;- torch_block_diag(\n    B2$`repeat`(c(n_samples / 2, 1))$split(2)\n  )\n  acc &lt;- torch_matmul(B2_block, x)\n\n  # iterative implementation then starts with B4\n  n &lt;- 4\n\n  while (n &lt;= n_samples) {\n\n    # build up current butterfly matrix\n    I &lt;- torch_eye(n / 2)\n    O &lt;- torch_diag(\n      torch_exp(\n        -1i * 2 * pi *\n          torch_arange(0, n / 2 - 1) / (n / 2 * 2)\n      )\n    )\n    B &lt;- torch_cat(list(\n      torch_cat(list(I, O), dim = 2),\n      torch_cat(list(I, -O), dim = 2)\n    ), dim = 1)\n\n    # in the final multiplication,\n    # B directly matches input length\n    if (n == n_samples) {\n      return(torch_matmul(B, acc))\n    }\n\n    # create block-diagonal matrix from butterflies\n    # at each iteration,\n    # we need to replicate B {n_samples/rank(B) times}\n    # this is achieved by first repeating B row-wise,\n    # then splitting up into rank(n) parts)\n    B_block &lt;- torch_block_diag(\n      B$`repeat`(c(n_samples / n, 1))$split(n)\n    )\n    acc &lt;- torch_matmul(B_block, acc)\n    n &lt;- n * 2\n  }\n  acc\n}\n\n\n\n27.4.5 Radix-2 decimation in time FFT, optimized for vectorization\nFinally, let me present one more implementation. It is a literal translation of the Python code published by Jake van der Plas on his blog. Although it looks more involved than the recursive fft() above, it really implements the same algorithm, all while making use of vectorization as much as possible. In other words, it is to fft() what dft_vec() is to dft().\n\n# torch translation of\n# http://jakevdp.github.io/blog/2013/08/28/\n# understanding-the-fft/#Vectorized-Numpy-Version\n\nfft_vec &lt;- function(x) {\n  n_samples &lt;- length(x)\n  # could be chosen higher for performance reasons\n  n_min &lt;- 2 \n\n  # Perform an O[N^2] DFT on all length-N_min \n  # sub-problems at once\n  n &lt;- torch_arange(0, n_min - 1)$unsqueeze(1)\n  k &lt;- torch_arange(0, n_min - 1)$unsqueeze(2)\n\n  # by starting with one (vectorized-by-matmul)\n  # \"classic DFT\" (instead of a block matrix of B_mins),\n  # we don't need the bitrevorder step\n  mat_k_m &lt;- torch_exp(-1i * 2 * pi / n_min * k * n)\n  F &lt;- torch_matmul(\n    mat_k_m,\n    x$to(dtype = torch_cfloat())$reshape(list(n_min, -1))\n  )\n\n  # build-up each level of the recursive calculation\n  # all at once\n  while (dim(F)[1] &lt; n_samples) {\n    F_first &lt;- F[, 1:(dim(F)[2] / 2)]\n    F_second &lt;- F[, (dim(F)[2] / 2 + 1):dim(F)[2]]\n    # only need first half of w_ks\n    w_k &lt;- torch_exp(\n      -1i * pi *\n        torch_arange(0, dim(F)[1] - 1) / dim(F)[1]\n    )$unsqueeze(2)\n    F &lt;- torch_vstack(list(\n      F_first + w_k * F_second,\n      F_first - w_k * F_second\n    ))\n    # w_k * F_second multiplies both at once (column-wise)\n  }\n  F$ravel()\n}\n\n\n\n27.4.6 Checking against torch_fft_fft()\nNow, before we compare those five functions performance-wise, let’s check whether they yield the same results as torch_fft_fft(). We don’t expect identity over a large number of decimal places; but it will be good to know about eventual differences in accuracy between the different implementations.\n\nx &lt;- torch_randn(2^13)\natol &lt;- 1e-4\n\ny_ref &lt;- torch_fft_fft(x)\n\ny_dft &lt;- dft(x)\ny_dft_vec &lt;- dft_vec(x)\ny_fft &lt;- fft(x)\ny_fft_vec &lt;- fft_vec(x)\ny_fft_matrix &lt;- fft_matrix(x)\n\ntorch_allclose(y_dft, y_ref, atol = atol)\ntorch_allclose(y_dft_vec, y_ref, atol = atol)\ntorch_allclose(y_fft, y_ref, atol = atol)\ntorch_allclose(y_fft_vec, y_ref, atol = atol)\ntorch_allclose(y_fft_matrix, y_ref, atol = atol)\n\n[1] FALSE\n[1] FALSE\n[1] TRUE\n[1] TRUE\n[1] TRUE\nReassuringly, the FFT implementations all seem sufficiently accurate.\n\n\n27.4.7 Comparing performance\nTo assess relative performance, we again use bench::mark(), with twenty iterations per function (fig. 27.1).\n\nset.seed(777)\ntorch_manual_seed(777)\nlibrary(bench)\nlibrary(dplyr)\nlibrary(ggplot2)\n\nres &lt;- mark(dft(x),\n  dft_vec(x),\n  fft(x),\n  fft_vec(x),\n  fft_matrix(x),\n  torch_fft_fft(x),\n  iterations = 20,\n  check = FALSE\n)\n\nres %&gt;%\n  mutate(\n    expression =\n      forcats::fct_reorder(as.character(expression),\n        min,\n        .desc = TRUE\n      )\n  ) %&gt;%\n  as_bench_mark() %&gt;%\n  autoplot(type = \"ridge\") + theme_minimal()\n\n\n\n\nFigure 27.1: Benchmarking various FFT and DFT implementations (see text). Also includes torch_fft_fft() for reference.\n\n\nUnsurprisingly, none of the implementations comes close to torch_fft_fft(). Intriguingly, though, we see enormous differences in execution time between our manual implementations. There is an important insight to be had. Algorithmic properties are like Platonic ideas: pure and noble in theory, but often, hard to enliven in the lowlands of everyday life. Here, the hard facts of reality concern the defining features of the software stack one is using, and in particular, the characteristics of the programming language involved. Let me elaborate.\nIn torch as well as in R, the language, vectorized operations are much more efficient than iteration (let alone recursion). In that light, we can make sense of the results like so:\n\ndft() and fft(), the straightforward realizations, are slowest, since they don’t use vectorized operations. Among those two, fft(), which – in theory – should be superior, is severely punished for relying on recursion, whose use is strongly discouraged in R.\nBoth vectorized implementations, dft_vec() and fft_vec(), perform decidedly better than their un-vectorized counterparts. Among these, fft_vec() is a lot faster than dft_vec(), which is exactly what we’d like to see.\nGiven its conceptual and aesthetic appeal, the outcome for fft_matrix() is a bit disappointing.\n\nInspired by wishful thinking, is there anything we could do?\n\n\n27.4.8 Making use of Just-in-Time (JIT) compilation\nWe can try. In torch, a functionality named Just-in-Time (JIT) compilation allows us to trace a function, or a model, to obtain a highly optimized graph representation. This capability is useful in a number of ways: to reduce execution time, evidently; but also if, for example, you’d like to deploy a model (trained in R) in an environment that does not have R installed. If you’re interested, please see the vignette, as well as a dedicated blog post. Here, we’ll just directly put JIT compilation to practical use.\nAll we have to do is call a function, jit_trace(), with a dummy tensor, whose shape has to match the shape of future intended inputs. We want to do this for the two most promising FFT implementations, fft_vec() and fft_matrix(), as well as torch’s own torch_fft_fft(). The one thing to be aware of is that, should we think the improvement is worth it, we will need to trace the function for all input shapes we’re interested in. (Considering that our implementations expect input size to be a power of two, that wouldn’t be too much of a problem.)\nAbove, the input size we used for benchmarking amounted to 2^13, so this is what we’ll use for tracing, too. Here, first, we run jit_trace() for fft_vec() and torch_fft_fft():\n\nx_trace &lt;- torch_randn(2^13)\n\nfft_vec_jit &lt;- jit_trace(fft_vec, x_trace)\nfft_fft_jit &lt;- jit_trace(torch_fft_fft, x_trace)\n\nThe case of fft_matrix(), however, is a bit special: Inside, we’re calling bitrevorder(), a function that should not be part of the torch graph. A workaround, however, is quickly found: Just move that part of the logic outside the function – it scarcely contributes to execution time, anyway. The redefined function now looks like this:\n\nfft_matrix_for_jit &lt;- function(x) {\n  x &lt;- x$to(dtype = torch_cfloat())\n\n  n_samples &lt;- length(x)\n\n  # smallest butterfly matrix, needed for all valid inputs\n  B2 &lt;- torch_tensor(\n    c(1, 1, 1, -1),\n    dtype = torch_cfloat()\n  )$view(c(2, 2))\n  B2_block &lt;- torch_block_diag(\n    B2$`repeat`(c(n_samples / 2, 1))$split(2)\n  )\n  acc &lt;- torch_matmul(B2_block, x)\n\n  # iterative implementation then starts with B4\n  n &lt;- 4\n\n  while (n &lt;= n_samples) {\n\n    # build up current butterfly matrix\n    I &lt;- torch_eye(n / 2)\n    O &lt;- torch_diag(\n      torch_exp(\n        -1i * 2 * pi *\n          torch_arange(0, n / 2 - 1) / (n / 2 * 2)\n      )\n    )\n    B &lt;- torch_cat(list(\n      torch_cat(list(I, O), dim = 2),\n      torch_cat(list(I, -O), dim = 2)\n    ), dim = 1)\n\n    # in the final multiplication,\n    # B directly matches input length\n    if (n == n_samples) {\n      return(torch_matmul(B, acc))\n    }\n\n    # create block-diagonal matrix from butterflies\n    # at each iteration,\n    # we need to replicate B {n_samples/rank(B) times}\n    # this is achieved by first repeating B row-wise,\n    # then splitting up into rank(n) parts)\n    B_block &lt;- torch_block_diag(\n      B$`repeat`(c(n_samples / n, 1))$split(n)\n    )\n    acc &lt;- torch_matmul(B_block, acc)\n    n &lt;- n * 2\n  }\n  acc\n}\n\nWe trace it with pre-sorted input:\n\nfft_matrix_jit &lt;- jit_trace(\n  fft_matrix_for_jit,\n  torch_tensor(\n    bitrevorder(as.numeric(x_trace)),\n    dtype = torch_cfloat()\n  )\n)\n\nIn benchmarking, we call fft_matrix_jit() with a pre-sorted tensor, as well. Here, then, is a comparison of fft_vec(), fft_matrix(), torch_fft_fft(), and their respective optimized versions (fig. 27.2):\n\nx_rev &lt;- torch_tensor(\n  bitrevorder(\n    as.numeric(x_trace)\n  ),\n  dtype = torch_cfloat()\n)\n\nres &lt;- mark(fft_vec(x),\n  fft_matrix(x),\n  torch_fft_fft(x),\n  fft_vec_jit(x),\n  fft_matrix_jit(x_rev),\n  fft_fft_jit(x),\n  iterations = 20,\n  check = FALSE\n)\n\nres %&gt;%\n  mutate(\n    expression = forcats::fct_reorder(\n      as.character(expression),\n      min,\n      .desc = TRUE\n    )\n  ) %&gt;%\n  as_bench_mark() %&gt;%\n  autoplot(type = \"ridge\") + theme_minimal()\n\n\n\n\nFigure 27.2: Exploring the effect of Just-in-Time Compilation (JIT) on the performance of fft_vec(), fft_matrix(), and torch_fft_fft().\n\n\nTwo things, I’d say, are remarkable about this result. First, each of the three implementations benefits from getting JIT-compiled, even torch_fft_fft().\nSecond, there is one that profits a lot: namely, fft_vec(). In its compiled version, it is nearly as fast as torch_fft_fft() – a function that, remember, has all calculations done in highly optimized C++ code. This again underlines the enormous impact “linguistic fit” has on final performance outcomes.\nBy now, we’ve learned a lot about the Discrete Fourier Transform, including how to implement it in an efficient way. With all that knowledge, we should feel comfortable enough making direct use of torch_fft_ftt(). This is exactly what we’ll do in the next – and final – chapter: We’ll complement classic Fourier Analysis with its younger counterpart, wavelets.\n\n\n\n\nOsgood, Brad. 2019. Lectures on the Fourier Transform and Its Applications. American Mathematical Society."
  },
  {
    "objectID": "fourier_transform_fft.html#footnotes",
    "href": "fourier_transform_fft.html#footnotes",
    "title": "27  The Fast Fourier Transform (FFT)",
    "section": "",
    "text": "Note that now we have a scalar, not a vector.↩︎"
  },
  {
    "objectID": "wavelets.html#introducing-the-morlet-wavelet",
    "href": "wavelets.html#introducing-the-morlet-wavelet",
    "title": "28  Wavelets",
    "section": "28.1 Introducing the Morlet wavelet",
    "text": "28.1 Introducing the Morlet wavelet\nThe Morlet, also known as Gabor1, wavelet is defined like so:\n\\[\n\\Psi_{\\omega_{a},K,t_{k}}(t_n) = (e^{-i \\omega_{a} (t_n - t_k)} - e^{-K^2}) \\ e^{- \\omega_a^2 (t_n - t_k )^2 /(2K )^2}\n\\tag{28.1}\\]\nThis formulation pertains to discretized data, the kinds of data we work with in practice. Thus, \\(t_k\\) and \\(t_n\\) designate points in time, or equivalently, individual time-series samples.\nThis equation looks daunting at first, but we can “tame” it a bit by analyzing its structure, and pointing to the main actors. For concreteness, though, we first look at an example wavelet.\nWe start by implementing Equation 28.1:\n\nlibrary(torch)\nlibrary(ggplot2)\nlibrary(patchwork)\nlibrary(tidyr)\nlibrary(zeallot)\n  \nmorlet &lt;- function(omega, K, t_k, t) {\n  (torch_exp(-1i * omega * (t - t_k)) -\n    torch_exp(-torch_square(K))) *\n    torch_exp(-torch_square(omega) * torch_square(t - t_k) /\n      torch_square(2 * K))\n}\n\nComparing code and mathematical formulation, we notice a difference. The function itself takes one argument, \\(t_n\\); its realization, four (omega, K, t_k, and t). This is because the torch code is vectorized: On the one hand, omega, K, and t_k, which, in Equation 28.1, correspond to \\(\\omega_{a}\\), \\(K\\), and \\(t_k\\) , are scalars. (In the equation, they’re assumed to be fixed.) t, on the other hand, is a vector; it will hold the measurement times of the series to be analyzed.\nWe pick example values for omega, K, and t_k, as well as a range of times to evaluate the wavelet on, and plot its values (fig. 28.1):\n\nomega &lt;- 6 * pi\nK &lt;- 6\nt_k &lt;- 5\n \nsample_time &lt;- torch_arange(3, 7, 0.0001)\n\ncreate_wavelet_plot &lt;- function(omega, K, t_k, sample_time) {\n  morlet &lt;- morlet(omega, K, t_k, sample_time)\n  df &lt;- data.frame(\n    x = as.numeric(sample_time),\n    real = as.numeric(morlet$real),\n    imag = as.numeric(morlet$imag)\n  ) %&gt;%\n    pivot_longer(-x, names_to = \"part\", values_to = \"value\")\n  ggplot(df, aes(x = x, y = value, color = part)) +\n    geom_line() +\n    scale_colour_grey(start = 0.8, end = 0.4) +\n    xlab(\"time\") +\n    ylab(\"wavelet value\") +\n    ggtitle(\"Morlet wavelet\",\n      subtitle = paste0(\"ω_a = \", omega / pi, \"π , K = \", K)\n    ) +\n    theme_minimal()\n}\n\ncreate_wavelet_plot(omega, K, t_k, sample_time)\n\n\n\n\nFigure 28.1: A Morlet wavelet.\n\n\nWhat we see here is a complex sine curve – note the real and imaginary parts, separated by a phase shift of \\(\\pi/2\\) – that decays on both sides of the center. Looking back at Equation 28.1, we can identify the factors responsible for both features. The first term in the equation, \\(e^{-i \\omega_{a} (t_n - t_k)}\\), generates the oscillation; the third, \\(e^{- \\omega_a^2 (t_n - t_k )^2 /(2K )^2}\\), causes the exponential decay away from the center. (In case you’re wondering about the second term, \\(e^{-K^2}\\): For given \\(K\\), it is just a constant.)\nThe third term actually is a Gaussian, with location parameter \\(t_k\\) and scale \\(K\\). We’ll talk about \\(K\\) in great detail soon, but what’s with \\(t_k\\)? \\(t_k\\) is the center of the wavelet; for the Morlet wavelet, this is also the location of maximum amplitude. As distance from the center increases, values quickly approach zero. This is what is meant by wavelets being localized: They are “active” only on a short range of time."
  },
  {
    "objectID": "wavelets.html#the-roles-of-k-and-omega_a",
    "href": "wavelets.html#the-roles-of-k-and-omega_a",
    "title": "28  Wavelets",
    "section": "28.2 The roles of \\(K\\) and \\(\\omega_a\\)",
    "text": "28.2 The roles of \\(K\\) and \\(\\omega_a\\)\nNow, we already said that \\(K\\) is the scale of the Gaussian; it thus determines how far the curve spreads out in time. But there is also \\(\\omega_a\\). Looking back at the Gaussian term, it, too, will impact the spread.\nFirst though, what is \\(\\omega_a\\)? The subscript \\(a\\) stands for “analysis”; thus, \\(\\omega_a\\) denotes a single frequency being probed. At this point, a quick aside on workflow – this will become much clearer later, when we actually run the Wavelet Transform):\n\nA single transform, resulting in spectrogram-like, three-dimensional output, operates on a set of analysis frequencies (\\(\\omega_a\\)), and covers the whole time range (\\(t_n\\)).\nThat covering is achieved by having the wavelet slide over the input, its position at each step being identified by its center, \\(t_k\\).\nWhat’s missing from this characterization is \\(K\\). That’s because you run a separate wavelet analysis for each \\(K\\) of interest.\n\nNow, let’s first inspect visually the respective impacts of \\(\\omega_a\\) and \\(K\\) (fig. 28.2).\n\np1 &lt;- create_wavelet_plot(6 * pi, 4, 5, sample_time)\np2 &lt;- create_wavelet_plot(6 * pi, 6, 5, sample_time)\np3 &lt;- create_wavelet_plot(6 * pi, 8, 5, sample_time)\np4 &lt;- create_wavelet_plot(4 * pi, 6, 5, sample_time)\np5 &lt;- create_wavelet_plot(6 * pi, 6, 5, sample_time)\np6 &lt;- create_wavelet_plot(8 * pi, 6, 5, sample_time)\n\n(p1 | p4) /\n  (p2 | p5) /\n  (p3 | p6)\n\n\n\n\nFigure 28.2: Morlet wavelet: Effects of varying scale and analysis frequency.\n\n\nIn the left column, we keep \\(\\omega_a\\) constant, and vary \\(K\\). On the right, \\(\\omega_a\\) changes, and \\(K\\) stays the same.\nFirstly, we observe that the higher \\(K\\), the more the curve gets spread out. In a wavelet analysis, this means that more points in time will contribute to the transform’s output, resulting in high precision as to frequency content, but loss of resolution in time. (We’ll return to this – central – trade-off soon.)\nAs to \\(\\omega_a\\), its impact is twofold. On the one hand, in the Gaussian term, it counteracts – exactly, even – the scale parameter, \\(K\\). On the other, it determines the frequency, or equivalently, the period, of the wave. To see this, take a look at the right column. Corresponding to the different frequencies, we have, in the interval between 4 and 6, four, six, or eight peaks, respectively.\nThis double role of \\(\\omega_a\\) is the reason why, all-in-all, it does make a difference whether we shrink \\(K\\), keeping \\(\\omega_a\\) constant, or increase \\(\\omega_a\\), holding \\(K\\) fixed.\nThis state of things sounds complicated, but is less problematic than it might seem. In practice, understanding the role of \\(K\\) is important, since we need to pick sensible \\(K\\) values to try. As to the \\(\\omega_a\\), on the other hand, there will be a multitude of them, corresponding to the range of frequencies we analyze.\nSo we can understand the impact of \\(K\\) in more detail, we need to take a first look at the Wavelet Transform."
  },
  {
    "objectID": "wavelets.html#wavelet-transform-a-straightforward-implementation",
    "href": "wavelets.html#wavelet-transform-a-straightforward-implementation",
    "title": "28  Wavelets",
    "section": "28.3 Wavelet Transform: A straightforward implementation",
    "text": "28.3 Wavelet Transform: A straightforward implementation\nWhile overall, the topic of wavelets is more multifaceted, and thus, may seem more enigmatic than Fourier analysis, the transform itself is easier to grasp. It is a sequence of local convolutions between wavelet and signal. Here is the formula for specific scale parameter \\(K\\), analysis frequency \\(\\omega_a\\), and wavelet location \\(t_k\\):\n\\[\nW_{K, \\omega_a, t_k} = \\sum_n  x_n \\Psi_{\\omega_{a},K,t_{k}}^*(t_n)\n\\]\nThis is just a dot product, computed between signal and complex-conjugated wavelet. (Here complex conjugation flips the wavelet in time, making this convolution, not correlation – a fact that matters a lot, as you’ll see soon.)\nCorrespondingly, straightforward implementation results in a sequence of dot products, each corresponding to a different alignment of wavelet and signal. Below, in wavelet_transform(), arguments omega and K are scalars, while x, the signal, is a vector. The result is the wavelet-transformed signal, for some specific K and omega of interest.\n\nwavelet_transform &lt;- function(x, omega, K) {\n  n_samples &lt;- dim(x)[1]\n  W &lt;- torch_complex(\n    torch_zeros(n_samples), torch_zeros(n_samples)\n  )\n  for (i in 1:n_samples) {\n    # move center of wavelet\n    t_k &lt;- x[i, 1]\n    m &lt;- morlet(omega, K, t_k, x[, 1])\n    # compute local dot product\n    # note wavelet is conjugated\n    dot &lt;- torch_matmul(\n      m$conj()$unsqueeze(1),\n      x[, 2]$to(dtype = torch_cfloat())\n    )\n    W[i] &lt;- dot\n  }\n  W\n}\n\nTo test this, we generate a simple sine wave that has a frequency of 100 Hertz in its first part, and double that in the second (fig. 28.3).\n\ngencos &lt;- function(amp, freq, phase, fs, duration) {\n  x &lt;- torch_arange(0, duration, 1 / fs)[1:-2]$unsqueeze(2)\n  y &lt;- amp * torch_cos(2 * pi * freq * x + phase)\n  torch_cat(list(x, y), dim = 2)\n}\n\n# sampling frequency\nfs &lt;- 8000\n\nf1 &lt;- 100\nf2 &lt;- 200\nphase &lt;- 0\nduration &lt;- 0.25\n\ns1 &lt;- gencos(1, f1, phase, fs, duration)\ns2 &lt;- gencos(1, f2, phase, fs, duration)\n\ns3 &lt;- torch_cat(list(s1, s2), dim = 1)\ns3[(dim(s1)[1] + 1):(dim(s1)[1] * 2), 1] &lt;-\n  s3[(dim(s1)[1] + 1):(dim(s1)[1] * 2), 1] + duration\n\ndf &lt;- data.frame(\n  x = as.numeric(s3[, 1]),\n  y = as.numeric(s3[, 2])\n)\nggplot(df, aes(x = x, y = y)) +\n  geom_line() +\n  xlab(\"time\") +\n  ylab(\"amplitude\") +\n  theme_minimal()\n\n\n\n\nFigure 28.3: An example signal, consisting of a low-frequency and a high-frequency half.\n\n\nNow, we run the Wavelet Transform on this signal, for an analysis frequency of 100 Hertz, and with a K parameter of 2, found through quick experimentation (fig. 28.4):\n\nK &lt;- 2\nomega &lt;- 2 * pi * f1\n\nres &lt;- wavelet_transform(x = s3, omega, K)\ndf &lt;- data.frame(\n  x = as.numeric(s3[, 1]),\n  y = as.numeric(res$abs())\n)\n\nggplot(df, aes(x = x, y = y)) +\n  geom_line() +\n  xlab(\"time\") +\n  ylab(\"Wavelet Transform\") +\n  theme_minimal()\n\n\n\n\nFigure 28.4: Wavelet Transform of the above two-part signal. Analysis frequency is 100 Hertz.\n\n\nThe transform correctly picks out the part of the signal that matches the analysis frequency. If you feel like, you might want to double-check what happens for an analysis frequency of 200 Hertz.\nNow, in reality we will want to run this analysis not for a single frequency, but a range of frequencies we’re interested in. And we will want to try different scales K. Now, if you executed the code above, you might be worried that this could take a lot of time.\nWell, it by necessity takes longer to compute than its Fourier analogue, the spectrogram. For one, that’s because with spectrograms, the analysis is “just” two-dimensional, the axes being time and frequency. With wavelets there are, in addition, different scales to be explored. And secondly, spectrograms operate on whole windows (with configurable overlap); a wavelet, on the other hand, slides over the signal in unit steps.\nStill, the situation is not as grave as it sounds. The Wavelet Transform being a convolution, we can implement it in the Fourier domain instead. We’ll do that very soon, but first, as promised, let’s revisit the topic of varying K."
  },
  {
    "objectID": "wavelets.html#resolution-in-time-versus-in-frequency",
    "href": "wavelets.html#resolution-in-time-versus-in-frequency",
    "title": "28  Wavelets",
    "section": "28.4 Resolution in time versus in frequency",
    "text": "28.4 Resolution in time versus in frequency\nWe already saw that the higher K, the more spread-out the wavelet. We can use our first, maximally straightforward, example, to investigate one immediate consequence. What, for example, happens for K set to twenty (fig. 28.5)?\n\nK &lt;- 20\n\nres &lt;- wavelet_transform(x = s3, omega, K)\ndf &lt;- data.frame(\n  x = as.numeric(s3[, 1]),\n  y = as.numeric(res$abs())\n)\n\nggplot(df, aes(x = x, y = y)) +\n  geom_line() +\n  xlab(\"time\") +\n  ylab(\"Wavelet Transform\") +\n  theme_minimal()\n\n\n\n\nFigure 28.5: Wavelet Transform of the above two-part signal, with K set to twenty instead of two.\n\n\nThe Wavelet Transform still picks out the correct region of the signal – but now, instead of a rectangle-like result, we get a significantly smoothed version that does not sharply separate the two regions.\nNotably, the first 0.05 seconds, too, show considerable smoothing. The larger a wavelet, the more element-wise products will be lost at the end and the beginning. This is because transforms are computed aligning the wavelet at all signal positions, from the very first to the last. Concretely, when we compute the dot product at location t_k = 1, just a single sample of the signal is considered.\nApart from possibly introducing unreliability at the boundaries, how does wavelet scale affect the analysis? Well, since we’re correlating (convolving, technically; but in this case, the effect, in the end, is the same) the wavelet with the signal, point-wise similarity is what matters. Concretely, assume the signal is a pure sine wave, the wavelet we’re using is a windowed sinusoid like the Morlet, and that we’ve found an optimal K that nicely captures the signal’s frequency. Then any other K, be it larger or smaller, will result in less point-wise overlap.\nThis leads to a question we can’t get around."
  },
  {
    "objectID": "wavelets.html#how-is-this-different-from-a-spectrogram",
    "href": "wavelets.html#how-is-this-different-from-a-spectrogram",
    "title": "28  Wavelets",
    "section": "28.5 How is this different from a spectrogram?",
    "text": "28.5 How is this different from a spectrogram?\nChances are you have been mentally comparing what we’re doing here not with the Fourier Transform per se, but with its windowed progeny, the spectrogram. Both methods tell us how frequency composition varies over time. Both involve trade-offs between resolution in frequency and in time. How, then, are they different?\nLet’s start with the way spectrograms are normally computed. Usually, the choice of window size is the same for all frequency ranges. Then, resolution of high frequencies will be better than that of lower ones, since more periods fit into one analysis window.\nTo counteract this, it would be possible to choose different window sizes for different ranges of frequencies. But then, how do you make those choices? It is a lot more convenient to, in wavelet analysis, experiment with different K, all the more since with growing experience, those explorations will resemble to trial-and-error endeavors less and less."
  },
  {
    "objectID": "wavelets.html#performing-the-wavelet-transform-in-the-fourier-domain",
    "href": "wavelets.html#performing-the-wavelet-transform-in-the-fourier-domain",
    "title": "28  Wavelets",
    "section": "28.6 Performing the Wavelet Transform in the Fourier domain",
    "text": "28.6 Performing the Wavelet Transform in the Fourier domain\nSoon, we will run the Wavelet Transform on a longer signal. Thus, it is time to speed up computation. We already said that here, we benefit from time-domain convolution being equivalent to multiplication in the Fourier domain. The overall process then is this: First, compute the DFT of both signal and wavelet; second, multiply the results; third, inverse-transform back to the time domain.\nWith the Morlet wavelet, we don’t even have to run the FFT: Its Fourier-domain representation can be stated in closed form. We’ll just make use of that formulation from the outset.\nTo see how this works, we first manually step through the process. In the following section, we’ll package things up for quick execution.\nHere, first, is the signal. It again is composed of two different frequencies, 100 Hertz and 200 Hertz. But this time, we repeat the alternation several times, and map those frequencies to different amplitudes. The latter may seem like an innocent change, but there is an important consequence: We have to adapt our cosine-generating code to keep track of the phase. If we don’t do this, the signal will be distorted at the transition points.\n\ngencosines &lt;- function(freqs, amps, samples_per_frame, fs) {\n  t &lt;- (1:samples_per_frame) / fs\n  lastphase &lt;- 0\n  x &lt;- torch_zeros(length(freqs), samples_per_frame)\n\n  for (i in 1:length(freqs)) {\n    amp &lt;- torch_ones(samples_per_frame) * amps[i]\n    freq &lt;- torch_ones(samples_per_frame) * freqs[i]\n\n    phase &lt;- torch_tensor(2 * pi * freq * t + lastphase)\n    x_frame &lt;- amp * torch_cos(phase)\n\n    # save phase to be used in next frame\n    lastphase &lt;- as.numeric(phase[samples_per_frame]) %%\n      (2 * pi)\n    x[i, ] &lt;- x_frame\n  }\n\n  x$reshape(length(freqs) * samples_per_frame)\n}\n\nHere is the signal (fig. 28.6):\n\nfreqs &lt;- c(100, 200, 100, 200, 100, 200, 100)\namps &lt;- c(1.2, 0.8, 1.2, 0.8, 1.2, 0.8, 1.2)\nsamples_per_frame &lt;- 100\nfs &lt;- 800\n\nx &lt;- gencosines(freqs, amps, samples_per_frame, fs)\ndf &lt;- data.frame(x = 1:dim(x)[1], y = as.numeric(x))\nggplot(df, aes(x = x, y = y)) +\n  geom_line() +\n  xlab(\"sample\") +\n  ylab(\"amplitude\") +\n  theme_minimal()\n\n\n\n\nFigure 28.6: A signal, consisting of alternating, different-amplitude low-frequency and high-frequency halves.\n\n\nThe DFT of the signal is quickly computed:\n\nF &lt;- torch_fft_fft(x)\n\nThe Fourier-domain representation of the wavelet, on the other hand, is given by:\n\nmorlet_fourier &lt;- function(K, omega_a, omega) {\n  2 * (torch_exp(-torch_square(\n    K * (omega - omega_a) / omega_a\n  )) -\n    torch_exp(-torch_square(K)) *\n      torch_exp(-torch_square(K * omega / omega_a)))\n}\n\nComparing this statement of the wavelet to the time-domain one, we see that – as expected – instead of parameters t and t_k it now takes omega and omega_a. The latter, omega_a, is the analysis frequency, the one we’re probing for, a scalar; the former, omega, the range of frequencies that appear in the DFT of the signal.\nIn instantiating the wavelet, there is one thing we need to pay special attention to. In FFT-think, the frequencies are bins; their number is determined by the length of the signal (a length that, for its part, directly depends on sampling frequency). Our wavelet, on the other hand, works with frequencies in Hertz (nicely, from a user’s perspective; since this unit is meaningful to us). What this means is that to morlet_fourier, as omega_a we need to pass not the value in Hertz, but the corresponding FFT bin. Conversion is done relating the number of bins, dim(x)[1], to the sampling frequency of the signal, fs:\n\n# again look for 100Hz parts\nomega &lt;- 2 * pi * f1\n\n# need the bin corresponding to some frequency in Hz\nomega_bin &lt;- f1/fs * dim(x)[1]\n\nWe instantiate the wavelet, perform the Fourier-domain multiplication, and inverse-transform the result:\n\nK &lt;- 3\n\nm &lt;- morlet_fourier(K, omega_bin, 1:dim(x)[1])\nprod &lt;- F * m\ntransformed &lt;- torch_fft_ifft(prod)\n\nAnd this is the result (fig. 28.7):\n\ndf &lt;- data.frame(\n  x = as.numeric(1:dim(x)[1]) / fs,\n  y = as.numeric(transformed$abs())\n)\nggplot(df, aes(x = x, y = y)) +\n  geom_line() +\n  xlab(\"time\") +\n  ylab(\"Wavelet Transform\") +\n  theme_minimal()\n\n\n\n\nFigure 28.7: Wavelet Transform of the above alternating-frequency signal. Analysis frequency is 100 Hertz.\n\n\nAll 100-Hertz-regions are identified flawlessly. Also, looking at the y-axis, we see that the amplitude is correct.\nPutting together wavelet instantiation and the steps involved in the analysis, we have the following. (Note how to wavelet_transform_fourier, we now, conveniently, pass in the frequency value in Hertz.)\n\nwavelet_transform_fourier &lt;- function(x, omega_a, K, fs) {\n  N &lt;- dim(x)[1]\n  omega_bin &lt;- omega_a / fs * N\n  m &lt;- morlet_fourier(K, omega_bin, 1:N)\n  x_fft &lt;- torch_fft_fft(x)\n  prod &lt;- x_fft * m\n  w &lt;- torch_fft_ifft(prod)\n  w\n}\n\nLet’s use this to probe for the second frequency used, 200 Hertz (fig. 28.8).\n\nK &lt;- 6\n\ntransformed &lt;- wavelet_transform_fourier(x, f2, K, fs)\ndf &lt;- data.frame(\n  x = 1:dim(x)[1] / fs,\n  y = as.numeric(transformed$abs())\n)\nggplot(df, aes(x = x, y = y)) +\n  geom_line() +\n  xlab(\"time\") +\n  ylab(\"Wavelet Transform\") +\n  theme_minimal()\n\n\n\n\nFigure 28.8: Wavelet Transform of the above alternating-frequency signal. Analysis frequency is 200 Hertz.\n\n\nAgain, we find that matching regions, as well as the amplitude, are correctly identified.\nWe’ve already made significant progress. We’re ready for the final step: automating analysis over a range of frequencies of interest. This will result in a three-dimensional representation, the wavelet diagram."
  },
  {
    "objectID": "wavelets.html#creating-the-wavelet-diagram",
    "href": "wavelets.html#creating-the-wavelet-diagram",
    "title": "28  Wavelets",
    "section": "28.7 Creating the wavelet diagram",
    "text": "28.7 Creating the wavelet diagram\nIn the Fourier Transform, the number of coefficients we obtain depends on signal length, and effectively reduces to half the sampling frequency. With its wavelet analogue, since anyway we’re doing a loop over frequencies, we might as well decide which frequencies to analyze.\nFirstly, the range of frequencies of interest can be determined running the DFT. The next question, then, is about granularity. Here, I’ll be following the recommendation given in Vistnes’ book, which is based on the relation between current frequency value and wavelet scale, K.\nIteration over frequencies is then implemented as a loop:\n\nwavelet_grid &lt;- function(x, K, f_start, f_end, fs) {\n  # downsample analysis frequency range\n  # as per Vistnes, eq. 14.17\n  num_freqs &lt;- 1 + log(f_end / f_start)/ log(1 + 1/(8 * K))\n  freqs &lt;- seq(f_start, f_end, length.out = floor(num_freqs))\n  \n  transformed &lt;- torch_zeros(\n    num_freqs, dim(x)[1],\n    dtype = torch_cfloat()\n    )\n  for(i in 1:num_freqs) {\n    w &lt;- wavelet_transform_fourier(x, freqs[i], K, fs)\n    transformed[i, ] &lt;- w\n  }\n  list(transformed, freqs)\n}\n\nCalling wavelet_grid() will give us the analysis frequencies used, together with the respective outputs from the Wavelet Transform.\nNext, we create a utility function that visualizes the result. By default, plot_wavelet_diagram() displays the magnitude of the wavelet-transformed series; it can, however, plot the squared magnitudes, too, as well as their square root, a method much recommended by Vistnes whose effectiveness we will soon have opportunity to witness.\nThe function deserves a few further comments.\nFirstly, same as we did with the analysis frequencies, we down-sample the signal itself, avoiding to suggest a resolution that is not actually present. The formula, again, is taken from Vistnes’ book.\nThen, we use interpolation to obtain a new time-frequency grid. This step may even be necessary if we keep the original grid, since when distances between grid points are very small, R’s image() may refuse to accept axes as evenly spaced.\nFinally, note how frequencies are arranged on a log scale. This leads to much more useful visualizations.\n\nplot_wavelet_diagram &lt;- function(x,\n                                 freqs,\n                                 grid,\n                                 K,\n                                 fs,\n                                 f_end,\n                                 type = \"magnitude\") {\n  grid &lt;- switch(type,\n    magnitude = grid$abs(),\n    magnitude_squared = torch_square(grid$abs()),\n    magnitude_sqrt = torch_sqrt(grid$abs())\n  )\n\n  # downsample time series\n  # as per Vistnes, eq. 14.9\n  new_x_take_every &lt;- max(K / 24 * fs / f_end, 1)\n  new_x_length &lt;- floor(dim(grid)[2] / new_x_take_every)\n  new_x &lt;- torch_arange(\n    x[1],\n    x[dim(x)[1]],\n    step = x[dim(x)[1]] / new_x_length\n  )\n  \n  # interpolate grid\n  new_grid &lt;- nnf_interpolate(\n    grid$view(c(1, 1, dim(grid)[1], dim(grid)[2])),\n    c(dim(grid)[1], new_x_length)\n  )$squeeze()\n  out &lt;- as.matrix(new_grid)\n\n  # plot log frequencies\n  freqs &lt;- log10(freqs)\n  \n  image(\n    x = as.numeric(new_x),\n    y = freqs,\n    z = t(out),\n    ylab = \"log frequency [Hz]\",\n    xlab = \"time [s]\",\n    col = hcl.colors(12, palette = \"Light grays\")\n  )\n  main &lt;- paste0(\"Wavelet Transform, K = \", K)\n  sub &lt;- switch(type,\n    magnitude = \"Magnitude\",\n    magnitude_squared = \"Magnitude squared\",\n    magnitude_sqrt = \"Magnitude (square root)\"\n  )\n\n  mtext(side = 3, line = 2, at = 0, adj = 0, cex = 1.3, main)\n  mtext(side = 3, line = 1, at = 0, adj = 0, cex = 1, sub)\n}\n\nNow, let’s see a few wavelet diagrams. We are going to compare choices in two categories. The first concerns what is displayed: magnitude, magnitude squared, or the square root. Since options are finite, in practice you could always try them all. The second, however, is essential: It is about K, the scale of the Morlet wavelet.\nThough, in theory, the range of possible Ks is infinite, as soon as we start to enlarge or shrink K we see the respective effects very quickly. And in a publication you can, of course, provide diagrams for a few different K. (Maybe you even should; the information conveyed might be complementary.)\nWe start with a fixed K of 12, and stay with the default display (which is magnitude; see fig. 28.9). As to the range of frequencies to be analysed, it directly follows from our having generated the data ourselves.\n\nf_start &lt;- 70\nf_end &lt;- 230\n\nK &lt;- 12\nc(grid, freqs) %&lt;-% wavelet_grid(\n  x, K, f_start, f_end, fs\n)\nplot_wavelet_diagram(\n  torch_tensor(1:dim(grid)[2]), freqs, grid, K, fs, f_end\n)\n\n\n\n\nFigure 28.9: Wavelet diagram of the above alternating-frequency signal for K = 12. Displaying magnitude as per default.\n\n\nNow compare this with how it looks for squared magnitudes (fig. 28.10).\n\nplot_wavelet_diagram(\n  torch_tensor(1:dim(grid)[2]), freqs, grid, K, fs, f_end,\n  type = \"magnitude_squared\"\n)\n\n\n\n\nFigure 28.10: Wavelet diagram of the above alternating-frequency signal for K = 12. Displaying magnitude squared.\n\n\nAs well as square roots (fig. 28.11):\n\nplot_wavelet_diagram(\n  torch_tensor(1:dim(grid)[2]), freqs, grid, K, fs, f_end,\n  type = \"magnitude_sqrt\"\n)\n\n\n\n\nFigure 28.11: Wavelet diagram of the above alternating-frequency signal for K = 12. Displaying the square root of the magnitude.\n\n\nIn this case it is clear which display mode works best. But that’s just because we’ve created a very regular signal.\nNow, let’s see what happens for smaller K: 6, say. We only plot magnitude squared (fig. 28.12).\n\nK &lt;- 6\nc(grid, freqs) %&lt;-% wavelet_grid(\n  x, K, f_start, f_end, fs\n)\nplot_wavelet_diagram(\n  torch_tensor(1:dim(grid)[2]), freqs, grid, K, fs, f_end,\n  type = \"magnitude_squared\"\n)\n\n\n\n\nFigure 28.12: Wavelet diagram of the above alternating-frequency signal for K = 6.\n\n\nWhile frequency resolution clearly got worse, there is no complementary improvement in how time is handled. Again, this is no surprise, the signal being what it is.\nWhat about the other direction? For K = 24 we have (fig. 28.13):\n\nK &lt;- 24\nc(grid, freqs) %&lt;-% wavelet_grid(\n  x, K, f_start, f_end, fs\n)\nplot_wavelet_diagram(\n  torch_tensor(1:dim(grid)[2]), freqs, grid, K, fs, f_end,\n  type = \"magnitude_squared\"\n)\n\n\n\n\nFigure 28.13: Wavelet diagram of the above alternating-frequency signal for K = 24.\n\n\nAt this scale, we do see a decline in time resolution. Evidently, our initial choice of 12 was better. Just for curiosity’s sake, though, let’s see what would happen for yet bigger K (fig. 28.14).\n\nK &lt;- 48\nc(grid, freqs) %&lt;-% wavelet_grid(\n  x, K, f_start, f_end, fs\n)\nplot_wavelet_diagram(\n  torch_tensor(1:dim(grid)[2]), freqs, grid, K, fs, f_end,\n  type = \"magnitude_squared\"\n)\n\n\n\n\nFigure 28.14: Wavelet diagram of the above alternating-frequency signal for K = 48.\n\n\nThe result speaks for itself.\nFinally – now that we have available re-usable pieces of code, and acquired some first intuition on how time-frequency trade-off works for wavelets – we conclude with a real-world example."
  },
  {
    "objectID": "wavelets.html#a-real-world-example-chaffinchs-song",
    "href": "wavelets.html#a-real-world-example-chaffinchs-song",
    "title": "28  Wavelets",
    "section": "28.8 A real-world example: Chaffinch’s song",
    "text": "28.8 A real-world example: Chaffinch’s song\nFor the case study, I’ve chosen what, to me, was the most impressive wavelet analysis shown in Vistnes’ book. It’s a sample of a chaffinch’s singing, and it’s available on Vistnes’ website.\n\nurl &lt;- \"http://www.physics.uio.no/pow/wavbirds/chaffinch.wav\"\n\ndownload.file(\n file.path(url),\n destfile = \"resources/chaffinch.wav\"\n)\n\nWe use torchaudio to load the file, and convert from stereo to mono using tuneR’s appropriately named mono(). (For the kind of analysis we’re doing, there is no point in keeping two channels around.)\n\nlibrary(torchaudio)\nlibrary(tuneR)\n\nwav &lt;- tuneR_loader(\"resources/chaffinch.wav\")\nwav &lt;- mono(wav, \"both\")\nwav\n\nWave Object\n    Number of Samples:      1864548\n    Duration (seconds):     42.28\n    Samplingrate (Hertz):   44100\n    Channels (Mono/Stereo): Mono\n    PCM (integer format):   TRUE\n    Bit (8/16/24/32/64):    16 \nFor analysis, we don’t need the complete sequence. Helpfully, Vistnes also published a recommendation as to which range of samples to analyze.\n\nwaveform_and_sample_rate &lt;- transform_to_tensor(wav)\nx &lt;- waveform_and_sample_rate[[1]]$squeeze()\nfs &lt;- waveform_and_sample_rate[[2]]\n\n# http://www.physics.uio.no/pow/wavbirds/chaffinchInfo.txt\nstart &lt;- 34000\nN &lt;- 1024 * 128\nend &lt;- start + N - 1\nx &lt;- x[start:end]\n\ndim(x)\n\n[1] 131072\nSee fig. 28.15 for a time-domain view. (Don’t miss out on the occasion to actually listen to it, on your laptop.)\n\ndf &lt;- data.frame(x = 1:dim(x)[1], y = as.numeric(x))\nggplot(df, aes(x = x, y = y)) +\n  geom_line() +\n  xlab(\"sample\") +\n  ylab(\"amplitude\") +\n  theme_minimal()\n\n\n\n\nFigure 28.15: Chaffinch’s song.\n\n\nNow, we need to determine a reasonable range of analysis frequencies. To that end, we run the FFT:\n\nF &lt;- torch_fft_fft(x)\n\nOn the x-axis, we plot frequencies, not sample numbers, and for better visibility, we zoom in a bit (fig. 28.16).\n\nbins &lt;- 1:dim(F)[1]\nfreqs &lt;- bins / N * fs\n\n# the bin, not the frequency\ncutoff &lt;- N/4\n\ndf &lt;- data.frame(\n  x = freqs[1:cutoff],\n  y = as.numeric(F$abs())[1:cutoff]\n)\nggplot(df, aes(x = x, y = y)) +\n  geom_col() +\n  xlab(\"frequency (Hz)\") +\n  ylab(\"magnitude\") +\n  theme_minimal()\n\n\n\n\nFigure 28.16: Chaffinch’s song, Fourier spectrum (excerpt).\n\n\nBased on this distribution, we can safely restrict the range of analysis frequencies to between, approximately, 1800 and 8500 Hertz. (This is also the range recommended by Vistnes.)\nFirst, though, let’s anchor expectations by creating a spectrogram for this signal. Suitable values for FFT size and window size were found experimentally. And though, in spectrograms, you don’t see this done often, I found that displaying square roots of coefficient magnitudes yielded the most informative output.\n\nfft_size &lt;- 1024\nwindow_size &lt;- 1024\npower &lt;- 0.5\n\nspectrogram &lt;- transform_spectrogram(\n  n_fft = fft_size,\n  win_length = window_size,\n  normalized = TRUE,\n  power = power\n)\n\nspec &lt;- spectrogram(x)\ndim(spec)\n\n[1] 513 257\nLike we do with wavelet diagrams, we plot frequencies on a log scale (fig. 28.17).\n\nbins &lt;- 1:dim(spec)[1]\nfreqs &lt;- bins * fs / fft_size\nlog_freqs &lt;- log10(freqs)\n\nframes &lt;- 1:(dim(spec)[2])\nseconds &lt;- (frames / dim(spec)[2])  * (dim(x)[1] / fs)\n\nimage(x = seconds,\n      y = log_freqs,\n      z = t(as.matrix(spec)),\n      ylab = 'log frequency [Hz]',\n      xlab = 'time [s]',\n      col = hcl.colors(12, palette = \"Light grays\")\n)\nmain &lt;- paste0(\"Spectrogram, window size = \", window_size)\nsub &lt;- \"Magnitude (square root)\"\nmtext(side = 3, line = 2, at = 0, adj = 0, cex = 1.3, main)\nmtext(side = 3, line = 1, at = 0, adj = 0, cex = 1, sub)\n\n\n\n\nFigure 28.17: Chaffinch’s song, spectrogram.\n\n\nThe spectrogram already shows a distinctive pattern. Let’s see what can be done with wavelet analysis. Having experimented with a few different K, I agree with Vistnes that K = 48 makes for an excellent choice (fig. 28.18):\n\nf_start &lt;- 1800\nf_end &lt;- 8500\n\nK &lt;- 48\nc(grid, freqs) %&lt;-% wavelet_grid(x, K, f_start, f_end, fs)\nplot_wavelet_diagram(\n  torch_tensor(1:dim(grid)[2]),\n  freqs, grid, K, fs, f_end,\n  type = \"magnitude_sqrt\"\n)\n\n\n\n\nFigure 28.18: Chaffinch’s song, wavelet diagram.\n\n\nThe gain in resolution, on both the time and the frequency axis, is utterly impressive.\nWith this example, that hopefully has been an inspiring one, I conclude this chapter, and the book. I wish you all the best in your future endeavours with torch!\n\n\n\n\nVistnes, Arnt Inge. 2018. Physics of Oscillations and Waves. With Use of Matlab and Python. Springer."
  },
  {
    "objectID": "wavelets.html#footnotes",
    "href": "wavelets.html#footnotes",
    "title": "28  Wavelets",
    "section": "",
    "text": "After Dennis Gabor, who came up with the idea of using Gaussian-windowed complex exponentials for time-frequency decomposition, and Jean Morlet, who elaborated on and formalized the resulting transform.↩︎"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Bronstein, Michael M., Joan Bruna, Taco Cohen, and Petar Velickovic.\n2021. “Geometric Deep Learning: Grids, Groups, Graphs, Geodesics,\nand Gauges.” CoRR abs/2104.13478. https://arxiv.org/abs/2104.13478.\n\n\nCho, Dongjin, Cheolhee Yoo, Jungho Im, and Dong-Hyun Cha. 2020.\n“Comparative Assessment of Various Machine Learning-Based Bias\nCorrection Methods for Numerical Weather Prediction Model Forecasts of\nExtreme Air Temperatures in Urban Areas.” Earth and Space\nScience 7 (4): e2019EA000740. https://doi.org/https://doi.org/10.1029/2019EA000740.\n\n\nCho, Kyunghyun, Bart van Merrienboer, Çaglar Gülçehre, Fethi Bougares,\nHolger Schwenk, and Yoshua Bengio. 2014. “Learning Phrase\nRepresentations Using RNN Encoder-Decoder for Statistical\nMachine Translation.” CoRR abs/1406.1078. http://arxiv.org/abs/1406.1078.\n\n\nDumoulin, Vincent, and Francesco Visin. 2016. “A guide to convolution arithmetic for deep\nlearning.” arXiv e-Prints, March,\narXiv:1603.07285. https://arxiv.org/abs/1603.07285.\n\n\nHe, Kaiming, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2015.\n“Deep Residual Learning for Image Recognition.”\nCoRR abs/1512.03385. http://arxiv.org/abs/1512.03385.\n\n\nHochreiter, Sepp, and Jürgen Schmidhuber. 1997. “Long Short-Term\nMemory.” Neural Computation 9 (8): 1735–80.\n\n\nIoffe, Sergey, and Christian Szegedy. 2015. “Batch Normalization:\nAccelerating Deep Network Training by Reducing Internal Covariate\nShift.” https://arxiv.org/abs/1502.03167.\n\n\nLoshchilov, Ilya, and Frank Hutter. 2016. “SGDR:\nStochastic Gradient Descent with Restarts.” CoRR\nabs/1608.03983. http://arxiv.org/abs/1608.03983.\n\n\nOlah, Chris, Alexander Mordvintsev, and Ludwig Schubert. 2017.\n“Feature Visualization.” Distill. https://doi.org/10.23915/distill.00007.\n\n\nOsgood, Brad. 2019. Lectures on the Fourier Transform and Its\nApplications. American Mathematical Society.\n\n\nRonneberger, Olaf, Philipp Fischer, and Thomas Brox. 2015. “U-Net:\nConvolutional Networks for Biomedical Image Segmentation.”\nCoRR abs/1505.04597. http://arxiv.org/abs/1505.04597.\n\n\nSandler, Mark, Andrew G. Howard, Menglong Zhu, Andrey Zhmoginov, and\nLiang-Chieh Chen. 2018. “Inverted Residuals and Linear\nBottlenecks: Mobile Networks for Classification, Detection and\nSegmentation.” CoRR abs/1801.04381. http://arxiv.org/abs/1801.04381.\n\n\nSmith, Leslie N. 2015. “No More Pesky Learning Rate Guessing\nGames.” CoRR abs/1506.01186. http://arxiv.org/abs/1506.01186.\n\n\nSmith, Leslie N., and Nicholay Topin. 2017. “Super-Convergence:\nVery Fast Training of Residual Networks Using Large Learning\nRates.” CoRR abs/1708.07120. http://arxiv.org/abs/1708.07120.\n\n\nSrivastava, Nitish, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever,\nand Ruslan Salakhutdinov. 2014. “Dropout: A Simple Way to Prevent\nNeural Networks from Overfitting.” J. Mach. Learn. Res.\n15 (1): 1929–58.\n\n\nTrefethen, Lloyd N., and David Bau. 1997. Numerical Linear\nAlgebra. SIAM.\n\n\nVistnes, Arnt Inge. 2018. Physics of Oscillations and Waves. With\nUse of Matlab and Python. Springer.\n\n\nWarden, Pete. 2018. “Speech Commands: A Dataset for\nLimited-Vocabulary Speech Recognition.” CoRR\nabs/1804.03209. http://arxiv.org/abs/1804.03209.\n\n\nZhang, Hongyi, Moustapha Cisse, Yann N. Dauphin, and David Lopez-Paz.\n2017. “mixup: Beyond Empirical Risk\nMinimization.” arXiv e-Prints, October,\narXiv:1710.09412. https://arxiv.org/abs/1710.09412."
  }
]